# app/chapter_generation.py
"""
Module for generating video chapters from *raw* ASR and OCR inputs.
- No ASR/OCR preprocessing/merging; we use what you pass in.
- Enforces a ~128k token prompt budget (approx) for large-context models.
- Parses, lightly cleans, balances chapters, and converts titles to Traditional Chinese.
- Exposes RAW LLM output so you can inspect what the model produced before any parsing/balancing.

CLI examples:
    python video_chaptering.py --asr-file raw_asr.txt --duration 3600 --video-id test_01
    python video_chaptering.py --asr-file raw_asr.txt --ocr-file ocr_raw.txt --duration 1800 --video-id test_02
    # Show both RAW and FINAL in console:
    python video_chaptering.py --asr-file raw_asr.txt --duration 1800 --video-id debug_run --debug
"""

import argparse
import hashlib
import json
import logging
import os
import re
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

PASS3_JSON_SCHEMA = """
{
  "SuggestedUnits": [
    {
      "UnitNo": 1,
      "ParentUnitNo": null,
      "Title": "ç« ç¯€æ¨™é¡Œï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
      "Time": "HH:MM:SS"
    }
  ],
  "CourseSummary": {
    "topic": "...",
    "core_content": "...",
    "learning_objectives": "...",
    "target_audience": "...",
    "difficulty": "..."
  }
}
""".strip()


# Optional Azure AI Inference imports (only if used)
try:
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import SystemMessage, UserMessage
    from azure.core.credentials import AzureKeyCredential
except Exception:  # optional at runtime
    ChatCompletionsClient = None  # type: ignore
    SystemMessage = None  # type: ignore
    UserMessage = None  # type: ignore
    AzureKeyCredential = None  # type: ignore

# Optional OpenAI import (only if used)
try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Optional Simplifiedâ†’Traditional conversion (OpenCC preferred)
try:
    from opencc import OpenCC
    _opencc = OpenCC('s2t')
except Exception:
    _opencc = None

logger = logging.getLogger(__name__)

# ==================== CONFIGURATION ====================
@dataclass
class ChapterConfig:
    """Configuration for chapter generation service"""
    service_type: str = os.getenv("CHAPTER_SERVICE_TYPE", "openai")  # "openai" or "azure"
    openai_model: str = os.getenv("CHAPTER_OPENAI_MODEL", "gpt-4o-mini")
    azure_model: str = os.getenv("CHAPTER_AZURE_MODEL", "Meta-Llama-3.1-8B-Instruct")
    openai_api_key: Optional[str] = os.getenv("OPENAI_API_KEY")
    azure_endpoint: Optional[str] = os.getenv("AZURE_AI_ENDPOINT")
    azure_key: Optional[str] = os.getenv("AZURE_AI_KEY")
    azure_api_version: str = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    openai_base_url: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1/")

def validate_config(config: ChapterConfig) -> bool:
    """Validate that required configuration is present"""
    if config.service_type == "azure":
        if not config.azure_endpoint or not config.azure_key:
            logger.error("Azure AI credentials not configured. Set AZURE_AI_ENDPOINT and AZURE_AI_KEY.")
            return False
    elif config.service_type == "openai":
        if not config.openai_api_key:
            logger.error("OpenAI API key not configured. Set OPENAI_API_KEY.")
            return False
    else:
        logger.error(f"Unknown service type: {config.service_type}")
        return False
    return True

def get_content_hash(transcript: str, ocr_context: str, duration: float) -> str:
    """Generate hash for content to enable caching"""
    content = f"{transcript}{ocr_context}{duration}"
    return hashlib.md5(content.encode()).hexdigest()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utilities
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHAPTER_LINE_RE = re.compile(
    r"""
    ^\s*
    (?:[\-\*\u2022]\s*)?
    \[?(?P<ts>\d{1,2}:\d{2}(?::\d{2})?)\]?\s*
    (?:[\-â€“â€”:]\s*)?
    (?P<title>.+?)
    \s*$
    """,
    re.VERBOSE,
)

ASR_TS_RE = re.compile(
    r"""^\s*
        \[?(\d{1,2}:\d{2}:\d{2})\]?
        \s*(?:[:\-â€“â€”]\s*|\s+)
    """,
    re.VERBOSE,
)
    
def ts_to_seconds_hms(ts: str) -> int:
    try:
        h, m, s = ts.strip().split(":")
        h, m, s = int(h), int(m), int(s)
        if h < 0 or m < 0 or s < 0 or m >= 60 or s >= 60:
            return -1
        return h * 3600 + m * 60 + s
    except Exception:
        return -1

def extract_asr_timestamps_sorted(raw_asr_text: str) -> List[str]:
    """Return sorted unique HH:MM:SS timestamps found in raw ASR."""
    seen = set()
    for line in (raw_asr_text or "").splitlines():
        m = ASR_TS_RE.match(line)
        if m:
            seen.add(_normalize_ts(m.group(1)))
    out = sorted(seen, key=ts_to_seconds_hms)
    return out

def get_first_last_asr_ts(raw_asr_text: str, duration_sec: int) -> Tuple[str, str]:
    """
    Returns (first_ts, last_ts) from raw ASR timestamps if present,
    otherwise falls back to 00:00:00 and full duration.
    """
    ts_sorted = extract_asr_timestamps_sorted(raw_asr_text)
    if ts_sorted:
        return ts_sorted[0], ts_sorted[-1]
    return "00:00:00", sec_to_hms(int(duration_sec))

def pick_anchor_timestamps(asr_ts_sorted: List[str], k: int = 12) -> List[str]:
    """
    Pick k timestamps spread across the whole ASR (including the tail).
    Always includes first and last if available.
    """
    if not asr_ts_sorted:
        return []
    if len(asr_ts_sorted) <= k:
        return asr_ts_sorted

    idxs = [0]
    # evenly spaced indices
    for i in range(1, k - 1):
        idxs.append(round(i * (len(asr_ts_sorted) - 1) / (k - 1)))
    idxs.append(len(asr_ts_sorted) - 1)

    # unique + ordered
    idxs = sorted(set(idxs))
    return [asr_ts_sorted[i] for i in idxs if 0 <= i < len(asr_ts_sorted)]

def chapters_coverage_ratio(
    suggested_units_structured: List[Dict[str, Any]],
    last_asr_sec: int
) -> float:
    """Compute last chapter time / last ASR time."""
    if not suggested_units_structured or last_asr_sec <= 0:
        return 1.0
    last_ch_ts = suggested_units_structured[-1].get("Time")
    last_ch_sec = ts_to_seconds_hms(str(last_ch_ts or ""))
    if last_ch_sec < 0:
        return 0.0
    return last_ch_sec / last_asr_sec


def sec_to_hms(sec: int) -> str:
    """Convert seconds to HH:MM:SS format"""
    if sec < 0:
        sec = 0
    h, rem = divmod(sec, 3600)
    m, s = divmod(rem, 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def _is_cjk(ch: str) -> bool:
    return '\u4e00' <= ch <= '\u9fff'

def clean_chapter_titles(chapters: Dict[str, str]) -> Dict[str, str]:
    """
    Clean up chapter titles by removing filler words and improving clarity.
    For Chinese titles, avoid English-centric capitalization; if over-trimmed (<4 chars),
    fall back to the original title.
    """
    cleaned: Dict[str, str] = {}
    filler_words = ['é‚£', 'æ‰€ä»¥', 'é€™å€‹', 'é‚£å€‹', 'å°±æ˜¯', 'å‘¢', 'å•Š', 'å–”', 'ç„¶å¾Œ', 'æ¥è‘—']
    for ts, original_title in chapters.items():
        title = original_title
        for word in filler_words:
            title = title.replace(word, '')
        title = re.sub(r'[ã€‚ï¼Œâ€œâ€ã€ï¼ï¼Ÿ\.!?,]+$', '', title.strip())
        title = re.sub(r'\s+', ' ', title)

        # If cleaning made it too short, revert to the original
        if 0 < len(title) < 4:
            title = original_title.strip()

        cleaned[ts] = title 
    return cleaned

def count_tokens_llama(text: str) -> int:
    """Approximate token counting for mixed Chinese/English (â‰ˆ1 token per CJK char; 1/4 per other chars)"""
    if not text:
        return 0
    chinese_chars = sum(1 for char in text if _is_cjk(char))
    non_chinese_len = len(text) - chinese_chars
    return chinese_chars + max(1, non_chinese_len // 4)

def truncate_text_by_tokens(text: str, max_tokens: int = 120_000) -> str:
    """Truncate text to approximately max_tokens, preserving sentence boundaries where possible"""
    if max_tokens <= 0:
        return ""
    if count_tokens_llama(text) <= max_tokens:
        return text
    logger.warning(f"Truncating transcript from {count_tokens_llama(text):,} tokens to {max_tokens:,} tokens")
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])', text)
    truncated = ""
    current_tokens = 0
    for sentence in sentences:
        sentence_tokens = count_tokens_llama(sentence)
        if current_tokens + sentence_tokens > max_tokens:
            break
        truncated += sentence
        current_tokens += sentence_tokens
    return truncated

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Chapter policy & parsing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chapter_policy(duration_sec: int) -> Tuple[int, Tuple[int, int], int]:
    """Determine chapter generation parameters based on video duration"""
    if duration_sec < 30 * 60:     # < 30 min
        return 120, (5, 10), 30    # min_gap: 2 min, target: 5-10 chapters
    elif duration_sec < 60 * 60:   # < 1 hour  
        return 180, (6, 12), 40    # min_gap: 3 min, target: 6-12 chapters
    elif duration_sec < 120 * 60:  # < 2 hours
        return 240, (8, 16), 50    # min_gap: 4 min, target: 8-16 chapters
    elif duration_sec < 180 * 60:  # < 3 hours
        return 300, (10, 20), 60   # min_gap: 5 min, target: 10-20 chapters
    else:                           # 3+ hours
        return 360, (12, 24), 80   # min_gap: 6 min, target: 12-24 chapters

def _normalize_ts(ts: str) -> str:
    """Normalize timestamp format to HH:MM:SS"""
    parts = ts.split(":")
    if len(parts) == 2:
        return f"00:{parts[0].zfill(2)}:{parts[1].zfill(2)}"
    if len(parts) == 3:
        return f"{parts[0].zfill(2)}:{parts[1].zfill(2)}:{parts[2].zfill(2)}"
    return ts

def parse_chapters_from_output(output_text: str) -> Dict[str, str]:
    """Parse chapter timestamps and titles from LLM output"""
    chapters: Dict[str, str] = {}
    
    # Direct parsing for "HH:MM:SS - Title" format
    for line in output_text.splitlines():
        line = line.strip()
        if not line:
            continue
        
        # Look for the pattern "HH:MM:SS - Title"
        if ' - ' in line:
            parts = line.split(' - ', 1)
            if len(parts) == 2:
                ts = parts[0].strip()
                title = parts[1].strip()
                # Validate timestamp format
                if re.fullmatch(r'\d{2}:\d{2}:\d{2}', ts):
                    chapters[ts] = title
    
    # If no chapters found, try the original regex approach
    if not chapters:
        for line in output_text.splitlines():
            line = line.strip()
            if not line:
                continue
            m = CHAPTER_LINE_RE.match(line)
            if m:
                ts = _normalize_ts(m.group("ts").strip())
                title = m.group("title").strip()
                if title:
                    chapters.setdefault(ts, title)
    
    return chapters

def parse_summary_from_output(output_text: str) -> Dict[str, str]:
    """Extract the structured summary from the LLM output"""
    summary = {}
    lines = output_text.split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('èª²ç¨‹ä¸»é¡Œï¼š'):
            summary['topic'] = line.replace('èª²ç¨‹ä¸»é¡Œï¼š', '').strip()
        elif line.startswith('æ ¸å¿ƒå…§å®¹ï¼š'):
            summary['core_content'] = line.replace('æ ¸å¿ƒå…§å®¹ï¼š', '').strip()
        elif line.startswith('å­¸ç¿’ç›®æ¨™ï¼š'):
            summary['learning_objectives'] = line.replace('å­¸ç¿’ç›®æ¨™ï¼š', '').strip()
        elif line.startswith('é©åˆå°è±¡ï¼š'):
            summary['target_audience'] = line.replace('é©åˆå°è±¡ï¼š', '').strip()
        elif line.startswith('é›£åº¦ç´šåˆ¥ï¼š'):
            summary['difficulty'] = line.replace('é›£åº¦ç´šåˆ¥ï¼š', '').strip()
    
    # Apply Traditional Chinese conversion to summary fields
    if _opencc:
        for key in summary:
            summary[key] = to_traditional(summary[key])
    
    return summary

def _extract_json_blob(text: str) -> Optional[str]:
    """
    Try to extract JSON from:
    - raw JSON
    - ```json ... ```
    - ``` ... ```
    - or: first {...} / [...] span inside surrounding text
    Returns JSON string or None.
    """
    if not text:
        return None

    # ```json ... ```
    m = re.search(r"```json\s*([\[{].*?[\]}])\s*```", text, re.DOTALL | re.IGNORECASE)
    if m:
        return m.group(1).strip()

    # ``` ... ```
    m = re.search(r"```\s*([\[{].*?[\]}])\s*```", text, re.DOTALL)
    if m:
        return m.group(1).strip()

    s = text.strip()

    # raw json only
    if (s.startswith("{") and s.endswith("}")) or (s.startswith("[") and s.endswith("]")):
        return s

    # NEW: attempt to extract the first JSON object/array embedded in other text
    first_obj = s.find("{")
    last_obj = s.rfind("}")
    if 0 <= first_obj < last_obj:
        candidate = s[first_obj:last_obj + 1].strip()
        if candidate.startswith("{") and candidate.endswith("}"):
            return candidate

    first_arr = s.find("[")
    last_arr = s.rfind("]")
    if 0 <= first_arr < last_arr:
        candidate = s[first_arr:last_arr + 1].strip()
        if candidate.startswith("[") and candidate.endswith("]"):
            return candidate

    return None


def safe_load_json(text: str) -> Optional[Any]:
    blob = _extract_json_blob(text)
    if not blob:
        return None
    try:
        return json.loads(blob)
    except Exception:
        return None

def _is_hms(ts: str) -> bool:
    return bool(re.fullmatch(r"\d{2}:\d{2}:\d{2}", (ts or "").strip()))

# --- Client unit parsing helpers (back-compat) ---
_CLIENT_UNIT_RE = re.compile(r"\[\s*å–®å…ƒ\s*(\d+)\s*[:ï¼š]")

def _extract_client_unit_no_from_title(title: str) -> Optional[int]:
    m = _CLIENT_UNIT_RE.search(title or "")
    if not m:
        return None
    try:
        return int(m.group(1))
    except Exception:
        return None

def normalize_suggested_units(
    suggested_units: Any,
    units: Optional[List[Dict]] = None
) -> List[Dict[str, Any]]:
    """
    Normalize SuggestedUnits list:
    - enforce fields
    - ensure Time is HH:MM:SS
    - ParentUnitNo is optional and used ONLY for chapter hierarchy (do not validate it vs client Units)
    - If client units are provided:
        - validate ClientUnitNo against Units[].UnitNo
        - fill ClientUnitTitle from Units[].Title
    - sort by Time
    - renumber UnitNo sequentially
    """
    if not isinstance(suggested_units, list):
        return []

    valid_client_units = None
    unit_title_by_no: Dict[int, str] = {}
    if units:
        valid_client_units = set()
        for u in units:
            try:
                uno = int(u.get("UnitNo"))
                valid_client_units.add(uno)
                unit_title_by_no[uno] = str(u.get("Title") or "").strip()
            except Exception:
                pass

    out: List[Dict[str, Any]] = []
    for su in suggested_units:
        if not isinstance(su, dict):
            continue

        title = str(su.get("Title") or "").strip()
        ts = str(su.get("Time") or "").strip()
        if not title or not _is_hms(ts):
            continue

        # If title already starts with a unit prefix, strip it to avoid double-prefixing later
        title = re.sub(r'^\s*\[\s*å–®å…ƒ\s*\d+\s*(?:[:ï¼š][^\]]+)?\]\s*', '', title).strip()

        # ParentUnitNo is chapter-hierarchy only now (optional; sanitize int/null)
        parent = su.get("ParentUnitNo", None)
        if parent is not None:
            try:
                parent = int(parent)
            except Exception:
                parent = None

        # NEW: ClientUnitNo mapping (validate vs client Units if provided)
        client_unit_no = su.get("ClientUnitNo", None)
        if client_unit_no is not None:
            try:
                client_unit_no = int(client_unit_no)
            except Exception:
                client_unit_no = None

        # Back-compat fallback: parse from title "[å–®å…ƒNï¼š...]"
        if client_unit_no is None:
            client_unit_no = _extract_client_unit_no_from_title(title)

        if valid_client_units is None:
            # No client Units provided => allow null mapping
            client_unit_no = None
            client_unit_title = None
        else:
            if client_unit_no not in valid_client_units:
                client_unit_no = None
            client_unit_title = unit_title_by_no.get(client_unit_no) if client_unit_no else None

        out.append({
            "UnitNo": 0,  # will renumber
            "ParentUnitNo": parent,
            "Title": title,
            "Time": ts,
            "ClientUnitNo": client_unit_no,
            "ClientUnitTitle": client_unit_title,
        })

    out.sort(key=lambda x: x["Time"])
    for i, su in enumerate(out, 1):
        su["UnitNo"] = i
    return out

def back_calculate_unit_timestamps(
    suggested_units_structured: List[Dict[str, Any]],
    client_units: Optional[List[Dict[str, Any]]]
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Back-calculate timestamps for client's Units based on SuggestedUnits mapping.
    Also validates logical order and provides diagnostics.
    
    Args:
        suggested_units_structured: AI-generated chapters with ClientUnitNo mapping
        client_units: Original Units from client (can be None/empty)
        
    Returns:
        Tuple of:
        - enriched_units: Units with timestamps and metadata
        - diagnostics: Validation results and statistics
        
    Example:
        Input client_units:
        [
            {"UnitNo": 1, "Title": "å»šå…·è¦åŠƒ"},
            {"UnitNo": 2, "Title": "å¤©èŠ±æ¿å¤§æ¨£åœ–"},
            {"UnitNo": 3, "Title": "å†·æ°£é…ç½®"}
        ]
        
        Input suggested_units_structured:
        [
            {UnitNo: 1, Title: "å»šæˆ¿ä¸‰è§’åŸç†", Time: "00:05:10", ClientUnitNo: 1},
            {UnitNo: 2, Title: "å»šå…·å°ºå¯¸æ¨™æº–", Time: "00:18:30", ClientUnitNo: 1},
            {UnitNo: 3, Title: "å¤§æ¨£åœ–è¦ç¯„", Time: "00:32:15", ClientUnitNo: 2},
            ...
        ]
        
        Output enriched_units:
        [
            {
                "UnitNo": 1,
                "Title": "å»šå…·è¦åŠƒ",
                "Time": "00:05:10",  # First SuggestedUnit with ClientUnitNo=1
                "EndTime": "00:32:15",
                "Duration": "00:27:05",
                "SuggestedUnitCount": 2,
                "FirstChapter": "å»šæˆ¿å·¥ä½œä¸‰è§’åŸç†èˆ‡å‹•ç·šè¨­è¨ˆ",
                "LastChapter": "å»šå…·å°ºå¯¸æ¨™æº–èˆ‡äººé«”å·¥å­¸è€ƒé‡"
            },
            ...
        ]
    """
    
    # Handle case where no Units provided
    if not client_units:
        logger.info("â„¹ï¸ No client Units provided - skipping Unit timestamp back-calculation")
        return [], {
            "units_provided": False,
            "validation_passed": True,
            "message": "No client Units to process"
        }
    
    if not suggested_units_structured:
        logger.warning("âš ï¸ No SuggestedUnits generated - cannot back-calculate Unit timestamps")
        return client_units, {
            "units_provided": True,
            "validation_passed": False,
            "error": "No SuggestedUnits available for mapping"
        }
    
    # Build mapping: ClientUnitNo -> list of SuggestedUnits
    unit_chapters_map: Dict[int, List[Dict[str, Any]]] = {}
    unmapped_chapters: List[Dict[str, Any]] = []
    
    for su in suggested_units_structured:
        client_unit_no = su.get("ClientUnitNo")
        
        if client_unit_no is None:
            unmapped_chapters.append(su)
            continue
        
        if client_unit_no not in unit_chapters_map:
            unit_chapters_map[client_unit_no] = []
        
        unit_chapters_map[client_unit_no].append(su)
    
    # Sort chapters within each Unit by time
    for unit_no in unit_chapters_map:
        unit_chapters_map[unit_no].sort(
            key=lambda x: ts_to_seconds_hms(x["Time"])
        )
    
    # Enrich client units with timestamps and metadata
    enriched_units = []
    validation_issues = []
    
    for i, unit in enumerate(client_units):
        unit_no = unit.get("UnitNo")
        enriched_unit = dict(unit)  # Copy original
        
        if unit_no not in unit_chapters_map:
            # Unit not found in video
            logger.warning(
                f"âš ï¸ Unit {unit_no} ('{unit.get('Title')}') has NO mapped chapters in video!"
            )
            enriched_unit.update({
                "Time": "",
                "EndTime": None,
                "Duration": None,
                "SuggestedUnitCount": 0,
                "FirstChapter": None,
                "LastChapter": None
            })
            validation_issues.append({
                "unit_no": unit_no,
                "issue": "not_found",
                "message": f"Unit '{unit.get('Title')}' not found in video"
            })
            enriched_units.append(enriched_unit)
            continue
        
        # Get chapters for this Unit
        chapters = unit_chapters_map[unit_no]
        first_chapter = chapters[0]
        last_chapter = chapters[-1]
        
        # Calculate start time (first chapter of this Unit)
        start_time = first_chapter["Time"]
        start_sec = ts_to_seconds_hms(start_time)
        
        # Calculate end time (first chapter of NEXT Unit, or None if last)
        end_time = None
        end_sec = None
        duration_sec = None
        
        if i + 1 < len(client_units):
            next_unit_no = client_units[i + 1].get("UnitNo")
            if next_unit_no in unit_chapters_map:
                next_chapters = unit_chapters_map[next_unit_no]
                end_time = next_chapters[0]["Time"]
                end_sec = ts_to_seconds_hms(end_time)
                duration_sec = end_sec - start_sec
        
        enriched_unit.update({
            "Time": start_time,
            "EndTime": end_time,
            "Duration": sec_to_hms(duration_sec) if duration_sec else None,
            "SuggestedUnitCount": len(chapters),
            "FirstChapter": first_chapter["Title"],
            "LastChapter": last_chapter["Title"]
        })
        
        enriched_units.append(enriched_unit)
        
        logger.info(
            f"âœ… Unit {unit_no}: {unit['Title']}\n"
            f"   â†’ Starts at: {start_time}\n"
            f"   â†’ Contains: {len(chapters)} chapters\n"
            f"   â†’ First: {first_chapter['Title']}\n"
            f"   â†’ Last: {last_chapter['Title']}"
        )
    
    # Validate logical order
    timestamps_valid = True
    for i in range(len(enriched_units) - 1):
        current = enriched_units[i]
        next_unit = enriched_units[i + 1]
        
        if not current.get("Time") or not next_unit.get("Time"):
            continue
        
        current_sec = ts_to_seconds_hms(current["Time"])
        next_sec = ts_to_seconds_hms(next_unit["Time"])
        
        if current_sec >= next_sec:
            timestamps_valid = False
            validation_issues.append({
                "unit_no": current["UnitNo"],
                "issue": "order_violation",
                "message": f"Unit {current['UnitNo']} ({current['Time']}) should come before Unit {next_unit['UnitNo']} ({next_unit['Time']})"
            })
            logger.error(
                f"âŒ ORDER VIOLATION: Unit {current['UnitNo']} ({current['Time']}) >= "
                f"Unit {next_unit['UnitNo']} ({next_unit['Time']})"
            )
    
    # Build diagnostics
    diagnostics = {
        "units_provided": True,
        "total_units": len(client_units),
        "units_found": sum(1 for u in enriched_units if u.get("Time")),
        "units_missing": sum(1 for u in enriched_units if not u.get("Time")),
        "total_suggested_units": len(suggested_units_structured),
        "mapped_suggested_units": sum(len(chapters) for chapters in unit_chapters_map.values()),
        "unmapped_suggested_units": len(unmapped_chapters),
        "timestamps_valid": timestamps_valid,
        "validation_issues": validation_issues
    }
    
    # Log summary
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“ UNIT TIMESTAMP BACK-CALCULATION SUMMARY")
    logger.info("=" * 60)
    logger.info(f"âœ… Units found: {diagnostics['units_found']}/{diagnostics['total_units']}")
    logger.info(f"âš ï¸ Units missing: {diagnostics['units_missing']}/{diagnostics['total_units']}")
    logger.info(f"ğŸ“Š SuggestedUnits mapped: {diagnostics['mapped_suggested_units']}/{diagnostics['total_suggested_units']}")
    logger.info(f"ğŸ“Š SuggestedUnits unmapped: {diagnostics['unmapped_suggested_units']}")
    logger.info(f"{'âœ…' if timestamps_valid else 'âŒ'} Timestamp order: {'Valid' if timestamps_valid else 'INVALID'}")
    
    if validation_issues:
        logger.warning(f"\nâš ï¸ {len(validation_issues)} validation issues found:")
        for issue in validation_issues:
            logger.warning(f"   - {issue['message']}")
    
    logger.info("=" * 60 + "\n")
    
    return enriched_units, diagnostics

def suggested_units_to_chapters_dict(
    suggested_units: List[Dict[str, Any]],
    *,
    duration_sec: Optional[int] = None,
    bump_limit_sec: int = 120,  # allow a bit more room than 59s
) -> Dict[str, str]:
    """
    Convert SuggestedUnits -> chapters dict, preventing timestamp key collisions
    WITHOUT producing non-HH:MM:SS keys (so later validation won't drop them).

    Strategy:
    - Prefer the original HH:MM:SS.
    - If already used, bump forward by +1..+bump_limit_sec.
    - If still used, bump backward by -1..-bump_limit_sec.
    - If still impossible, drop the duplicate with a warning.
    """

    def bump_ts(ts: str, delta: int) -> Optional[str]:
        base = ts_to_seconds_hms(ts)
        if base < 0:
            return None
        bumped = base + delta
        if duration_sec is not None:
            if bumped < 0 or bumped > duration_sec - 1:
                return None  # <-- don't clamp; reject
        return sec_to_hms(bumped)

    chapters: Dict[str, str] = {}
    used = set()

    for su in suggested_units:
        ts = str(su.get("Time") or "").strip()
        title = str(su.get("Title") or "").strip()
        cu = su.get("ClientUnitNo")
        cut = su.get("ClientUnitTitle")
        prefix = f"[å–®å…ƒ{cu}ï¼š{cut}] " if cu and cut else (f"[å–®å…ƒ{cu}] " if cu else "")

        if not _is_hms(ts) or not title:
            continue

        candidate = ts

        if candidate in used:
            placed = False

            # 1) bump forward
            for d in range(1, bump_limit_sec + 1):
                cand2 = bump_ts(ts, d)
                if cand2 and cand2 not in used:
                    candidate = cand2
                    placed = True
                    break

            # 2) bump backward if forward didn't work
            if not placed:
                for d in range(1, bump_limit_sec + 1):
                    cand2 = bump_ts(ts, -d)
                    if cand2 and cand2 not in used:
                        candidate = cand2
                        placed = True
                        break

            if not placed:
                logger.warning(
                    "âš ï¸ Could not place unique HH:MM:SS timestamp for chapter (ts=%s, title=%s). Dropping.",
                    ts, title[:80]
                )
                continue  # drop rather than create invalid key

        used.add(candidate)
        chapters[candidate] = prefix + title

    return chapters


def validate_and_normalize_timestamps(
    chapters: Dict[str, str], 
    duration_sec: int,
    video_id: str = "unknown"
) -> Dict[str, str]:
    """
    Validate and normalize chapter timestamps to ensure:
    1. All timestamps are in HH:MM:SS format
    2. All timestamps are within video duration
    3. Timestamps are distributed across the video (not just first few minutes)
    4. Suspicious clustering is detected and logged
    
    Returns: Cleaned and validated chapters dict
    """
    if not chapters:
        return {}

    def ts_to_seconds(ts: str) -> int:
        """Convert HH:MM:SS to seconds. Return -1 if invalid."""
        try:
            parts = ts.split(':')
            if len(parts) == 3:
                h, m, s = int(parts[0]), int(parts[1]), int(parts[2])
                if h < 0 or m < 0 or s < 0 or m >= 60 or s >= 60:
                    return -1
                return h * 3600 + m * 60 + s
            elif len(parts) == 2:
                m, s = int(parts[0]), int(parts[1])
                if m < 0 or s < 0 or m >= 60 or s >= 60:
                    return -1
                return m * 60 + s
            return -1
        except Exception:
            return -1
        
    validated = {}
    timestamps_in_seconds = []
    
    for ts, title in chapters.items():
        ts_normalized = _normalize_ts(ts)  # Ensure HH:MM:SS format
        ts_sec = ts_to_seconds(ts_normalized)
        
        # Check 1: Within video duration
        if ts_sec > duration_sec:
            logger.warning(f"âš ï¸ [{video_id}] Skipping chapter at {ts_normalized} ({ts_sec}s) - exceeds duration ({duration_sec}s)")
            continue
        
        # Check 2: Not negative
        if ts_sec < 0:
            logger.warning(f"âš ï¸ [{video_id}] Skipping chapter at {ts_normalized} - invalid timestamp")
            continue
        
        validated[ts_normalized] = title
        timestamps_in_seconds.append(ts_sec)
    
    if not validated:
        logger.error(f"âŒ [{video_id}] No valid chapters after timestamp validation!")
        return {}
    
    # Check 3: Detect suspicious clustering (all chapters in first 10% of video)
    first_chapter_sec = min(timestamps_in_seconds)
    last_chapter_sec = max(timestamps_in_seconds)
    chapter_span_sec = last_chapter_sec - first_chapter_sec
    chapter_span_percent = (chapter_span_sec / duration_sec) * 100 if duration_sec > 0 else 0
    
    logger.info(f"ğŸ“Š [{video_id}] Chapter span: {sec_to_hms(int(chapter_span_sec))} ({chapter_span_percent:.1f}% of video)")
    
    # CRITICAL CHECK: If all chapters are in first 10% of video, something is wrong
    if chapter_span_percent < 10 and duration_sec > 600:  # Only check for videos > 10 min
        logger.warning("=" * 70)
        logger.warning(f"âš ï¸  [{video_id}] SUSPICIOUS CHAPTER CLUSTERING DETECTED!")
        logger.warning(f"âš ï¸  All {len(validated)} chapters are in first {chapter_span_percent:.1f}% of video")
        logger.warning(f"âš ï¸  Video duration: {sec_to_hms(int(duration_sec))} ({duration_sec}s)")
        logger.warning(f"âš ï¸  Chapter range: {sec_to_hms(int(first_chapter_sec))} to {sec_to_hms(int(last_chapter_sec))}")
        logger.warning(f"âš ï¸  This likely indicates LLM timestamp format confusion")
        logger.warning("=" * 70)
        
        # Log first few chapters for debugging
        logger.warning(f"âš ï¸  Chapters generated:")
        for ts, title in sorted(validated.items(), key=lambda x: ts_to_seconds(x[0]))[:5]:
            logger.warning(f"     {ts} - {title[:60]}")
        if len(validated) > 5:
            logger.warning(f"     ... and {len(validated) - 5} more")
    
    # Check 4: Detect if too many chapters are within first minute
    chapters_in_first_minute = sum(1 for ts_sec in timestamps_in_seconds if ts_sec < 60)
    if chapters_in_first_minute > len(timestamps_in_seconds) * 0.5:
        logger.warning(f"âš ï¸ [{video_id}] {chapters_in_first_minute}/{len(timestamps_in_seconds)} chapters in first 60 seconds - possible timestamp error")
    
    # Summary
    logger.info(f"âœ… [{video_id}] Validated {len(validated)}/{len(chapters)} chapters")
    logger.info(f"   First: {sec_to_hms(int(first_chapter_sec))} | Last: {sec_to_hms(int(last_chapter_sec))} | Span: {chapter_span_percent:.1f}%")
    
    return validated

def globally_balance_chapters(
    chapters: Dict[str, str],
    duration_sec: int,
    min_gap_sec: int,
    target_range: Tuple[int, int],
    max_caps: int,
) -> Dict[str, str]:
    """Balance chapters with content-aware merging"""
    
    def extract_module_tag(title: str) -> str:
        """Extract [module] tag if present"""
        match = re.match(r'\[([^\]]+)\]', title)
        return match.group(1) if match else ""

    def ts_to_s(ts: str) -> int:
        """
        Convert a timestamp string to seconds.
        Accepts: "MM:SS" or "HH:MM:SS"
        Returns: seconds, or -1 if invalid
        """
        try:
            p = ts.strip().split(":")
            if len(p) == 2:
                m, s = int(p[0]), int(p[1])
                if m < 0 or s < 0 or m >= 60 or s >= 60:
                    return -1
                return m * 60 + s
            if len(p) == 3:
                h, m, s = int(p[0]), int(p[1]), int(p[2])
                if h < 0 or m < 0 or s < 0 or m >= 60 or s >= 60:
                    return -1
                return h * 3600 + m * 60 + s
            return -1
        except Exception:
            return -1

    cands = []
    for ts, t in chapters.items():
        s = ts_to_s(ts)
        if 0 <= s <= duration_sec:
            cands.append((s, ts, t.strip()))
        
    cands.sort(key=lambda x: x[0])
    if not cands:
        return {}

    # Content-aware deduplication
    dedup = []
    # âœ… UNIVERSAL FIX: Scale merge threshold with video duration
    # This prevents over-aggressive merging for long videos

    adaptive_min_gap = max(120, min_gap_sec // 2)
    
    logger.info(f"ğŸ“Š Chapter balancing: {len(chapters)} input chapters, merge threshold = {adaptive_min_gap}s (policy min_gap: {min_gap_sec}s)")
    
    for s, ts, title in cands:
        if not dedup:
            dedup.append((s, ts, title))
            continue
            
        time_gap = s - dedup[-1][0]
        
        # Merge if same module and close, or very close regardless
        should_merge = False
        prev_module = extract_module_tag(dedup[-1][2])
        curr_module = extract_module_tag(title)
        
        # âœ… FIXED: Use adaptive threshold instead of hardcoded 120s
        if time_gap < adaptive_min_gap:
            should_merge = True
    
        elif prev_module and curr_module and prev_module == curr_module:
            # Same module tag - use min_gap_sec
            should_merge = time_gap < min_gap_sec * 0.7
        elif time_gap < min_gap_sec // 2:  # Half the min_gap for different modules
            should_merge = True
            
        if should_merge:
            # Keep the better title (prefer tagged, then longer)
            prev_s, prev_ts, prev_title = dedup[-1]
            if curr_module and not prev_module:
                dedup[-1] = (prev_s, prev_ts, title)
            elif len(title) > len(prev_title) * 1.3:
                dedup[-1] = (prev_s, prev_ts, title)
        else:
            dedup.append((s, ts, title))

    t_low, t_high = target_range
    
    # Log the balancing result
    logger.info(f"Chapter balancing: {len(chapters)} â†’ {len(dedup)} (target: {t_low}-{t_high})")
    
    if t_low <= len(dedup) <= t_high:
        return {ts: title for _, ts, title in dedup}

    # Too many chapters: choose representatives from each segment
    if len(dedup) > t_high:
        selected = []
        segment_length = max(1, duration_sec // t_high)
        for i in range(t_high):
            segment_start = i * segment_length
            segment_end = (i + 1) * segment_length if i < t_high - 1 else duration_sec + 1
            segment_chapters = [c for c in dedup if segment_start <= c[0] < segment_end]
            if segment_chapters:
                # Pick the one closest to segment center
                segment_center = (segment_start + segment_end) // 2
                chosen = min(segment_chapters, key=lambda c: abs(c[0] - segment_center))
                selected.append(chosen)
        selected.sort(key=lambda x: x[0])
        return {ts: title for _, ts, title in selected}

    # Not enough chapters: just cap to max_caps
    return {ts: title for _, ts, title in dedup[:max_caps]}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OCR handling (optional legacy "segments" mode)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_ocr_segments(file_obj, filename: str) -> List[Dict]:
    """
    Accepts:
      - JSON array:        [ { "start": 0, "end": 3, "text": "..." }, ... ]
      - Wrapped JSON:      { "segments": [ {...}, ... ] }
      - JSON Lines (JSONL): one JSON object per line
      - Plain text (.txt): whole file becomes a single segment at t=0
    Returns: List[Dict] with keys: start (int), end (int, optional), text (str)
    """
    try:
        data = json.load(file_obj)
        if isinstance(data, dict) and "segments" in data and isinstance(data["segments"], list):
            segments = data["segments"]
        elif isinstance(data, list):
            segments = data
        else:
            return []
        out = []
        for item in segments:
            if not isinstance(item, dict):
                continue
            text = str(item.get("text", "")).strip()
            if not text:
                continue
            start = int(item.get("start", 0))
            end = int(item.get("end", start))
            out.append({"start": start, "end": end, "text": text})
        return out
    except json.JSONDecodeError:
        pass

    # Try JSONL
    try:
        file_obj.seek(0)
        segments = []
        for line in file_obj:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                segments = None
                break
            if not isinstance(obj, dict):
                continue
            text = str(obj.get("text", "")).strip()
            if not text:
                continue
            start = int(obj.get("start", 0))
            end = int(obj.get("end", start))
            segments.append({"start": start, "end": end, "text": text})
        if segments is not None:
            return segments
    except Exception:
        pass

    # Plain text fallback
    try:
        file_obj.seek(0)
        txt = file_obj.read().strip()
    except Exception:
        txt = ""
    return [{"start": 0, "end": 0, "text": txt}] if txt else []

def build_ocr_context_from_segments(ocr_segments: List[Dict]) -> str:
    """Legacy minimal OCR formatting: timestamped lines with a simple header."""
    if not ocr_segments:
        return ""
    lines = ["# è¢å¹•/æŠ•å½±ç‰‡æ“·å–æ–‡å­—ï¼ˆåŸå§‹ï¼‰ï¼š"]
    for seg in ocr_segments:
        start = int(seg.get("start", 0))
        text = str(seg.get("text", "")).strip()
        if not text:
            continue
        lines.append(f"* {sec_to_hms(start)}ï¼š{text}")
    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Simplifiedâ†’Traditional conversion
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_S2T_FALLBACK_MAP = {
    "ä½“": "é«”", "å°": "è‡º", "å": "å¾Œ", "å¹¿": "å»£", "ç”»": "ç•«", "å½•": "éŒ„", "è§‚": "è§€",
    "é¢": "éºµ", "å‘": "ç™¼", "é—¨": "é–€", "é—®": "å•", "ç±»": "é¡", "ç½‘": "ç¶²", "å›¾": "åœ–",
    "ä¹¦": "æ›¸", "è®°": "è¨˜", "è¯»": "è®€", "å…š": "é»¨", "æœ¯": "è¡“", "å±‚": "å±¤", "çº¦": "ç´„",
}

def to_traditional(text: str) -> str:
    """Convert a string to Traditional Chinese. Uses OpenCC if available; otherwise minimal mapping."""
    if not text:
        return text
    if _opencc is not None:
        try:
            return _opencc.convert(text)
        except Exception:
            pass
    return ''.join(_S2T_FALLBACK_MAP.get(ch, ch) for ch in text)

def ensure_traditional_chapters(chapters: Dict[str, str]) -> Dict[str, str]:
    """Convert all chapter titles to Traditional Chinese (idempotent if already Traditional)."""
    return {ts: to_traditional(title) for ts, title in chapters.items()}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Client Initialization & LLM call
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def initialize_client(service_type: str, **kwargs) -> Any:
    """Initialize the appropriate LLM client"""
    if service_type == "azure":
        if ChatCompletionsClient is None or AzureKeyCredential is None:
            raise RuntimeError("Azure dependencies are not available in this environment.")
        return ChatCompletionsClient(
            endpoint=kwargs["endpoint"],
            credential=AzureKeyCredential(kwargs["key"]),
            api_version=kwargs.get("api_version", "2024-05-01-preview"),
        )
    elif service_type == "openai":
        if OpenAI is None:
            raise RuntimeError("OpenAI client is not available in this environment.")
        return OpenAI(
            api_key=kwargs["api_key"],
            base_url=kwargs.get("base_url", "https://api.openai.com/v1/"),
        )
    else:
        raise ValueError(f"Unknown service type: {service_type}")

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((Exception,)),
    reraise=True,
)
def call_llm(
    service_type: str,
    client: Any,
    system_message: str,
    user_message: str,
    model: str,
    max_tokens: int = 2048,
    temperature: float = 0.2,
    top_p: float = 0.9,
) -> Any:
    """Call LLM API with retry logic"""
    if service_type == "azure":
        return client.complete(
            messages=[
                SystemMessage(content=system_message),
                UserMessage(content=user_message),
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            model=model,
        )
    elif service_type == "openai":
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        return response
    else:
        raise ValueError(f"Unknown service type: {service_type}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompt builder (ASR first, OCR second, OCR verbatim supported)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_prompt_body(
    transcript: str,
    duration_sec: int,
    ocr_context: str = "",
    video_title: Optional[str] = None,
    first_ts_override: Optional[str] = None,
    last_ts_override: Optional[str] = None,
) -> str:
    duration_hms = sec_to_hms(int(duration_sec))
    min_gap_sec, (t_low, t_high), max_caps = chapter_policy(int(duration_sec))
    
    # Extract first/last REAL ASR timestamps using the same matcher as the rest of the pipeline.
    # This supports:
    #   00:00:12: text
    #   00:00:12 - text
    #   [00:00:12] text
    #   00:00:12 text
    timestamps: List[str] = []
    for line in transcript.splitlines():
        m = ASR_TS_RE.match(line)
        if m:
            timestamps.append(_normalize_ts(m.group(1)))
    first_ts = first_ts_override or (timestamps[0] if timestamps else "00:00:00")
    last_ts  = last_ts_override  or (timestamps[-1] if timestamps else duration_hms)


    video_title_context = ""
    if video_title:
        # Strip common video extensions
        clean_title = re.sub(r'\.(mp4|avi|mov|mkv|webm|flv|m4v)$', '', video_title, flags=re.IGNORECASE)
        video_title_context = f"""
        
# ğŸ“š èª²ç¨‹æª”æ¡ˆè³‡è¨Š
æª”åï¼š{clean_title}
è«‹åƒè€ƒæª”åç†è§£èª²ç¨‹ä¸»é¡Œã€ç« ç¯€ç·¨è™Ÿã€æ¶µè“‹å…§å®¹ç­‰é‡è¦è³‡è¨Šï¼Œä¸¦æ“šæ­¤è¨­è¨ˆç« ç¯€çµæ§‹ã€‚
"""
    
    prompt = f"""
# æ•™è‚²ç« ç¯€è¨­è¨ˆå°ˆå®¶ - æ™‚é–“æˆ³è¨˜ç²¾æº–å°æ‡‰ç‰ˆ
ä½ æ˜¯è³‡æ·±ç·šä¸Šèª²ç¨‹è¨­è¨ˆå°ˆå®¶ï¼Œè² è²¬å°‡æ•™å­¸å½±ç‰‡è½‰åŒ–ç‚ºå°ˆæ¥­æ•™è‚²ç« ç¯€çµæ§‹ã€‚

{video_title_context}
# ğŸš¨ æœ€é‡è¦çš„è¦å‰‡ - æ™‚é–“æˆ³è¨˜å¿…é ˆç²¾æº–å°æ‡‰
**é€å­—ç¨¿å¯¦éš›æ™‚é–“ç¯„åœï¼š{first_ts} åˆ° {last_ts}**

## çµ•å°ç¦æ­¢çš„è¡Œç‚ºï¼š
âŒ ç”Ÿæˆ 00:00:00 ç« ç¯€ï¼ˆé™¤éé€å­—ç¨¿çœŸçš„å¾ 00:00:00 é–‹å§‹ï¼‰
âŒ è¦å¾‹æ™‚é–“é–“éš”ï¼šæ¯15åˆ†é˜ã€æ¯30åˆ†é˜ç­‰å›ºå®šæ¨¡å¼
âŒ æ†‘ç©ºæƒ³åƒæ™‚é–“é»ï¼ˆå¿…é ˆå°æ‡‰é€å­—ç¨¿ä¸­çš„å¯¦éš›æ™‚é–“æˆ³ï¼‰
âŒ å¿½ç•¥é€å­—ç¨¿çš„æ™‚é–“ç¯„åœ

## å¿…é ˆéµå®ˆçš„è¦å‰‡ï¼š
âœ… ç¬¬ä¸€å€‹ç« ç¯€æ™‚é–“ >= {first_ts}ï¼ˆé€å­—ç¨¿é–‹å§‹æ™‚é–“ï¼‰
âœ… æœ€å¾Œä¸€å€‹ç« ç¯€æ™‚é–“ <= {last_ts}ï¼ˆé€å­—ç¨¿çµæŸæ™‚é–“ï¼‰  
âœ… æ¯å€‹ç« ç¯€æ™‚é–“å¿…é ˆæ¥è¿‘é€å­—ç¨¿ä¸­å¯¦éš›è¨è«–è©²ä¸»é¡Œçš„æ™‚é–“æˆ³ï¼ˆÂ±60ç§’å…§ï¼‰
âœ… åŸºæ–¼å…§å®¹è‡ªç„¶è½‰æŠ˜é»ï¼Œè€Œéå›ºå®šé–“éš”

# å¦‚ä½•æ‰¾åˆ°çœŸå¯¦çš„ç« ç¯€è½‰æŠ˜é»ï¼š
## èªè¨€ä¿¡è™Ÿè©ï¼ˆè¬›å¸«è½‰æ›è©±é¡Œï¼‰ï¼š
- ã€Œæ¥ä¸‹ä¾†æˆ‘å€‘è¦è¬›...ã€ã€Œç¾åœ¨é€²å…¥...ã€ã€Œé¦–å…ˆ...ç¬¬äºŒ...ã€
- ã€Œæˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹...ã€ã€Œé€™å€‹éƒ¨åˆ†å®Œæˆå¾Œï¼Œæˆ‘å€‘ä¾†çœ‹...ã€
- ã€Œæœ‰äº†åŸºç¤æ¦‚å¿µï¼Œç¾åœ¨ä¾†å¯¦éš›æ“ä½œ...ã€
- ã€Œå•èˆ‡ç­”æ™‚é–“ã€ã€Œç¸½çµä¸€ä¸‹ã€ã€Œæˆ‘å€‘ä¾†ç·´ç¿’...ã€

## æ•™å­¸å…§å®¹è½‰æ›ï¼š
- æ–°æ¦‚å¿µ/æŠ€è¡“çš„é¦–æ¬¡è©³ç´°è§£é‡‹
- ç†è«–è¬›è§£ â†’ å¯¦éš›æ“ä½œçš„è½‰æ›
- ä¸åŒå·¥å…·/è»Ÿé«”çš„åˆ‡æ›æ™‚é–“é»
- ç¯„ä¾‹æ¼”ç¤ºçš„é–‹å§‹èˆ‡çµæŸ
- ç·´ç¿’é¡Œ/äº’å‹•ç’°ç¯€çš„é–‹å§‹

## è¦–è¦º/æ“ä½œè½‰æ›ï¼ˆåƒè€ƒOCRï¼‰ï¼š
- ç•«é¢åˆ‡æ›åˆ°æ–°æŠ•å½±ç‰‡/è»Ÿé«”ç•Œé¢
- é–‹å§‹å¯¦éš›æ“ä½œç¤ºç¯„
- æª”æ¡ˆé–‹å•Ÿ/å·¥å…·åˆ‡æ›çš„æ™‚é–“é»

# éŒ¯èª¤ç¤ºç¯„ vs æ­£ç¢ºåšæ³•ï¼š
## âŒ éŒ¯èª¤ï¼ˆçµ•å°é¿å…ï¼‰ï¼š
00:00:00 - èª²ç¨‹ä»‹ç´¹
00:15:00 - åŸºç¤æ¦‚å¿µ  
00:30:00 - é€²éšæ‡‰ç”¨
00:45:00 - å¯¦ä½œç·´ç¿’

## âœ… æ­£ç¢ºï¼ˆåŸºæ–¼å¯¦éš›å…§å®¹ï¼‰ï¼š
{first_ts} - èª²ç¨‹é–‹å ´èˆ‡å­¸ç¿’ç›®æ¨™èªªæ˜
[å°‹æ‰¾é€å­—ç¨¿ä¸­ç¬¬ä¸€å€‹ä¸»é¡Œè½‰æ›çš„æ™‚é–“æˆ³] - ç¬¬ä¸€å€‹ä¸»è¦æ¦‚å¿µè¬›è§£
[å°‹æ‰¾é€å­—ç¨¿ä¸­ç†è«–è½‰å¯¦ä½œçš„æ™‚é–“æˆ³] - å¯¦éš›æ“ä½œæ¼”ç¤ºé–‹å§‹
[å°‹æ‰¾é€å­—ç¨¿ä¸­é‡è¦ç¯„ä¾‹çš„æ™‚é–“æˆ³] - é—œéµç¯„ä¾‹åˆ†æ

# å½±ç‰‡è³‡è¨Š
- ç¸½æ™‚é•·: {duration_hms}
- é€å­—ç¨¿æ™‚é–“ç¯„åœ: {first_ts} åˆ° {last_ts}
- ç›®æ¨™ç« ç¯€: {t_low}-{t_high} å€‹å­¸ç¿’å–®å…ƒ
- æœ€å°é–“éš”: {min_gap_sec//60} åˆ†é˜

# åˆ†ææ­¥é©Ÿï¼š
1. **è­˜åˆ¥æ™‚é–“ç¯„åœ**ï¼šç¢ºèªé€å­—ç¨¿å¾ {first_ts} é–‹å§‹ï¼Œåˆ° {last_ts} çµæŸ
2. **é€šè®€å…§å®¹**ï¼šç†è§£æ•´é«”æ•™å­¸æµç¨‹å’ŒçŸ¥è­˜æ¶æ§‹
3. **æ¨™è¨˜è½‰æŠ˜**ï¼šæ‰¾å‡º {t_low}-{t_high} å€‹æœ€é‡è¦çš„ä¸»é¡Œè½‰æ›é»
4. **æ™‚é–“å°æ‡‰**ï¼šæ¯å€‹ç« ç¯€æ™‚é–“å¿…é ˆå°æ‡‰é€å­—ç¨¿ä¸­å¯¦éš›è¨è«–çš„æ™‚é–“
5. **æ¨™é¡Œç²¾æº–**ï¼šç”¨å…·é«”è¡“èªæè¿°è©²æ™‚é–“é»é–‹å§‹çš„æ•™å­¸å…§å®¹

# å…§å®¹è³‡æ–™
## ä¸»è¦é€å­—ç¨¿ï¼ˆåŒ…å«çœŸå¯¦æ™‚é–“æˆ³ï¼‰ï¼š
{transcript}

## è¼”åŠ©è¦–è¦ºå…§å®¹ï¼š
{ocr_context if ocr_context else "ï¼ˆç„¡è¢å¹•å…§å®¹åƒè€ƒï¼‰"}

# è¼¸å‡ºæ ¼å¼
## ç¬¬ä¸€éƒ¨åˆ†ï¼šç« ç¯€åˆ—è¡¨
åš´æ ¼éµå®ˆï¼š`HH:MM:SS - å…·é«”ç« ç¯€æ¨™é¡Œ`
- æ™‚é–“æˆ³å¿…é ˆæ˜¯é€å­—ç¨¿ä¸­å¯¦éš›å­˜åœ¨æˆ–éå¸¸æ¥è¿‘ï¼ˆÂ±60ç§’å…§ï¼‰çš„æ™‚é–“
- æ¨™é¡Œç”¨ç¹é«”ä¸­æ–‡ï¼Œå…·é«”æè¿°è©²æ™‚é–“é»é–‹å§‹çš„æ•™å­¸å…§å®¹

## ç¬¬äºŒéƒ¨åˆ†ï¼šèª²ç¨‹æ‘˜è¦ï¼ˆç« ç¯€åˆ—è¡¨å®Œæˆå¾Œï¼Œç©ºä¸€è¡Œè¼¸å‡ºï¼‰
è«‹æä¾›çµæ§‹åŒ–çš„èª²ç¨‹æ‘˜è¦ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

èª²ç¨‹ä¸»é¡Œï¼š[ä¸»è¦æ•™å­¸é ˜åŸŸï¼Œå¦‚ï¼šPythonç¨‹å¼è¨­è¨ˆã€Premiere Proå‰ªè¼¯]
æ ¸å¿ƒå…§å®¹ï¼š[åˆ—å‡º6-12å€‹ä¸»è¦æ•™å­¸æ¦‚å¿µï¼Œä»¥é “è™Ÿåˆ†éš”ï¼Œæ¶µè“‹æ•´å€‹èª²ç¨‹çš„é—œéµçŸ¥è­˜é»]
å­¸ç¿’ç›®æ¨™ï¼š[å­¸ç”Ÿå®Œæˆå¾Œæ‡‰å…·å‚™çš„èƒ½åŠ›]
é©åˆå°è±¡ï¼š[ç›®æ¨™å­¸å“¡èƒŒæ™¯]
é›£åº¦ç´šåˆ¥ï¼š[åˆç´š/ä¸­ç´š/é«˜ç´š]

# æœ€çµ‚æª¢æŸ¥
ç”Ÿæˆæ¯å€‹ç« ç¯€å‰ï¼Œå•è‡ªå·±ï¼š
1. é€™å€‹æ™‚é–“é»åœ¨é€å­—ç¨¿ä¸­æ˜¯å¦æœ‰å°æ‡‰çš„å…§å®¹è½‰æ›ï¼Ÿ
2. ç« ç¯€æ™‚é–“æ˜¯å¦åœ¨ {first_ts} åˆ° {last_ts} ç¯„åœå…§ï¼Ÿ
3. æ¨™é¡Œæ˜¯å¦æº–ç¢ºåæ˜ å¾é€™å€‹æ™‚é–“é»é–‹å§‹çš„æ•™å­¸å…§å®¹ï¼Ÿ

å®Œæˆç« ç¯€å¾Œï¼Œæª¢æŸ¥æ‘˜è¦ï¼š
1. èª²ç¨‹ä¸»é¡Œæ˜¯å¦æº–ç¢ºåæ˜ æ ¸å¿ƒæ•™å­¸å…§å®¹ï¼Ÿ
2. æ ¸å¿ƒå…§å®¹æ˜¯å¦åŒ…å«æœ€é‡è¦çš„2-3å€‹æŠ€è¡“é»ï¼Ÿ
3. å­¸ç¿’ç›®æ¨™æ˜¯å¦å…·é«”å¯è¡¡é‡ï¼Ÿ
"""
    return prompt

def build_educational_context(section_title: Optional[str], units: Optional[List[Dict]]) -> str:
    """
    Build educational context from metadata for prompt enhancement.
    
    Returns formatted string with course structure information.
    """
    if not section_title and not units:
        return ""
    
    context_parts = []
    
    if section_title:
        context_parts.append(f"# ğŸ“š èª²ç¨‹å–®å…ƒè³‡è¨Š")
        context_parts.append(f"æœ¬å½±ç‰‡å±¬æ–¼èª²ç¨‹å–®å…ƒï¼š**{section_title}**")
        context_parts.append("")
    
    if units:
        context_parts.append(f"## é å®šæ•™å­¸å–®å…ƒçµæ§‹ ({len(units)} å€‹å–®å…ƒ)")
        context_parts.append("è¬›å¸«è¨ˆåŠƒåœ¨æœ¬èª²ç¨‹ä¸­æ¶µè“‹ä»¥ä¸‹æ•™å­¸å–®å…ƒï¼š")
        context_parts.append("")
        for unit in units:
            context_parts.append(f"{unit['UnitNo']}. {unit['Title']}")
        
        context_parts.append("")
        context_parts.append("## ç« ç¯€è¨­è¨ˆæŒ‡å¼•")
        context_parts.append("âœ… å„ªå…ˆè€ƒæ…®é€™äº›é å®šå–®å…ƒä½œç‚ºä¸»è¦ç« ç¯€åˆ†çµ„")
        context_parts.append("âœ… åœ¨é€å­—ç¨¿ä¸­å°‹æ‰¾è¬›å¸«å¯¦éš›è¬›è§£é€™äº›å–®å…ƒçš„æ™‚é–“é»")
        context_parts.append(f"âœ… ç›®æ¨™ï¼šå‰µå»º {len(units) * 2} åˆ° {len(units) * 4} å€‹ç« ç¯€")
        context_parts.append("âœ… ç« ç¯€æ¨™é¡Œå»ºè­°æ ¼å¼ï¼š[å–®å…ƒNï¼šå–®å…ƒåç¨±] å…·é«”å…§å®¹")
        context_parts.append("")
        context_parts.append("**ç¯„ä¾‹æ ¼å¼ï¼š**")
        context_parts.append("00:05:30 - [å–®å…ƒ1ï¼šå»šå…·è¦åŠƒ] å»šæˆ¿å·¥ä½œä¸‰è§’åŸç†èˆ‡å‹•ç·šè¨­è¨ˆ")
        context_parts.append("00:18:45 - [å–®å…ƒ1ï¼šå»šå…·è¦åŠƒ] å»šå…·å°ºå¯¸æ¨™æº–èˆ‡äººé«”å·¥å­¸è€ƒé‡")
        context_parts.append("00:32:10 - [å–®å…ƒ2ï¼šå¤©èŠ±æ¿å¤§æ¨£åœ–] å¤§æ¨£åœ–ç¹ªè£½åŸºæœ¬è¦ç¯„èˆ‡åœ–ä¾‹èªªæ˜")
        context_parts.append("")
    
    return "\n".join(context_parts)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Hierarchical Multi-Pass Generation (NEW)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def should_use_hierarchical(duration: float, transcript_length: int) -> bool:
    """Determine if hierarchical multi-pass should be used"""
    # Use hierarchical for longer, content-rich educational videos
    return (duration >= 1800 and  # 30+ minutes
            transcript_length >= 5000 and  # Substantial content
            duration <= 14400)  # Under 4 hours (very long videos might need different handling)

def hierarchical_multipass_generation(
    raw_asr_text: str,
    duration: float,
    ocr_context: str,
    video_title: Optional[str],
    section_title: Optional[str],      # â† ADD
    units: Optional[List[Dict]],       # â† ADD
    client: Any,
    config: ChapterConfig,
    progress_callback: Optional[Callable[[str, int], None]] = None
) -> Tuple[str, Dict[str, str], Dict[str, Any]]:
    """
    Three-pass hierarchical generation for high-quality educational chapters.
    
    Strategy:
    - ASR (Primary): Provides teaching content, explanations, natural timing, and narrative flow
    - OCR (Supporting): Provides visual structure, precise terminology, and organized summaries
    
    Prioritization:
    - Chapter timing: ASR timestamps (when instructor announces topics)
    - Chapter titles: ASR content enriched with OCR terminology
    - Q&A content: ASR explanations supplemented with OCR structured data

    
    Token Budget:
    - ASR: 100,000 tokens per pass (primary source)
    - OCR: 15,000 tokens per pass (supporting detail)
    - Total: ~115,000 content + ~2,000 instructions = ~117,000 tokens (safe for GPT-4o's 128k context)
    
    Returns: (raw_llm_text, chapters, metadata)
    """
    
    # ==================== Token Budget Initialization ====================
    ASR_LIMIT = 100_000   # ASR transcript limit per pass
    OCR_LIMIT = 15_000    # OCR context limit per pass
    
    asr_tokens = count_tokens_llama(raw_asr_text)
    ocr_tokens = count_tokens_llama(ocr_context) if ocr_context else 0
    total_content_tokens = asr_tokens + ocr_tokens
    
    logger.info("=" * 60)
    logger.info("ğŸ“ HIERARCHICAL MULTI-PASS CHAPTER GENERATION")
    logger.info("   Strategy: ASR-primary (timing) + OCR-supporting (detail)")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Original ASR tokens: {asr_tokens:,} (limit: {ASR_LIMIT:,})")
    logger.info(f"ğŸ“Š Original OCR tokens: {ocr_tokens:,} (limit: {OCR_LIMIT:,})")
    logger.info(f"ğŸ“Š Total content tokens: {total_content_tokens:,}")
    logger.info(f"ğŸ“Š Video duration: {sec_to_hms(int(duration))}")
    
    # Truncate once, reuse in all passes for consistency
    asr_text = truncate_text_by_tokens(raw_asr_text, ASR_LIMIT)
    ocr_text = truncate_text_by_tokens(ocr_context, OCR_LIMIT) if ocr_context else ""

    asr_ts_sorted = extract_asr_timestamps_sorted(raw_asr_text)  # use RAW, not truncated
    asr_end_ts = asr_ts_sorted[-1] if asr_ts_sorted else sec_to_hms(int(duration))
    asr_end_sec = ts_to_seconds_hms(asr_end_ts)
    if asr_end_sec <= 0:
        asr_end_sec = int(duration)
        asr_end_ts = sec_to_hms(asr_end_sec)
    anchors = pick_anchor_timestamps(asr_ts_sorted, k=14)

    logger.info(f"ğŸ§¾ ASR time coverage: first={asr_ts_sorted[0] if asr_ts_sorted else 'N/A'} "
                f"last={asr_end_ts} (asr_ts_count={len(asr_ts_sorted)})")
    logger.info(f"ğŸ§· Anchor timestamps (spread): {anchors}")

    
    asr_used = count_tokens_llama(asr_text)
    ocr_used = count_tokens_llama(ocr_text)
    content_used = asr_used + ocr_used
    
    asr_coverage = (asr_used / asr_tokens * 100) if asr_tokens > 0 else 100
    ocr_coverage = (ocr_used / ocr_tokens * 100) if ocr_tokens > 0 else 100
    
    logger.info(f"âœ… Using per pass:")
    logger.info(f"   â€¢ ASR: {asr_used:,} tokens ({asr_coverage:.1f}% of original)")
    logger.info(f"   â€¢ OCR: {ocr_used:,} tokens ({ocr_coverage:.1f}% of original)")
    logger.info(f"   â€¢ Total: {content_used:,} tokens")
    
    if asr_tokens > ASR_LIMIT:
        logger.warning(f"âš ï¸ ASR truncated from {asr_tokens:,} to {asr_used:,} tokens")
    if ocr_tokens > OCR_LIMIT:
        logger.warning(f"âš ï¸ OCR truncated from {ocr_tokens:,} to {ocr_used:,} tokens")
    
    # ==================== PASS 1: Course Structure Analysis ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ” PASS 1: Course Structure Analysis")
    logger.info("   Goal: Identify learning objectives and overall architecture")
    logger.info("   Approach: Analyze both ASR and OCR equally")
    logger.info("-" * 60)
    
    if progress_callback:
        progress_callback("analyzing_course_structure", 40)
    
    video_info = ""
    if video_title:
        clean_title = re.sub(r'\.(mp4|avi|mov|mkv|webm|flv|m4v)$', '', video_title, flags=re.IGNORECASE)
        video_info = f"èª²ç¨‹æª”åï¼š{clean_title}\n"
        logger.info(f"ğŸ“š Video title: {clean_title}")
    
    # â† ADD THIS LOGGING BLOCK
    if section_title or units:
        logger.info("=" * 60)
        logger.info("ğŸ“š EDUCATIONAL METADATA PROVIDED")
        if section_title:
            logger.info(f"   ğŸ“– Section: {section_title}")
        if units:
            logger.info(f"   ğŸ“‘ Units: {len(units)} predefined learning units")
            for unit in units:
                logger.info(f"      {unit['UnitNo']}. {unit['Title']}")
        logger.info("=" * 60)
    
    educational_context = build_educational_context(section_title, units)
    
    structure_prompt = f"""
ä½œç‚ºè³‡æ·±æ•™å­¸è¨­è¨ˆå°ˆå®¶ï¼Œåˆ†æé€™å€‹{sec_to_hms(int(duration))}æ•™å­¸å½±ç‰‡çš„æ•´é«”æ¶æ§‹ï¼š

{video_info}
{educational_context}

ã€æ ¸å¿ƒå­¸ç¿’ç›®æ¨™ã€‘
1. å­¸ç”Ÿå®Œæˆæœ¬èª²ç¨‹å¾Œæ‡‰æŒæ¡å“ªäº›é—œéµèƒ½åŠ›ï¼Ÿ
2. æœ‰å“ªäº›å¿…é ˆç†è§£çš„æ ¸å¿ƒç†è«–æˆ–æ¦‚å¿µï¼Ÿ
3. æœ‰å“ªäº›éœ€è¦ç†Ÿç·´çš„å¯¦ç”¨æŠ€èƒ½ï¼Ÿ

ã€çŸ¥è­˜æ¶æ§‹åˆ†æã€‘
- åŸºç¤é‹ªé™³ï¼šå“ªäº›æ˜¯å‰æçŸ¥è­˜æˆ–åŸºç¤æ¦‚å¿µï¼Ÿ
- æ ¸å¿ƒæ•™å­¸ï¼šæœ€é‡è¦çš„ç†è«–/æ–¹æ³•/æŠ€è¡“æ˜¯ä»€éº¼ï¼Ÿ
- æ‡‰ç”¨å»¶ä¼¸ï¼šå¦‚ä½•å°‡æ‰€å­¸æ‡‰ç”¨æ–¼å¯¦éš›å ´æ™¯ï¼Ÿ
- ç¸½çµæ•´åˆï¼šå¦‚ä½•å°‡é›¶æ•£çŸ¥è­˜ç³»çµ±åŒ–ï¼Ÿ

ã€æ•™å­¸æ–¹æ³•è­˜åˆ¥ã€‘
- ç†è«–è¬›è§£ vs. å¯¦ä¾‹æ¼”ç¤º vs. æ“ä½œç·´ç¿’ çš„æ¯”ä¾‹åˆ†ä½ˆ
- æ˜¯å¦æœ‰å•ç­”äº’å‹•ã€æ€è€ƒé¡Œã€é‡é»å›é¡§ï¼Ÿ

ã€åˆ†æè¦é»ã€‘
- å¾è¬›å¸«çš„æ•™å­¸æ•˜è¿°ï¼ˆASRï¼‰ç†è§£æ•™å­¸é‚è¼¯å’Œé‡é»
- å¾æŠ•å½±ç‰‡å…§å®¹ï¼ˆOCRï¼‰è­˜åˆ¥ä¸»è¦ç« ç¯€çµæ§‹å’Œå°ˆæ¥­è¡“èª
- ç¶œåˆå…©è€…ï¼Œå»ºæ§‹å®Œæ•´çš„èª²ç¨‹æ¡†æ¶

å®Œæ•´é€å­—ç¨¿ï¼ˆè¬›å¸«æ•™å­¸å…§å®¹èˆ‡æ™‚é–“è»¸ï¼‰ï¼š
{asr_text}

è¦–è¦ºè¼”åŠ©å…§å®¹ï¼ˆæŠ•å½±ç‰‡/è¢å¹•æ–‡å­—ï¼Œç”¨æ–¼ç¢ºèªä¸»é¡Œèˆ‡è¡“èªï¼‰ï¼š
{ocr_text if ocr_text else "ç„¡è¦–è¦ºè¼”åŠ©å…§å®¹"}
"""
    
    logger.info(f"ğŸ“¤ PASS 1 prompt: ~{count_tokens_llama(structure_prompt):,} tokens")
    logger.info("ğŸ¤– Calling LLM for structure analysis...")
    t0 = time.time()
    
    try:
        structure_response = call_llm(
            service_type=config.service_type,
            client=client,
            system_message="ä½ æ˜¯èª²ç¨‹æ¶æ§‹åˆ†æå°ˆå®¶ï¼Œæ“…é•·è­˜åˆ¥æ•™å­¸å½±ç‰‡çš„æ•´é«”å­¸ç¿’ç›®æ¨™å’ŒçŸ¥è­˜é«”ç³»ã€‚ä½ æœƒç¶œåˆåˆ†æè¬›å¸«è¬›è§£ï¼ˆASRï¼‰å’ŒæŠ•å½±ç‰‡å…§å®¹ï¼ˆOCRï¼‰ä¾†ç†è§£èª²ç¨‹çš„å®Œæ•´çµæ§‹ã€‚",
            user_message=structure_prompt,
            model=config.openai_model if config.service_type == "openai" else config.azure_model,
            max_tokens=1200,
            temperature=0.3
        )
        
        elapsed = time.time() - t0
        logger.info(f"âœ… PASS 1 completed in {elapsed:.1f}s")
        
        structure_text = (structure_response.choices[0].message.content 
                         if config.service_type == "openai" 
                         else structure_response.choices[0].message.content)
        
        logger.info(f"ğŸ“ Structure analysis: {len(structure_text)} characters")
        
    except Exception as e:
        logger.error(f"âŒ PASS 1 failed: {e}", exc_info=True)
        raise
    
    # ==================== PASS 2: Learning Modules Identification ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ“š PASS 2: Learning Modules Identification")
    logger.info("   Goal: Break down course into 7-12 coherent learning units")
    logger.info("   Approach: ASR-primary (conceptual transitions)")
    logger.info("-" * 60)
    
    if progress_callback:
        progress_callback("identifying_learning_modules", 60)
    
    modules_prompt = f"""
åŸºæ–¼èª²ç¨‹çµæ§‹åˆ†æï¼š
{structure_text}

{educational_context}

ã€èˆ‡é å®šå–®å…ƒçš„å°æ‡‰é—œä¿‚ã€‘
å¦‚æœæä¾›äº†é å®šæ•™å­¸å–®å…ƒï¼Œè«‹ç‰¹åˆ¥æ³¨æ„ï¼š
- è¬›å¸«å¦‚ä½•åœ¨å¯¦éš›æ•™å­¸ä¸­æ¶µè“‹é€™äº›é å®šå–®å…ƒ
- å¯¦éš›æ•™å­¸æ¨¡å¡Šèˆ‡é å®šå–®å…ƒçš„å°æ‡‰é—œä¿‚
- å¯èƒ½çš„å–®å…ƒåˆä½µã€æ‹†åˆ†æˆ–é †åºèª¿æ•´

ç¾åœ¨è­˜åˆ¥å…·é«”çš„å­¸ç¿’æ¨¡å¡Šï¼ˆ7-12å€‹ï¼‰ï¼Œæ¯å€‹æ¨¡å¡Šæ‡‰æ»¿è¶³ï¼š
1. æœ‰æ˜ç¢ºçš„å­¸ç¿’ç›®æ¨™
2. åŒ…å«å®Œæ•´çš„æ•™å­¸é–‰ç’°ï¼ˆè¬›è§£â†’ç¯„ä¾‹â†’ç·´ç¿’ï¼‰
3. æ™‚é•·åˆç†ï¼ˆ10-30åˆ†é˜ï¼‰
4. æœ‰æ¸…æ™°çš„é–‹å§‹å’ŒçµæŸæ¨™è¨˜

ã€æ¨¡å¡Šé‚Šç•Œè­˜åˆ¥ç­–ç•¥ï¼ˆé‡è¦æ€§æ’åºï¼‰ã€‘

**ç¬¬ä¸€å„ªå…ˆï¼šè¬›å¸«çš„é‡å¤§ä¸»é¡Œè½‰æ›ï¼ˆASR - ä¸»è¦ä¾æ“šï¼‰**
- æ˜ç¢ºçš„ç« ç¯€å®£å‘Šï¼š"æ¥ä¸‹ä¾†é€²å…¥æ–°çš„ç« ç¯€/éƒ¨åˆ†"ã€"ç¬¬ä¸€éƒ¨åˆ†/ç¬¬äºŒéƒ¨åˆ†"
- é‡å¤§å…§å®¹è½‰æ›ï¼š"åŸºç¤/ç†è«–è¬›å®Œäº†ï¼Œç¾åœ¨ä¾†çœ‹..."ã€"å¾æ¦‚å¿µåˆ°å¯¦è¸"
- æ•™å­¸æ–¹æ³•çš„é‡å¤§è½‰è®Šï¼šç†è«–è¬›è§£ â†’ å¯¦éš›æ“ä½œ â†’ æ¡ˆä¾‹åˆ†æ
- è¬›å¸«çš„ç¸½çµèˆ‡éæ¸¡ï¼š"æˆ‘å€‘å‰›æ‰è¬›äº†...ï¼Œç¾åœ¨ä¾†çœ‹..."

**ç¬¬äºŒå„ªå…ˆï¼šæ•™å­¸é‚è¼¯çš„çµæ§‹è½‰æ›ï¼ˆASRå…§å®¹åˆ†æï¼‰**
- å¾ç°¡å–®åˆ°è¤‡é›œçš„æ˜é¡¯å±¤ç´šè®ŠåŒ–
- å¾å–®ä¸€å·¥å…·/æ¦‚å¿µåˆ°ç¶œåˆæ‡‰ç”¨
- å¾è¬›è§£åˆ°ç·´ç¿’çš„è½‰æ›
- éšæ®µæ€§ç¸½çµå¾Œé–‹å§‹æ–°ä¸»é¡Œ

**ç¬¬ä¸‰å„ªå…ˆï¼šè¦–è¦ºçµæ§‹è®ŠåŒ–ï¼ˆOCR - è¼”åŠ©åƒè€ƒï¼‰**
- æŠ•å½±ç‰‡çš„å¤§æ¨™é¡Œè®ŠåŒ–ï¼ˆç« ç¯€ç·¨è™Ÿã€å¤§æ®µè½æ¨™è¨˜ï¼‰
- é¡¯è‘—çš„å…§å®¹é¡å‹åˆ‡æ›ï¼ˆç†è«–æŠ•å½±ç‰‡ â†’ è»Ÿé«”æ“ä½œç•Œé¢ â†’ å¯¦ä¾‹æ¼”ç¤ºï¼‰
- ç”¨æ–¼ç¢ºèªæ¨¡å¡Šçš„ä¸»é¡Œåç¨±å’Œå°ˆæ¥­è¡“èª

âš ï¸ é‡é»æé†’ï¼š
- æ¨¡å¡Šæ˜¯å¤§çš„å­¸ç¿’å–®å…ƒï¼Œä¸è¦è¢«é »ç¹çš„å°æ¨™é¡Œè®ŠåŒ–èª¤å°
- å–®å€‹æŠ•å½±ç‰‡è®ŠåŒ–ä¸ç­‰æ–¼æ¨¡å¡Šé‚Šç•Œ
- å„ªå…ˆé—œæ³¨è¬›å¸«çš„èªè¨€ä¿¡è™Ÿï¼ŒæŠ•å½±ç‰‡ç”¨æ–¼ç¢ºèªä¸»é¡Œ

å®Œæ•´é€å­—ç¨¿ï¼ˆä¸»è¦ä¾æ“š - åŒ…å«è¬›å¸«çš„ä¸»é¡Œè½‰æ›ä¿¡è™Ÿï¼‰ï¼š
{asr_text}

è¦–è¦ºè¼”åŠ©å…§å®¹ï¼ˆæ¬¡è¦åƒè€ƒ - ç”¨æ–¼ç¢ºèªä¸»é¡Œåç¨±ï¼‰ï¼š
{ocr_text if ocr_text else "ç„¡è¦–è¦ºè¼”åŠ©å…§å®¹"}

è«‹è¼¸å‡ºæ ¼å¼ï¼š
æ¨¡å¡Šåç¨± ~ èµ·å§‹æ™‚é–“æˆ³(HH:MM:SS) ~ çµæŸæ™‚é–“æˆ³(HH:MM:SS) ~ æ ¸å¿ƒå­¸ç¿’é» ~ æ•™å­¸æ–¹æ³•
èµ·å§‹æ™‚é–“æˆ³å¿…é ˆæ˜¯é€å­—ç¨¿ä¸­å‡ºç¾éçš„æ™‚é–“æˆ³ä¹‹ä¸€

ç¯„ä¾‹ï¼ˆæ³¨æ„ï¼šæ™‚é–“æˆ³å¿…é ˆæ˜¯ HH:MM:SSï¼‰ï¼š
åŸºç¤å·¥å…·æ“ä½œ ~ 00:05:24 ~ 00:18:45 ~ ä»‹é¢èªè­˜ã€åŸºæœ¬å·¥å…·ä½¿ç”¨ ~ ç†è«–è¬›è§£+å¯¦ä¾‹æ¼”ç¤º
é€²éšè¨­è¨ˆæŠ€å·§ ~ 00:18:45 ~ 00:36:10 ~ è‰²å½©ç®¡ç†ã€è¼¸å‡ºè¨­å®š ~ ç¶œåˆæ¡ˆä¾‹+å¯¦éš›æ“ä½œ
"""
    
    logger.info(f"ğŸ“¤ PASS 2 prompt: ~{count_tokens_llama(modules_prompt):,} tokens")
    logger.info("ğŸ¤– Calling LLM for module identification...")
    t0 = time.time()
    
    try:
        modules_response = call_llm(
            service_type=config.service_type,
            client=client,
            system_message="ä½ æ˜¯èª²ç¨‹æ¨¡å¡Šè¨­è¨ˆå¸«ï¼Œæ“…é•·å°‡æ•™å­¸å…§å®¹åˆ†è§£ç‚ºé‚è¼¯é€£è²«çš„å­¸ç¿’å–®å…ƒã€‚ä½ ä¸»è¦ä¾æ“šè¬›å¸«çš„èªè¨€ä¿¡è™Ÿï¼ˆASRï¼‰ä¾†è­˜åˆ¥æ¨¡å¡Šé‚Šç•Œï¼Œå› ç‚ºæ¨¡å¡Šæ˜¯åŸºæ–¼æ¦‚å¿µè½‰æ›è€Œéè¦–è¦ºè®ŠåŒ–ã€‚æŠ•å½±ç‰‡ï¼ˆOCRï¼‰ä¸»è¦ç”¨æ–¼ç¢ºèªæ¨¡å¡Šçš„ä¸»é¡Œåç¨±ã€‚",
            user_message=modules_prompt,
            model=config.openai_model if config.service_type == "openai" else config.azure_model,
            max_tokens=1500,
            temperature=0.2
        )
        
        elapsed = time.time() - t0
        logger.info(f"âœ… PASS 2 completed in {elapsed:.1f}s")
        
        modules_text = (modules_response.choices[0].message.content 
                       if config.service_type == "openai" 
                       else modules_response.choices[0].message.content)
        
        logger.info(f"ğŸ“ Modules analysis: {len(modules_text)} characters")
        
    except Exception as e:
        logger.error(f"âŒ PASS 2 failed: {e}", exc_info=True)
        raise
    
    # ==================== PASS 3: Detailed Chapter Generation ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ“‘ PASS 3: Detailed Chapter Generation")
    logger.info("   Goal: Create 15-30 precise chapter timestamps with titles")
    logger.info("   Approach: ASR-primary (timing) + OCR-supporting (detail)")
    logger.info("-" * 60)
    
    if progress_callback:
        progress_callback("generating_detailed_chapters", 80)
    
    chapters_prompt = f"""
ã€èª²ç¨‹æ•´é«”çµæ§‹ã€‘
{structure_text}

ã€å­¸ç¿’æ¨¡å¡Šè¦åŠƒã€‘  
{modules_text}

{educational_context}

ã€å–®å…ƒå°æ‡‰å»ºè­°ï¼ˆå¦‚é©ç”¨ï¼‰ã€‘
å¦‚æœæä¾›äº†é å®šæ•™å­¸å–®å…ƒï¼Œç« ç¯€æ¨™é¡Œå¯ä½¿ç”¨æ ¼å¼ï¼š
- [å–®å…ƒNï¼šå–®å…ƒåç¨±] å…·é«”ç« ç¯€å…§å®¹
- é€™æ¨£å¯ä»¥å¹«åŠ©å­¸ç”Ÿç†è§£ç« ç¯€èˆ‡æ•´é«”èª²ç¨‹çµæ§‹çš„é—œä¿‚

ç¾åœ¨ç‚ºæ¯å€‹æ¨¡å¡Šç”Ÿæˆå…·é«”çš„ç« ç¯€æ™‚é–“é»ï¼ˆç¸½å…±15-30å€‹ç« ç¯€ï¼‰ï¼Œä¸¦æä¾›èª²ç¨‹æ‘˜è¦ã€‚

ã€ç« ç¯€è¨­è¨ˆåŸå‰‡ã€‘
1. æ¯å€‹ç« ç¯€ä»£è¡¨ä¸€å€‹å®Œæ•´çš„å­¸ç¿’å­ç›®æ¨™ï¼ˆ5-10åˆ†é˜ï¼‰
2. æ¨™è¨˜é—œéµæ¦‚å¿µçš„é¦–æ¬¡è©³ç´°è§£é‡‹
3. æ¨™è¨˜é‡è¦ç¯„ä¾‹æˆ–æ¡ˆä¾‹åˆ†æçš„é–‹å§‹
4. æ¨™è¨˜ç·´ç¿’é¡Œæˆ–äº’å‹•ç’°ç¯€
5. æ¨™è¨˜é‡é»å›é¡§æˆ–ç¸½çµè™•

ã€ç« ç¯€æ™‚é–“é»å®šä½ç­–ç•¥ï¼ˆé‡è¦æ€§æ’åºï¼‰ã€‘

**ç¬¬ä¸€å„ªå…ˆï¼šASRèªè¨€æ™‚é–“æˆ³ï¼ˆä¸»è¦ä¾æ“š - æ±ºå®šç« ç¯€é–‹å§‹æ™‚é–“ï¼‰**
- è¬›å¸«çš„æ˜ç¢ºä¸»é¡Œå®£å‘Šï¼š"æ¥ä¸‹ä¾†æˆ‘å€‘è¦è¬›..."ã€"ç¾åœ¨é€²å…¥..."ã€"é¦–å…ˆ..."
- æ•™å­¸è½‰æ›ä¿¡è™Ÿï¼š"å¥½çš„ï¼Œé€™éƒ¨åˆ†å®Œæˆäº†"ã€"ç¾åœ¨ä¾†çœ‹..."ã€"æˆ‘å€‘ä¾†ç¤ºç¯„..."
- é‡è¦æ¦‚å¿µçš„é¦–æ¬¡è©³ç´°è§£é‡‹é–‹å§‹é»
- å¯¦ä¾‹æ¼”ç¤ºçš„æ˜ç¢ºé–‹å§‹ï¼š"æˆ‘å€‘ä¾†å¯¦éš›æ“ä½œä¸€ä¸‹..."
- ç·´ç¿’æˆ–äº’å‹•çš„é–‹å§‹ï¼š"å¤§å®¶è©¦è©¦çœ‹..."

**ç¬¬äºŒå„ªå…ˆï¼šå…§å®¹é‚è¼¯è½‰æ›ï¼ˆASRå…§å®¹åˆ†æï¼‰**
- å¾ç†è«–è¬›è§£åˆ°å¯¦éš›æ¼”ç¤ºçš„è‡ªç„¶è½‰æ›é»
- æ–°å·¥å…·/æŠ€è¡“çš„é¦–æ¬¡è©³ç´°ä»‹ç´¹
- å¾ç°¡å–®ç¯„ä¾‹åˆ°è¤‡é›œæ‡‰ç”¨çš„éæ¸¡
- éšæ®µæ€§å°çµå¾Œé–‹å§‹æ–°çš„å­ä¸»é¡Œ

**ç¬¬ä¸‰å„ªå…ˆï¼šOCRè¦–è¦ºè¼”åŠ©ï¼ˆè£œå……ç¢ºèª - æä¾›ç« ç¯€æ¨™é¡Œç´°ç¯€ï¼‰**
- ç¢ºèªç•¶å‰è¨è«–çš„å…·é«”ä¸»é¡Œï¼ˆæŠ•å½±ç‰‡æ¨™é¡Œæä¾›æº–ç¢ºåç¨±ï¼‰
- æä¾›ç²¾ç¢ºçš„æŠ€è¡“è¡“èªï¼ˆç•¶è¬›å¸«èªª"é€™å€‹å·¥å…·"æ™‚ï¼ŒOCRé¡¯ç¤º"çŸ©å½¢å·¥å…·"ï¼‰
- è£œå……è¦–è¦ºå…§å®¹æè¿°ï¼ˆåœ–è¡¨æ¨™é¡Œã€ä»£ç¢¼ç‰‡æ®µã€æ“ä½œæ­¥é©Ÿï¼‰
- ç•¶ASRè¡¨è¿°ä¸å¤ æ˜ç¢ºæ™‚ï¼Œåƒè€ƒè¢å¹•å…§å®¹ä¾†è£œå……ç´°ç¯€

ã€æ¨™é¡Œå‘½åè¦ç¯„ã€‘
- **å„ªå…ˆä½¿ç”¨è¬›å¸«çš„è‡ªç„¶è¡¨è¿°**ï¼ˆä¾†è‡ªASRï¼Œæ›´å£èªåŒ–ã€æ˜“æ‡‚ï¼‰
- **çµåˆæŠ•å½±ç‰‡çš„å°ˆæ¥­è¡“èª**ï¼ˆä¾†è‡ªOCRï¼Œæä¾›æº–ç¢ºçš„æŠ€è¡“åç¨±ï¼‰
- ä½¿ç”¨å…·é«”ã€å¯æ“ä½œçš„æè¿°ï¼ˆé¿å…"ä»‹ç´¹"ã€"èªªæ˜"ç­‰æ¨¡ç³Šè©å½™ï¼‰
- åŒ…å«æ‰€å±¬æ¨¡å¡Šæ¨™ç±¤ï¼ˆå¦‚ï¼š[åŸºç¤å·¥å…·]ã€[é€²éšæŠ€å·§]ã€[å¯¦æˆ°æ¡ˆä¾‹]ï¼‰
- æ¨™é¡Œæ ¼å¼ï¼š[æ¨¡å¡Šæ¨™ç±¤] å‹•ä½œ/å°è±¡/ç›®æ¨™

ã€æ™‚é–“é»é¸æ“‡çš„é»ƒé‡‘åŸå‰‡ã€‘
âš ï¸ ASRæ™‚é–“æˆ³è¨˜éŒ„äº†è¬›å¸«å¯¦éš›é–‹å§‹è¬›è§£æ–°ä¸»é¡Œçš„æ™‚é–“ - é€™æ˜¯æœ€è‡ªç„¶ã€æœ€ç¬¦åˆå­¸ç¿’ç¯€å¥çš„ç« ç¯€èµ·é»
âš ï¸ æŠ•å½±ç‰‡ï¼ˆOCRï¼‰é€šå¸¸åœ¨è¬›å¸«å®£å‘Šä¸»é¡Œä¹‹å¾Œæ‰å‡ºç¾ï¼Œç”¨æ–¼ç¢ºèªå…§å®¹å’Œæä¾›è¡“èªï¼Œè€Œéæ±ºå®šæ™‚é–“é»
âš ï¸ å„ªå…ˆé¸æ“‡è¬›å¸«æ˜ç¢ºå®£å‘Šæ–°ä¸»é¡Œçš„æ™‚é–“é»ï¼ˆå¾ASRï¼‰ä½œç‚ºç« ç¯€é–‹å§‹æ™‚é–“
âš ï¸ ä½¿ç”¨æŠ•å½±ç‰‡å…§å®¹ï¼ˆå¾OCRï¼‰ä¾†è±å¯Œå’Œç²¾ç¢ºåŒ–ç« ç¯€æ¨™é¡Œ

å®Œæ•´é€å­—ç¨¿ï¼ˆå«ç²¾ç¢ºæ™‚é–“æˆ³ - ä¸»è¦ç”¨æ–¼ç¢ºå®šç« ç¯€æ™‚é–“ï¼‰ï¼š
{asr_text}

è¦–è¦ºè¼”åŠ©å…§å®¹ï¼ˆæŠ•å½±ç‰‡/è¢å¹•æ–‡å­— - ä¸»è¦ç”¨æ–¼è±å¯Œç« ç¯€æ¨™é¡Œï¼‰ï¼š
{ocr_text if ocr_text else "ç„¡è¦–è¦ºè¼”åŠ©å…§å®¹"}

ç¸½æ™‚é•·ï¼š{sec_to_hms(int(duration))}

ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ï¼ˆé‡è¦ï¼šåªè¼¸å‡º JSONï¼Œä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ã€‘
è«‹è¼¸å‡ºä¸€å€‹ JSON ç‰©ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

{PASS3_JSON_SCHEMA}

{{ 
  "SuggestedUnits": [
    {{ 
      "UnitNo": 1,
      "ParentUnitNo": null,
      "Title": "ç« ç¯€æ¨™é¡Œï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
      "Time": "HH:MM:SS",
      "ClientUnitNo": 2,
      "ClientUnitTitle": "ï¼ˆå°æ‡‰åˆ°å®¢æˆ¶æä¾›çš„ Units ä¸­è©² UnitNo çš„ Titleï¼‰"
    }}
  ],
  "CourseSummary": {{
    "topic": "...",
    "core_content": "...",
    "learning_objectives": "...",
    "target_audience": "...",
    "difficulty": "..."
  }}
}}

è¦å‰‡ï¼š
1) Time å¿…é ˆæ˜¯é€å­—ç¨¿ä¸­å­˜åœ¨æˆ–éå¸¸æ¥è¿‘ï¼ˆÂ±60 ç§’å…§ï¼‰çš„ HH:MM:SS
2) SuggestedUnits éœ€ä¾ Time éå¢æ’åº
3) è‹¥æœ‰æä¾›å®¢æˆ¶ Unitsï¼š
   - æ¯ä¸€å€‹ SuggestedUnit å¿…é ˆåŒ…å« ClientUnitNoï¼ˆå¿…é ˆç­‰æ–¼ Units è£¡æŸå€‹ UnitNoï¼‰
   - æ¯ä¸€å€‹ SuggestedUnit å¿…é ˆåŒ…å« ClientUnitTitleï¼ˆå¿…é ˆèˆ‡è©² UnitNo çš„ Title ç›¸åŒæˆ–éå¸¸æ¥è¿‘ï¼‰
   - ParentUnitNo åƒ…ç”¨æ–¼ç« ç¯€éšå±¤ï¼ˆå¯é¸ï¼‰ï¼Œä¸å¾—ç”¨ä¾†è¡¨ç¤º ClientUnitNo
4) è‹¥æœªæä¾› Unitsï¼šClientUnitNo èˆ‡ ClientUnitTitle å…è¨±ç‚º null æˆ–çœç•¥
5) åªè¼¸å‡º JSONï¼Œç¦æ­¢ ```ã€ç¦æ­¢å¤šé¤˜è§£é‡‹ã€ç¦æ­¢æ¢åˆ—æ–‡å­—
"""
    
    logger.info(f"ğŸ“¤ PASS 3 prompt: ~{count_tokens_llama(chapters_prompt):,} tokens")
    logger.info("ğŸ¤– Calling LLM for final chapter generation...")
    t0 = time.time()
    
    try:
        final_response = call_llm(
            service_type=config.service_type,
            client=client,
            system_message=(
                "ä½ æ˜¯ç´°å¿ƒçš„ç« ç¯€è¨­è¨ˆå¸«ã€‚"
                "ç« ç¯€æ™‚é–“ä»¥ ASR æ™‚é–“æˆ³ç‚ºæº–ï¼Œç« ç¯€æ¨™é¡Œå¯ç”¨ OCR è£œå……å°ˆæ¥­è¡“èªã€‚"
                "è«‹åªè¼¸å‡ºä¸€å€‹ JSON ç‰©ä»¶ï¼Œä¸” JSON å¿…é ˆåŒ…å« SuggestedUnits èˆ‡ CourseSummaryã€‚"
                "ç¦æ­¢è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€ç¦æ­¢ ```ã€‚"
            ),
            user_message=chapters_prompt,
            model=config.openai_model if config.service_type == "openai" else config.azure_model,
            max_tokens=3000,
            temperature=0.1
        )
        
        elapsed = time.time() - t0
        logger.info(f"âœ… PASS 3 completed in {elapsed:.1f}s")
        
        final_text = (final_response.choices[0].message.content 
                     if config.service_type == "openai" 
                     else final_response.choices[0].message.content)
        
        logger.info(f"ğŸ“ Final output: {len(final_text)} characters")
        
    except Exception as e:
        logger.error(f"âŒ PASS 3 failed: {e}", exc_info=True)
        raise
    
    # ==================== Parse Results ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ” Parsing Generated Content")
    logger.info("-" * 60)

    data = safe_load_json(final_text)

    suggested_units_structured: List[Dict[str, Any]] = []
    course_summary: Dict[str, Any] = {}

    # 1) Only treat dict-shaped JSON as valid for your schema
    if isinstance(data, dict):
        suggested_units_structured = normalize_suggested_units(
            data.get("SuggestedUnits"),
            units=units
        )
        cs = data.get("CourseSummary")
        if isinstance(cs, dict):
            course_summary = cs
        # Optional: keep summary consistent with chapters (Traditional)
        if _opencc and course_summary:
            for k, v in list(course_summary.items()):
                if isinstance(v, str):
                    course_summary[k] = to_traditional(v)
    elif isinstance(data, list):
        # Fallback behavior if model outputs a top-level list.
        # You can either ignore it or try to interpret it as SuggestedUnits directly.
        # This tries to interpret it as SuggestedUnits:
        suggested_units_structured = normalize_suggested_units(data, units=units)
        
    # 2) Warning should be outside the "if isinstance(data, dict)" block
    if units:
        if not suggested_units_structured:
            logger.warning(
                 f"âš ï¸ Client provided {len(units)} Units but SuggestedUnits is empty/invalid after normalization."
            )
        else:
            missing = sum(1 for x in suggested_units_structured if x.get("ClientUnitNo") is None)
            if missing:
                logger.warning(
                    f"âš ï¸ {missing}/{len(suggested_units_structured)} SuggestedUnits missing ClientUnitNo "
                    f"(client provided {len(units)} Units)"
                )
            
    # -------------------------
    # Coverage Guardrail (CRITICAL)
    # If chapters only cover early part of ASR, re-run PASS3 once with anchors.
    # -------------------------
    if suggested_units_structured and asr_end_sec > 0:
        cov = chapters_coverage_ratio(suggested_units_structured, asr_end_sec)
        last_ch = suggested_units_structured[-1]["Time"]
        logger.info(f"ğŸ“ PASS3 coverage check: last_chapter={last_ch}, asr_end={asr_end_ts}, ratio={cov:.2f}")
        
        # If ASR is long enough and chapters end too early => retry once
        # Example: ASR ends at 2:39 but chapters stop at 0:40 => ratio ~0.25 => retry
        if asr_end_sec >= 3600 and cov < 0.60:
            logger.warning(
                f"âš ï¸ PASS3 chapters end too early (ratio={cov:.2f}). Retrying PASS3 with anchor timestamps..."
            )
            retry_hint = f"""
    ã€å¼·åˆ¶è¦†è“‹è¦å‰‡ï¼ˆå¿…é ˆéµå®ˆï¼‰ã€‘
    - é€å­—ç¨¿æœ€å¾Œæ™‚é–“æˆ³ç´„ç‚ºï¼š{asr_end_ts}
    - ä½ è¼¸å‡ºçš„æœ€å¾Œä¸€å€‹ç« ç¯€ Time å¿…é ˆ >= {sec_to_hms(max(0, int(asr_end_sec * 0.85)))}ï¼ˆè‡³å°‘è¦†è“‹åˆ°å¾Œæ®µï¼‰
    - ç¦æ­¢åªç”Ÿæˆå‰æ®µç« ç¯€ï¼›å¿…é ˆæ¶µè“‹æ•´æ®µæ•™å­¸ï¼ˆåŒ…å«å¾ŒåŠæ®µ/å¾Œæ®µï¼‰
    - ä»¥ä¸‹æ˜¯é€å­—ç¨¿ä¸­åˆ†ä½ˆæ–¼å…¨ç¨‹çš„æ™‚é–“æˆ³æ¨£æœ¬ï¼ˆå¿…é ˆç”¨ä¾†é¸ç« ç¯€æ™‚é–“é»ï¼Œä¸”è¦åŒ…å«å¾Œæ®µæ™‚é–“æˆ³ï¼‰ï¼š
    {", ".join(anchors[-10:] if len(anchors) >= 10 else anchors)}
    """
            chapters_prompt_retry = chapters_prompt + "\n" + retry_hint
            retry_resp = call_llm(
                service_type=config.service_type,
                client=client,
                system_message=(
                    "ä½ æ˜¯ç´°å¿ƒçš„ç« ç¯€è¨­è¨ˆå¸«ã€‚"
                    "è«‹åªè¼¸å‡º JSONï¼ˆåŒ…å« SuggestedUnits èˆ‡ CourseSummaryï¼‰ï¼Œç¦æ­¢ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"
                    "ç« ç¯€ Time å¿…é ˆå°é½Š ASR çœŸå¯¦æ™‚é–“æˆ³ï¼Œä¸”å¿…é ˆè¦†è“‹æ•´æ®µé€å­—ç¨¿åˆ°å¾Œæ®µã€‚"
                ),
                user_message=chapters_prompt_retry,
                model=config.openai_model if config.service_type == "openai" else config.azure_model,
                max_tokens=3000,
                temperature=0.1
            )
            final_text_retry = (
                retry_resp.choices[0].message.content
                if config.service_type == "openai"
                else retry_resp.choices[0].message.content
            )
            data_retry = safe_load_json(final_text_retry)

            suggested_retry: List[Dict[str, Any]] = []
            course_summary_retry: Dict[str, Any] = {}
            if isinstance(data_retry, dict):
                suggested_retry = normalize_suggested_units(data_retry.get("SuggestedUnits"), units=units)
                cs2 = data_retry.get("CourseSummary")
                if isinstance(cs2, dict):
                    course_summary_retry = cs2
            elif isinstance(data_retry, list):
                # interpret top-level list as SuggestedUnits
                suggested_retry = normalize_suggested_units(data_retry, units=units)
            else:
                suggested_retry = []
            if suggested_retry:
                suggested_units_structured = suggested_retry
                if course_summary_retry:
                    course_summary = course_summary_retry
                # Optional: keep summary Traditional
                if _opencc and course_summary:
                    for k, v in list(course_summary.items()):
                        if isinstance(v, str):
                            course_summary[k] = to_traditional(v)
                final_text = final_text_retry  # keep raw text for debugging
                logger.info(f"âœ… PASS3 retry succeeded: SuggestedUnits={len(suggested_units_structured)}")
                # âœ… NEW: Recalculate enriched units after retry
                if units:
                    enriched_units, unit_diagnostics = back_calculate_unit_timestamps(
                        suggested_units_structured=suggested_units_structured,
                        client_units=units
                    )
                    logger.info(f"ğŸ”„ Recalculated Unit timestamps after retry")

            else:
                if data_retry is None:
                    logger.warning("âš ï¸ PASS3 retry JSON parse failed; keeping first result")
                else:
                    logger.warning("âš ï¸ PASS3 retry JSON parsed but SuggestedUnits empty/invalid; keeping first result")
                    
    # Build chapters_raw from SuggestedUnits if available, else fallback to text parsing
    if suggested_units_structured:
        chapters_raw = suggested_units_to_chapters_dict(
            suggested_units_structured,
            duration_sec=int(duration),
            bump_limit_sec=120
        )
        logger.info(f"ğŸ“Š Parsed {len(suggested_units_structured)} SuggestedUnits from JSON")
    else:
        logger.warning("âš ï¸ PASS 3 JSON parse failed; falling back to text chapter parsing")
        chapters_raw = parse_chapters_from_output(final_text)
        course_summary = parse_summary_from_output(final_text)
    # âœ… NEW: Back-calculate Unit timestamps (if client provided Units)
    enriched_units = None
    unit_diagnostics = None
    if units:
        enriched_units, unit_diagnostics = back_calculate_unit_timestamps(
            suggested_units_structured=suggested_units_structured,
            client_units=units
        )
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“ UNIT TIMESTAMP BACK-CALCULATION RESULTS")
        logger.info("=" * 60)
        if enriched_units:
            for unit in enriched_units:
                if unit.get("Time"):
                    logger.info(
                        f"âœ… Unit {unit['UnitNo']}: {unit['Title']}\n"
                        f"   â†’ Starts at: {unit['Time']}\n"
                        f"   â†’ First chapter: {unit.get('FirstChapter', 'N/A')}"
                    )
                else:
                    logger.info(
                        f"âš ï¸ Unit {unit['UnitNo']}: {unit['Title']}\n"
                        f"   â†’ Not found in video!"
                    )
        logger.info("=" * 60 + "\n")
 
    # âœ… ALWAYS build `chapters` from `chapters_raw`
    chapters = validate_and_normalize_timestamps(
        chapters_raw,
        int(duration),
        video_id="hierarchical_pass3"
    )
    if not chapters:
        logger.error("âŒ No valid chapters after timestamp validation, using time-based fallback")
        chapters = create_time_based_fallback(int(duration))

    if course_summary:
        logger.info(f"âœ… Successfully extracted course summary with {len(course_summary)} fields:")
        for key, value in course_summary.items():
            display_value = value[:80] + "..." if len(value) > 80 else value
            logger.info(f"   â€¢ {key}: {display_value}")
    else:
        logger.warning("âš ï¸ Course summary extraction failed, using empty dict")
    
    # Calculate educational quality score
    quality_score = estimate_educational_quality(chapters, structure_text)
    logger.info(f"ğŸ“ˆ Educational quality score: {quality_score:.2f}")
    
    # ==================== Build Metadata ====================
    metadata = {
        'generation_method': 'hierarchical_multi_pass_asr_primary',
        'strategy': 'ASR-primary for timing, OCR-supporting for detail',
        'structure_analysis': structure_text,
        'modules_analysis': modules_text,
        'educational_quality_score': quality_score,
        'course_summary': course_summary,
        'content_analysis': course_summary,
        'token_usage': {
            'original': {
                'asr_tokens': asr_tokens,
                'ocr_tokens': ocr_tokens,
                'total_tokens': total_content_tokens
            },
            'used_per_pass': {
                'asr_tokens': asr_used,
                'ocr_tokens': ocr_used,
                'total_tokens': content_used
            },
            'limits': {
                'asr_limit': ASR_LIMIT,
                'ocr_limit': OCR_LIMIT
            },
            'coverage': {
                'asr_coverage': f"{asr_coverage:.1f}%",
                'ocr_coverage': f"{ocr_coverage:.1f}%"
            }
        }
    }
    # âœ… CRITICAL: expose structured SuggestedUnits to downstream pipeline (tasks.py)
    metadata["suggested_units_structured"] = suggested_units_structured
    # âœ… NEW: Add enriched units and diagnostics
    metadata["client_units_original"] = units
    metadata["client_units_with_timestamps"] = enriched_units
    metadata["unit_diagnostics"] = unit_diagnostics

    # âœ… DEBUG: preserve raw PASS3 JSON/text for production debugging
    metadata["pass3_raw_json_text"] = final_text
    logger.info(
        "ğŸ§© PASS3 SuggestedUnits structured: %d (units_provided=%s)",
        len(suggested_units_structured),
        "yes" if units else "no"
    )

    logger.info("\n" + "=" * 60)
    logger.info("âœ… HIERARCHICAL GENERATION COMPLETE")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Chapters generated: {len(chapters)}")
    logger.info(f"ğŸ“Š Summary fields: {len(course_summary)}")
    logger.info(f"ğŸ“Š Quality score: {quality_score:.2f}")
    logger.info(f"ğŸ“Š Strategy: ASR-primary (timing) + OCR-supporting (detail)")
    logger.info(f"ğŸ“Š Total content used: {content_used:,} tokens (ASR: {asr_used:,}, OCR: {ocr_used:,})")
    logger.info("=" * 60 + "\n")
    
    return final_text, chapters, metadata

def estimate_educational_quality(chapters: Dict[str, str], structure: str) -> float:
    """Simple heuristic to estimate educational quality of chapters"""
    quality_indicators = [
        'è¬›è§£', 'åŸç†', 'ç¯„ä¾‹', 'ç·´ç¿’', 'å¯¦ä½œ', 'æ‡‰ç”¨', 'ç¸½çµ', 'é‡é»',
        'æ¦‚å¿µ', 'æ–¹æ³•', 'æŠ€å·§', 'æ­¥é©Ÿ', 'æ¡ˆä¾‹', 'åˆ†æ'
    ]
    
    title_text = ' '.join(chapters.values())
    indicator_count = sum(1 for indicator in quality_indicators 
                         if indicator in title_text)
    
    total_titles = len(chapters)
    return min(1.0, indicator_count / max(1, total_titles * 0.7))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Enhanced Main Function with Smart Routing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_chapters_debug(
    raw_asr_text: str,
    ocr_segments: List[Dict],
    duration: float,
    video_id: str,
    video_title: Optional[str] = None,  # ADD THIS
    section_title: Optional[str] = None,  # â† ADD
    units: Optional[List[Dict]] = None,    # â† ADD
    run_dir: Optional[Path] = None,
    progress_callback: Optional[Callable[[str, int], None]] = None,
    *,
    ocr_context_override: Optional[str] = None,
    # NEW: Add control parameter
    force_generation_method: Optional[str] = None,  # 'hierarchical' or 'single_pass'
) -> Tuple[str, Dict[str, str], Dict[str, str], Dict[str, Any]]:
    """
    Enhanced version with smart routing between hierarchical and single-pass generation
    """
    if progress_callback:
        progress_callback("initializing", 0)

    if run_dir is None:
        run_dir = Path(f"/tmp/chapter_generation/{video_id}_{int(time.time())}")
    run_dir.mkdir(parents=True, exist_ok=True)

    try:
        logger.info(f"Starting chapter generation for video {video_id} (duration: {duration}s)")

        # Load configuration
        config = ChapterConfig()
        if not validate_config(config):
            logger.warning("Configuration validation failed, using time-based fallback")
            fallback = create_time_based_fallback(int(duration))
            fallback = ensure_traditional_chapters(fallback)
            return ("", {}, fallback, {"generation_method": "time_based_fallback", "course_summary": {}})

        if progress_callback:
            progress_callback("processing_inputs", 10)

        # Build OCR context (existing logic)
        if ocr_context_override is not None:
            ocr_context = ocr_context_override
        else:
            ocr_context = build_ocr_context_from_segments(ocr_segments) if ocr_segments else ""

        min_gap_sec, target_range, max_caps = chapter_policy(int(duration))
        
        # Save raw inputs
        with open(run_dir / "raw_asr_text.txt", "w", encoding="utf-8") as f:
            f.write(raw_asr_text)
        if ocr_context_override is not None:
            with open(run_dir / "ocr_raw.txt", "w", encoding="utf-8") as f:
                f.write(ocr_context_override)
        else:
            with open(run_dir / "ocr_segments.json", "w", encoding="utf-8") as f:
                json.dump(ocr_segments, f, ensure_ascii=False, indent=2)

        if progress_callback:
            progress_callback("initializing_client", 20)

        # Initialize client (existing logic)
        service_type = config.service_type
        model = config.openai_model if service_type == "openai" else config.azure_model

        if service_type == "azure":
            client = initialize_client(
                service_type="azure",
                endpoint=config.azure_endpoint,
                key=config.azure_key,
                api_version=config.azure_api_version,
            )
        else:
            client = initialize_client(
                service_type="openai",
                api_key=config.openai_api_key,
                base_url=config.openai_base_url,
            )

        # ğŸ¯ NEW: Smart Generation Method Selection
        use_hierarchical = False
        if force_generation_method == 'hierarchical':
            use_hierarchical = True
        elif force_generation_method == 'single_pass':
            use_hierarchical = False
        else:
            # Auto-detect based on content characteristics
            use_hierarchical = should_use_hierarchical(duration, len(raw_asr_text))
        
        logger.info(f"Using generation method: {'hierarchical_multi_pass' if use_hierarchical else 'single_pass'}")

        if use_hierarchical:
            if progress_callback:
                progress_callback("hierarchical_analysis", 30)
            
            # Use hierarchical multi-pass generation
            raw_llm_text, chapters, metadata = hierarchical_multipass_generation(
                raw_asr_text=raw_asr_text,
                duration=duration,
                ocr_context=ocr_context,
                video_title=video_title,  # ADD THIS
                section_title=section_title,      # â† ADD
                units=units,                       # â† ADD
                client=client,
                config=config,
                progress_callback=progress_callback
            )
            
            # Save hierarchical metadata
            with open(run_dir / "hierarchical_metadata.json", "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            with open(run_dir / "course_structure.txt", "w", encoding="utf-8") as f:
                f.write(metadata.get('structure_analysis', ''))
            with open(run_dir / "learning_modules.txt", "w", encoding="utf-8") as f:
                f.write(metadata.get('modules_analysis', ''))
                
        else:
            if progress_callback:
                progress_callback("single_pass_processing", 30)

            first_ts, last_ts = get_first_last_asr_ts(raw_asr_text, int(duration))
            # Use original single-pass generation
            prompt_template = build_prompt_body(
                "", int(duration), ocr_context, video_title,
                first_ts_override=first_ts,
                last_ts_override=last_ts,
            )
            template_tokens = count_tokens_llama(prompt_template)

            CONTEXT_BUDGET = 128_000

            asr_tokens = count_tokens_llama(raw_asr_text)
            if template_tokens + asr_tokens <= CONTEXT_BUDGET:
                transcript_for_prompt = raw_asr_text
                logger.info(
                    f"âœ… Using full ASR (template={template_tokens:,}, asr={asr_tokens:,}, budget={CONTEXT_BUDGET:,})"
                )
            else:
                max_transcript_tokens = max(0, CONTEXT_BUDGET - template_tokens)
                transcript_for_prompt = truncate_text_by_tokens(raw_asr_text, max_transcript_tokens)
                logger.warning(
                    f"âš ï¸ Truncating ASR (template={template_tokens:,}, asr={asr_tokens:,}, "
                    f"budget={CONTEXT_BUDGET:,}, allowed_asr={max_transcript_tokens:,})"
                )
            full_prompt = build_prompt_body(
                transcript_for_prompt, int(duration), ocr_context, video_title,
                first_ts_override=first_ts,
                last_ts_override=last_ts,
            )
                
            with open(run_dir / "full_prompt.txt", "w", encoding="utf-8") as f:
                f.write(full_prompt)

            if progress_callback:
                progress_callback("calling_llm", 50)

            enhanced_system_message = (
                "ä½ æ˜¯å°ˆæ¥­çš„ç·šä¸Šèª²ç¨‹è¨­è¨ˆå°ˆå®¶ï¼Œæ“…é•·ç‚ºå„ç¨®å­¸ç§‘å‰µå»ºé«˜å“è³ªæ•™è‚²ç« ç¯€çµæ§‹ã€‚"
                "è‡ªå‹•è­˜åˆ¥èª²ç¨‹é ˜åŸŸä¸¦ä½¿ç”¨é©ç•¶å°ˆæ¥­è¡“èªï¼Œå°ˆæ³¨æ–¼å­¸ç¿’åƒ¹å€¼å’Œæ•™è‚²é€£è²«æ€§ã€‚"
                "åš´æ ¼é¿å…é‡è¤‡æ¨¡å¼ï¼Œå‰µå»ºåæ˜ çœŸå¯¦æ•™è‚²é€²ç¨‹çš„å°ˆæ¥­ç« ç¯€æ¨™é¡Œã€‚"
                "åƒ…è¼¸å‡ºç« ç¯€æ¸…å–®ï¼Œæ¯è¡Œæ ¼å¼: `HH:MM:SS - æ¨™é¡Œ`ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‚"
            )

            logger.info(f"Calling {service_type} API for single-pass chapter generation...")
            t0 = time.time()
            resp = call_llm(
                service_type=service_type,
                client=client,
                system_message=enhanced_system_message,
                user_message=full_prompt,
                model=model,
                max_tokens=2048,
                temperature=0.2,
                top_p=0.9,
            )
            dt = time.time() - t0
            logger.info(f"LLM API call completed in {dt:.2f}s")

            if service_type == "azure":
                raw_llm_text = resp.choices[0].message.content
            else:
                raw_llm_text = resp.choices[0].message.content

            # Parse chapters
            chapters_raw = parse_chapters_from_output(raw_llm_text)

            # âœ… NEW: Validate and normalize timestamps
            chapters = validate_and_normalize_timestamps(
                chapters_raw,
                int(duration),
                video_id=video_id
            )
            if not chapters:
                logger.error("âŒ No valid chapters after timestamp validation, using time-based fallback")
                chapters = create_time_based_fallback(int(duration))

            # Parse structured summary
            course_summary = parse_summary_from_output(raw_llm_text)
            metadata = {'generation_method': 'single_pass',
                        'course_summary': course_summary,
                        }

        # COMMON POST-PROCESSING (existing logic)
        if progress_callback:
            progress_callback("parsing_response", 70)

        with open(run_dir / "llm_output_raw.txt", "w", encoding="utf-8") as f:
            f.write(raw_llm_text)

        # Apply cleaning and Traditional Chinese conversion
        parsed_raw_clean_trad = ensure_traditional_chapters(clean_chapter_titles(chapters))

        with open(run_dir / "parsed_raw_chapters.json", "w", encoding="utf-8") as f:
            json.dump(parsed_raw_clean_trad, f, ensure_ascii=False, indent=2)

        if progress_callback:
            progress_callback("balancing_chapters", 80)

        # Balance according to policy
        chapters_final = globally_balance_chapters(
            parsed_raw_clean_trad, int(duration), min_gap_sec, target_range, max_caps
        )
        if not chapters_final:
            raise RuntimeError("No chapters left after balancing")

        with open(run_dir / "chapters_final.json", "w", encoding="utf-8") as f:
            json.dump(chapters_final, f, ensure_ascii=False, indent=2)

        # Save generation method info
        with open(run_dir / "generation_method.txt", "w", encoding="utf-8") as f:
            f.write(metadata.get('generation_method', 'unknown'))

        if progress_callback:
            progress_callback("completed", 100)

        # Return 4-tuple: (raw_text, parsed_chapters, final_chapters, metadata)
        return (raw_llm_text, parsed_raw_clean_trad, chapters_final, metadata)  # â† FIXED!

    except Exception as e:
        logger.error(f"Chapter generation failed: {e}", exc_info=True)
        fallback = ensure_traditional_chapters(create_time_based_fallback(int(duration)))
        # Return fallback with empty metadata
        fallback_metadata = {
            'generation_method': 'time_based_fallback',
            'educational_quality_score': 0.0,
            'course_summary': {}
        }
        return ("", {}, fallback, fallback_metadata)  # â† FIXED!
        
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN FUNCTIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def create_time_based_fallback(duration_sec: int) -> Dict[str, str]:
    """Create fallback chapters based on time intervals"""
    fallback_chapters: Dict[str, str] = {}
    interval = 300  # 5 minutes
    for i in range(0, int(duration_sec), interval):
        fallback_chapters[sec_to_hms(i)] = "ç« ç¯€ " + str((i // interval) + 1)
    logger.info(f"Created {len(fallback_chapters)} time-based fallback chapters")
    return fallback_chapters

def generate_chapters(
    raw_asr_text: str,
    ocr_segments: List[Dict],
    duration: float,
    video_id: str,
    video_title: Optional[str] = None,
    section_title: Optional[str] = None,  # â† ADD
    units: Optional[List[Dict]] = None,    # â† ADD
    run_dir: Optional[Path] = None,
    progress_callback: Optional[Callable[[str, int], None]] = None,
    *,
    ocr_context_override: Optional[str] = None,
    force_generation_method: Optional[str] = None,
) -> Tuple[Dict[str, str], Dict[str, Any]]:  # â† FIXED: Return tuple
    """
    Generate chapters and return (chapters_dict, metadata).
    
    For backward compatibility with old code that expects just chapters dict,
    you can use: chapters, _ = generate_chapters(...)
    
    Returns:
        Tuple of (chapters_dict, metadata)
    """
    _raw_text, _parsed_raw, final_chapters, metadata = generate_chapters_debug(  # â† FIXED: Unpack 4 values
        raw_asr_text=raw_asr_text,
        ocr_segments=ocr_segments,
        duration=duration,
        video_id=video_id,
        video_title=video_title,  # â† Make sure this is passed
        section_title=section_title,        # â† ADD
        units=units,                         # â† ADD
        run_dir=run_dir,
        progress_callback=progress_callback,
        ocr_context_override=ocr_context_override,
        force_generation_method=force_generation_method
    )
    return final_chapters, metadata  # â† FIXED: Return tuple

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(description="Generate video chapters from raw ASR and optional OCR.")
    parser.add_argument('--asr-file', type=argparse.FileType('r', encoding='utf-8'), required=True,
                        help='Path to file containing raw ASR text with timestamps.')
    parser.add_argument('--ocr-file', type=argparse.FileType('r', encoding='utf-8'),
                        help='Optional path to OCR file. In verbatim mode this is read as raw text.')
    parser.add_argument('--duration', type=float, required=True,
                        help='Duration of the video in seconds.')
    parser.add_argument('--video-id', type=str, required=True,
                        help='Unique identifier for the video (used for output directory).')
    parser.add_argument('--output-dir', type=str, default='./chapter_debug',
                        help='Directory to save debug outputs. Default: ./chapter_debug')
    parser.add_argument('--debug', action='store_true', help='Print RAW LLM output and parsed chapters too.')
    parser.add_argument('--ocr-mode', choices=['none', 'verbatim', 'segments'], default='verbatim',
                        help="How to include OCR: 'none' (omit), 'verbatim' (raw text), or 'segments' (legacy minimal formatting).")

    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        stream=sys.stdout
    )

    # Read ASR
    logger.info(f"Reading ASR text from {args.asr_file.name}...")
    raw_asr_text = args.asr_file.read()
    args.asr_file.close()

    # Read OCR according to the chosen mode
    ocr_segments: List[Dict] = []
    ocr_context_override: Optional[str] = None
    if args.ocr_file:
        if args.ocr_mode == 'none':
            logger.info("OCR mode: none (omit OCR from prompt).")
            try:
                args.ocr_file.close()
            except Exception:
                pass
        elif args.ocr_mode == 'verbatim':
            logger.info(f"OCR mode: verbatim. Reading {args.ocr_file.name} as raw text...")
            try:
                ocr_context_override = args.ocr_file.read()
            finally:
                try:
                    args.ocr_file.close()
                except Exception:
                    pass
            logger.info("OCR loaded verbatim.")
        else:
            logger.info(f"OCR mode: segments. Reading OCR segments from {args.ocr_file.name}...")
            try:
                ocr_segments = load_ocr_segments(args.ocr_file, args.ocr_file.name)
                args.ocr_file.close()
                logger.info(f"Loaded {len(ocr_segments)} OCR segments")
            except Exception as e:
                logger.warning(f"OCR file load failed, proceeding without OCR. Detail: {e}")
                try:
                    args.ocr_file.close()
                except Exception:
                    pass
                ocr_segments = []

    # Output directory
    run_dir = Path(args.output_dir) / args.video_id
    run_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"Saving debug outputs to: {run_dir}")

    # Simple progress callback
    def cli_progress_callback(stage: str, percent: int):
        logger.info(f"Progress: {percent}% - {stage}")

    # Generate
    logger.info("Starting chapter generation...")

    raw_text, parsed_raw, final_chapters, metadata = generate_chapters_debug(
        raw_asr_text=raw_asr_text,
        ocr_segments=ocr_segments,
        duration=args.duration,
        video_id=args.video_id,
        run_dir=run_dir,
        progress_callback=cli_progress_callback,
        ocr_context_override=ocr_context_override,
    )
    
    # Console output
    print("\n" + "="*50)
    print("âœ… CHAPTER GENERATION COMPLETE")
    print("="*50)

    if args.debug:
        print("\n--- RAW LLM OUTPUT (as returned) ---")
        print(raw_text if raw_text else "(empty/raw fallback)")
        print("\n--- PARSED (pre-balance) ---")
        for ts, title in parsed_raw.items():
            print(f"{ts} - {title}")

    print("\n--- FINAL (balanced) ---")
    for ts, title in final_chapters.items():
        print(f"{ts} - {title}")

    # Save final chapters to a clean file
    output_file = run_dir / "final_chapters.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        for timestamp, title in final_chapters.items():
            f.write(f"{timestamp} - {title}\n")
    logger.info(f"Final chapters saved to: {output_file}")

    # Also save a pre-balance view for convenience
    pre_file = run_dir / "parsed_raw_chapters.txt"
    with open(pre_file, 'w', encoding='utf-8') as f:
        for timestamp, title in parsed_raw.items():
            f.write(f"{timestamp} - {title}\n")
    logger.info(f"Parsed (pre-balance) chapters saved to: {pre_file}")

if __name__ == "__main__":
    main()
