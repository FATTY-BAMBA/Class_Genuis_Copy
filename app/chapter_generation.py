# app/video_chaptering.py
"""
Module for generating video chapters from *raw* ASR and OCR inputs.
- No ASR/OCR preprocessing/merging; we use what you pass in.
- Enforces a ~128k token prompt budget (approx) for large-context models.
- Parses, lightly cleans, balances chapters, and converts titles to Traditional Chinese.
- Exposes RAW LLM output so you can inspect what the model produced before any parsing/balancing.

CLI examples:
    python video_chaptering.py --asr-file raw_asr.txt --duration 3600 --video-id test_01
    python video_chaptering.py --asr-file raw_asr.txt --ocr-file ocr_raw.txt --duration 1800 --video-id test_02
    # Show both RAW and FINAL in console:
    python video_chaptering.py --asr-file raw_asr.txt --duration 1800 --video-id debug_run --debug
"""

import argparse
import hashlib
import json
import logging
import os
import re
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

# Optional Azure AI Inference imports (only if used)
try:
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import SystemMessage, UserMessage
    from azure.core.credentials import AzureKeyCredential
except Exception:  # optional at runtime
    ChatCompletionsClient = None  # type: ignore
    SystemMessage = None  # type: ignore
    UserMessage = None  # type: ignore
    AzureKeyCredential = None  # type: ignore

# Optional OpenAI import (only if used)
try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Optional Simplifiedâ†’Traditional conversion (OpenCC preferred)
try:
    from opencc import OpenCC
    _opencc = OpenCC('s2t')
except Exception:
    _opencc = None

logger = logging.getLogger(__name__)

# ==================== CONFIGURATION ====================
@dataclass
class ChapterConfig:
    """Configuration for chapter generation service"""
    service_type: str = os.getenv("CHAPTER_SERVICE_TYPE", "openai")  # "openai" or "azure"
    openai_model: str = os.getenv("CHAPTER_OPENAI_MODEL", "gpt-4o")
    azure_model: str = os.getenv("CHAPTER_AZURE_MODEL", "Meta-Llama-3.1-8B-Instruct")
    openai_api_key: Optional[str] = os.getenv("OPENAI_API_KEY")
    azure_endpoint: Optional[str] = os.getenv("AZURE_AI_ENDPOINT")
    azure_key: Optional[str] = os.getenv("AZURE_AI_KEY")
    azure_api_version: str = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    openai_base_url: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1/")

def validate_config(config: ChapterConfig) -> bool:
    """Validate that required configuration is present"""
    if config.service_type == "azure":
        if not config.azure_endpoint or not config.azure_key:
            logger.error("Azure AI credentials not configured. Set AZURE_AI_ENDPOINT and AZURE_AI_KEY.")
            return False
    elif config.service_type == "openai":
        if not config.openai_api_key:
            logger.error("OpenAI API key not configured. Set OPENAI_API_KEY.")
            return False
    else:
        logger.error(f"Unknown service type: {config.service_type}")
        return False
    return True

def get_content_hash(transcript: str, ocr_context: str, duration: float) -> str:
    """Generate hash for content to enable caching"""
    content = f"{transcript}{ocr_context}{duration}"
    return hashlib.md5(content.encode()).hexdigest()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utilities
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHAPTER_LINE_RE = re.compile(
    r"""
    ^\s*
    (?:[\-\*\u2022]\s*)?
    \[?(?P<ts>\d{1,2}:\d{2}(?::\d{2})?)\]?\s*
    (?:[\-â€“â€”:]\s*)?
    (?P<title>.+?)
    \s*$
    """,
    re.VERBOSE,
)

def sec_to_hms(sec: int) -> str:
    """Convert seconds to HH:MM:SS format"""
    if sec < 0:
        sec = 0
    h, rem = divmod(sec, 3600)
    m, s = divmod(rem, 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def _is_cjk(ch: str) -> bool:
    return '\u4e00' <= ch <= '\u9fff'

def clean_chapter_titles(chapters: Dict[str, str]) -> Dict[str, str]:
    """
    Clean up chapter titles by removing filler words and improving clarity.
    For Chinese titles, avoid English-centric capitalization; if over-trimmed (<4 chars),
    fall back to the original title.
    """
    cleaned: Dict[str, str] = {}
    filler_words = ['é‚£', 'æ‰€ä»¥', 'é€™å€‹', 'é‚£å€‹', 'å°±æ˜¯', 'å‘¢', 'å•Š', 'å–”', 'ç„¶å¾Œ', 'æ¥è‘—']
    for ts, original_title in chapters.items():
        title = original_title
        for word in filler_words:
            title = title.replace(word, '')
        title = re.sub(r'[ã€‚ï¼Œâ€œâ€ã€ï¼ï¼Ÿ\.!?,]+$', '', title.strip())
        title = re.sub(r'\s+', ' ', title)

        # If cleaning made it too short, revert to the original
        if 0 < len(title) < 4:
            title = original_title.strip()

        cleaned[ts] = title 
    return cleaned

def count_tokens_llama(text: str) -> int:
    """Approximate token counting for mixed Chinese/English (â‰ˆ1 token per CJK char; 1/4 per other chars)"""
    chinese_chars = sum(1 for char in text if _is_cjk(char))
    non_chinese_len = len(text) - chinese_chars
    return chinese_chars + max(1, non_chinese_len // 4)

def truncate_text_by_tokens(text: str, max_tokens: int = 120_000) -> str:
    """Truncate text to approximately max_tokens, preserving sentence boundaries where possible"""
    if max_tokens <= 0:
        return ""
    if count_tokens_llama(text) <= max_tokens:
        return text
    logger.warning(f"Truncating transcript from {count_tokens_llama(text):,} tokens to {max_tokens:,} tokens")
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])', text)
    truncated = ""
    current_tokens = 0
    for sentence in sentences:
        sentence_tokens = count_tokens_llama(sentence)
        if current_tokens + sentence_tokens > max_tokens:
            break
        truncated += sentence
        current_tokens += sentence_tokens
    return truncated

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Chapter policy & parsing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chapter_policy(duration_sec: int) -> Tuple[int, Tuple[int, int], int]:
    """Determine chapter generation parameters based on video duration"""
    if duration_sec < 30 * 60:     # < 30 min
        return 120, (5, 10), 30    # min_gap: 2 min, target: 5-10 chapters
    elif duration_sec < 60 * 60:   # < 1 hour  
        return 180, (6, 12), 40    # min_gap: 3 min, target: 6-12 chapters
    elif duration_sec < 120 * 60:  # < 2 hours
        return 240, (8, 16), 50    # min_gap: 4 min, target: 8-16 chapters
    elif duration_sec < 180 * 60:  # < 3 hours
        return 300, (10, 20), 60   # min_gap: 5 min, target: 10-20 chapters
    else:                           # 3+ hours
        return 360, (12, 24), 80   # min_gap: 6 min, target: 12-24 chapters

def _normalize_ts(ts: str) -> str:
    """Normalize timestamp format to HH:MM:SS"""
    parts = ts.split(":")
    if len(parts) == 2:
        return f"00:{parts[0].zfill(2)}:{parts[1].zfill(2)}"
    if len(parts) == 3:
        return f"{parts[0].zfill(2)}:{parts[1].zfill(2)}:{parts[2].zfill(2)}"
    return ts

def parse_chapters_from_output(output_text: str) -> Dict[str, str]:
    """Parse chapter timestamps and titles from LLM output"""
    chapters: Dict[str, str] = {}
    
    # Direct parsing for "HH:MM:SS - Title" format
    for line in output_text.splitlines():
        line = line.strip()
        if not line:
            continue
        
        # Look for the pattern "HH:MM:SS - Title"
        if ' - ' in line:
            parts = line.split(' - ', 1)
            if len(parts) == 2:
                ts = parts[0].strip()
                title = parts[1].strip()
                # Validate timestamp format
                if re.match(r'\d{2}:\d{2}:\d{2}', ts):
                    chapters[ts] = title
    
    # If no chapters found, try the original regex approach
    if not chapters:
        for line in output_text.splitlines():
            line = line.strip()
            if not line:
                continue
            m = CHAPTER_LINE_RE.match(line)
            if m:
                ts = _normalize_ts(m.group("ts").strip())
                title = m.group("title").strip()
                if title:
                    chapters.setdefault(ts, title)
    
    return chapters

def parse_summary_from_output(output_text: str) -> Dict[str, str]:
    """Extract the structured summary from the LLM output"""
    summary = {}
    lines = output_text.split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('èª²ç¨‹ä¸»é¡Œï¼š'):
            summary['topic'] = line.replace('èª²ç¨‹ä¸»é¡Œï¼š', '').strip()
        elif line.startswith('æ ¸å¿ƒå…§å®¹ï¼š'):
            summary['core_content'] = line.replace('æ ¸å¿ƒå…§å®¹ï¼š', '').strip()
        elif line.startswith('å­¸ç¿’ç›®æ¨™ï¼š'):
            summary['learning_objectives'] = line.replace('å­¸ç¿’ç›®æ¨™ï¼š', '').strip()
        elif line.startswith('é©åˆå°è±¡ï¼š'):
            summary['target_audience'] = line.replace('é©åˆå°è±¡ï¼š', '').strip()
        elif line.startswith('é›£åº¦ç´šåˆ¥ï¼š'):
            summary['difficulty'] = line.replace('é›£åº¦ç´šåˆ¥ï¼š', '').strip()
    
    # Apply Traditional Chinese conversion to summary fields
    if _opencc:
        for key in summary:
            summary[key] = to_traditional(summary[key])
    
    return summary

def globally_balance_chapters(
    chapters: Dict[str, str],
    duration_sec: int,
    min_gap_sec: int,
    target_range: Tuple[int, int],
    max_caps: int,
) -> Dict[str, str]:
    """Balance chapters with content-aware merging"""
    
    def extract_module_tag(title: str) -> str:
        """Extract [module] tag if present"""
        match = re.match(r'\[([^\]]+)\]', title)
        return match.group(1) if match else ""
    
    def ts_to_s(ts: str) -> int:
        p = ts.split(":")
        if len(p) == 2:
            return int(p[0]) * 60 + int(p[1])
        if len(p) == 3:
            return int(p[0]) * 3600 + int(p[1]) * 60 + int(p[2])
        return 0

    cands = [(ts_to_s(ts), ts, t.strip()) for ts, t in chapters.items() if 0 <= ts_to_s(ts) <= duration_sec]
    cands.sort(key=lambda x: x[0])
    if not cands:
        return {}

    # Content-aware deduplication
    dedup = []
    for s, ts, title in cands:
        if not dedup:
            dedup.append((s, ts, title))
            continue
            
        time_gap = s - dedup[-1][0]
        
        # Merge if same module and close, or very close regardless
        should_merge = False
        prev_module = extract_module_tag(dedup[-1][2])
        curr_module = extract_module_tag(title)
        
        if time_gap < 120:  # Less than 2 minutes - always merge
            should_merge = True
        elif prev_module and curr_module and prev_module == curr_module:
            # Same module tag - use min_gap_sec
            should_merge = time_gap < min_gap_sec * 0.7
        elif time_gap < min_gap_sec // 2:  # Half the min_gap for different modules
            should_merge = True
            
        if should_merge:
            # Keep the better title (prefer tagged, then longer)
            prev_s, prev_ts, prev_title = dedup[-1]
            if curr_module and not prev_module:
                dedup[-1] = (prev_s, prev_ts, title)
            elif len(title) > len(prev_title) * 1.3:
                dedup[-1] = (prev_s, prev_ts, title)
        else:
            dedup.append((s, ts, title))

    t_low, t_high = target_range
    
    # Log the balancing result
    logger.info(f"Chapter balancing: {len(chapters)} â†’ {len(dedup)} (target: {t_low}-{t_high})")
    
    if t_low <= len(dedup) <= t_high:
        return {ts: title for _, ts, title in dedup}

    # Too many chapters: choose representatives from each segment
    if len(dedup) > t_high:
        selected = []
        segment_length = max(1, duration_sec // t_high)
        for i in range(t_high):
            segment_start = i * segment_length
            segment_end = (i + 1) * segment_length if i < t_high - 1 else duration_sec + 1
            segment_chapters = [c for c in dedup if segment_start <= c[0] < segment_end]
            if segment_chapters:
                # Pick the one closest to segment center
                segment_center = (segment_start + segment_end) // 2
                chosen = min(segment_chapters, key=lambda c: abs(c[0] - segment_center))
                selected.append(chosen)
        selected.sort(key=lambda x: x[0])
        return {ts: title for _, ts, title in selected}

    # Not enough chapters: just cap to max_caps
    return {ts: title for _, ts, title in dedup[:max_caps]}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OCR handling (optional legacy "segments" mode)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_ocr_segments(file_obj, filename: str) -> List[Dict]:
    """
    Accepts:
      - JSON array:        [ { "start": 0, "end": 3, "text": "..." }, ... ]
      - Wrapped JSON:      { "segments": [ {...}, ... ] }
      - JSON Lines (JSONL): one JSON object per line
      - Plain text (.txt): whole file becomes a single segment at t=0
    Returns: List[Dict] with keys: start (int), end (int, optional), text (str)
    """
    try:
        data = json.load(file_obj)
        if isinstance(data, dict) and "segments" in data and isinstance(data["segments"], list):
            segments = data["segments"]
        elif isinstance(data, list):
            segments = data
        else:
            return []
        out = []
        for item in segments:
            if not isinstance(item, dict):
                continue
            text = str(item.get("text", "")).strip()
            if not text:
                continue
            start = int(item.get("start", 0))
            end = int(item.get("end", start))
            out.append({"start": start, "end": end, "text": text})
        return out
    except json.JSONDecodeError:
        pass

    # Try JSONL
    try:
        file_obj.seek(0)
        segments = []
        for line in file_obj:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                segments = None
                break
            if not isinstance(obj, dict):
                continue
            text = str(obj.get("text", "")).strip()
            if not text:
                continue
            start = int(obj.get("start", 0))
            end = int(obj.get("end", start))
            segments.append({"start": start, "end": end, "text": text})
        if segments is not None:
            return segments
    except Exception:
        pass

    # Plain text fallback
    try:
        file_obj.seek(0)
        txt = file_obj.read().strip()
    except Exception:
        txt = ""
    return [{"start": 0, "end": 0, "text": txt}] if txt else []

def build_ocr_context_from_segments(ocr_segments: List[Dict]) -> str:
    """Legacy minimal OCR formatting: timestamped lines with a simple header."""
    if not ocr_segments:
        return ""
    lines = ["# è¢å¹•/æŠ•å½±ç‰‡æ“·å–æ–‡å­—ï¼ˆåŸå§‹ï¼‰ï¼š"]
    for seg in ocr_segments:
        start = int(seg.get("start", 0))
        text = str(seg.get("text", "")).strip()
        if not text:
            continue
        lines.append(f"* {sec_to_hms(start)}ï¼š{text}")
    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Simplifiedâ†’Traditional conversion
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_S2T_FALLBACK_MAP = {
    "ä½“": "é«”", "å°": "è‡º", "å": "å¾Œ", "å¹¿": "å»£", "ç”»": "ç•«", "å½•": "éŒ„", "è§‚": "è§€",
    "é¢": "éºµ", "å‘": "ç™¼", "é—¨": "é–€", "é—®": "å•", "ç±»": "é¡", "ç½‘": "ç¶²", "å›¾": "åœ–",
    "ä¹¦": "æ›¸", "è®°": "è¨˜", "è¯»": "è®€", "å…š": "é»¨", "æœ¯": "è¡“", "å±‚": "å±¤", "çº¦": "ç´„",
}

def to_traditional(text: str) -> str:
    """Convert a string to Traditional Chinese. Uses OpenCC if available; otherwise minimal mapping."""
    if not text:
        return text
    if _opencc is not None:
        try:
            return _opencc.convert(text)
        except Exception:
            pass
    return ''.join(_S2T_FALLBACK_MAP.get(ch, ch) for ch in text)

def ensure_traditional_chapters(chapters: Dict[str, str]) -> Dict[str, str]:
    """Convert all chapter titles to Traditional Chinese (idempotent if already Traditional)."""
    return {ts: to_traditional(title) for ts, title in chapters.items()}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Client Initialization & LLM call
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def initialize_client(service_type: str, **kwargs) -> Any:
    """Initialize the appropriate LLM client"""
    if service_type == "azure":
        if ChatCompletionsClient is None or AzureKeyCredential is None:
            raise RuntimeError("Azure dependencies are not available in this environment.")
        return ChatCompletionsClient(
            endpoint=kwargs["endpoint"],
            credential=AzureKeyCredential(kwargs["key"]),
            api_version=kwargs.get("api_version", "2024-05-01-preview"),
        )
    elif service_type == "openai":
        if OpenAI is None:
            raise RuntimeError("OpenAI client is not available in this environment.")
        return OpenAI(
            api_key=kwargs["api_key"],
            base_url=kwargs.get("base_url", "https://api.openai.com/v1/"),
        )
    else:
        raise ValueError(f"Unknown service type: {service_type}")

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((Exception,)),
    reraise=True,
)
def call_llm(
    service_type: str,
    client: Any,
    system_message: str,
    user_message: str,
    model: str,
    max_tokens: int = 2048,
    temperature: float = 0.2,
    top_p: float = 0.9,
) -> Any:
    """Call LLM API with retry logic"""
    if service_type == "azure":
        return client.complete(
            messages=[
                SystemMessage(content=system_message),
                UserMessage(content=user_message),
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            model=model,
        )
    elif service_type == "openai":
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        return response
    else:
        raise ValueError(f"Unknown service type: {service_type}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompt builder (ASR first, OCR second, OCR verbatim supported)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_prompt_body(
    transcript: str,
    duration_sec: int,
    ocr_context: str = "",
    video_title: Optional[str] = None,  # ADD THIS
) -> str:
    duration_hms = sec_to_hms(int(duration_sec))
    min_gap_sec, (t_low, t_high), max_caps = chapter_policy(int(duration_sec))
    
    # Extract first and last timestamps for concrete examples
    timestamps = []
    for line in transcript.split('\n'):
        if ':' in line and len(line.split(':')) >= 4:
            ts_part = ':'.join(line.split(':')[:3])
            if re.match(r'\d{2}:\d{2}:\d{2}', ts_part):
                timestamps.append(ts_part)
    
    first_ts = timestamps[0] if timestamps else "00:00:00"
    last_ts = timestamps[-1] if timestamps else duration_hms

    video_title_context = ""
    if video_title:
        # Strip common video extensions
        clean_title = re.sub(r'\.(mp4|avi|mov|mkv|webm|flv|m4v)$', '', video_title, flags=re.IGNORECASE)
        video_title_context = f"""
        
# ğŸ“š èª²ç¨‹æª”æ¡ˆè³‡è¨Š
æª”åï¼š{clean_title}
è«‹åƒè€ƒæª”åç†è§£èª²ç¨‹ä¸»é¡Œã€ç« ç¯€ç·¨è™Ÿã€æ¶µè“‹å…§å®¹ç­‰é‡è¦è³‡è¨Šï¼Œä¸¦æ“šæ­¤è¨­è¨ˆç« ç¯€çµæ§‹ã€‚
"""
    
    prompt = f"""
# æ•™è‚²ç« ç¯€è¨­è¨ˆå°ˆå®¶ - æ™‚é–“æˆ³è¨˜ç²¾æº–å°æ‡‰ç‰ˆ
ä½ æ˜¯è³‡æ·±ç·šä¸Šèª²ç¨‹è¨­è¨ˆå°ˆå®¶ï¼Œè² è²¬å°‡æ•™å­¸å½±ç‰‡è½‰åŒ–ç‚ºå°ˆæ¥­æ•™è‚²ç« ç¯€çµæ§‹ã€‚

{video_title_context}
# ğŸš¨ æœ€é‡è¦çš„è¦å‰‡ - æ™‚é–“æˆ³è¨˜å¿…é ˆç²¾æº–å°æ‡‰
**é€å­—ç¨¿å¯¦éš›æ™‚é–“ç¯„åœï¼š{first_ts} åˆ° {last_ts}**

## çµ•å°ç¦æ­¢çš„è¡Œç‚ºï¼š
âŒ ç”Ÿæˆ 00:00:00 ç« ç¯€ï¼ˆé™¤éé€å­—ç¨¿çœŸçš„å¾ 00:00:00 é–‹å§‹ï¼‰
âŒ è¦å¾‹æ™‚é–“é–“éš”ï¼šæ¯15åˆ†é˜ã€æ¯30åˆ†é˜ç­‰å›ºå®šæ¨¡å¼
âŒ æ†‘ç©ºæƒ³åƒæ™‚é–“é»ï¼ˆå¿…é ˆå°æ‡‰é€å­—ç¨¿ä¸­çš„å¯¦éš›æ™‚é–“æˆ³ï¼‰
âŒ å¿½ç•¥é€å­—ç¨¿çš„æ™‚é–“ç¯„åœ

## å¿…é ˆéµå®ˆçš„è¦å‰‡ï¼š
âœ… ç¬¬ä¸€å€‹ç« ç¯€æ™‚é–“ >= {first_ts}ï¼ˆé€å­—ç¨¿é–‹å§‹æ™‚é–“ï¼‰
âœ… æœ€å¾Œä¸€å€‹ç« ç¯€æ™‚é–“ <= {last_ts}ï¼ˆé€å­—ç¨¿çµæŸæ™‚é–“ï¼‰  
âœ… æ¯å€‹ç« ç¯€æ™‚é–“å¿…é ˆæ¥è¿‘é€å­—ç¨¿ä¸­å¯¦éš›è¨è«–è©²ä¸»é¡Œçš„æ™‚é–“æˆ³ï¼ˆÂ±60ç§’å…§ï¼‰
âœ… åŸºæ–¼å…§å®¹è‡ªç„¶è½‰æŠ˜é»ï¼Œè€Œéå›ºå®šé–“éš”

# å¦‚ä½•æ‰¾åˆ°çœŸå¯¦çš„ç« ç¯€è½‰æŠ˜é»ï¼š
## èªè¨€ä¿¡è™Ÿè©ï¼ˆè¬›å¸«è½‰æ›è©±é¡Œï¼‰ï¼š
- ã€Œæ¥ä¸‹ä¾†æˆ‘å€‘è¦è¬›...ã€ã€Œç¾åœ¨é€²å…¥...ã€ã€Œé¦–å…ˆ...ç¬¬äºŒ...ã€
- ã€Œæˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹...ã€ã€Œé€™å€‹éƒ¨åˆ†å®Œæˆå¾Œï¼Œæˆ‘å€‘ä¾†çœ‹...ã€
- ã€Œæœ‰äº†åŸºç¤æ¦‚å¿µï¼Œç¾åœ¨ä¾†å¯¦éš›æ“ä½œ...ã€
- ã€Œå•èˆ‡ç­”æ™‚é–“ã€ã€Œç¸½çµä¸€ä¸‹ã€ã€Œæˆ‘å€‘ä¾†ç·´ç¿’...ã€

## æ•™å­¸å…§å®¹è½‰æ›ï¼š
- æ–°æ¦‚å¿µ/æŠ€è¡“çš„é¦–æ¬¡è©³ç´°è§£é‡‹
- ç†è«–è¬›è§£ â†’ å¯¦éš›æ“ä½œçš„è½‰æ›
- ä¸åŒå·¥å…·/è»Ÿé«”çš„åˆ‡æ›æ™‚é–“é»
- ç¯„ä¾‹æ¼”ç¤ºçš„é–‹å§‹èˆ‡çµæŸ
- ç·´ç¿’é¡Œ/äº’å‹•ç’°ç¯€çš„é–‹å§‹

## è¦–è¦º/æ“ä½œè½‰æ›ï¼ˆåƒè€ƒOCRï¼‰ï¼š
- ç•«é¢åˆ‡æ›åˆ°æ–°æŠ•å½±ç‰‡/è»Ÿé«”ç•Œé¢
- é–‹å§‹å¯¦éš›æ“ä½œç¤ºç¯„
- æª”æ¡ˆé–‹å•Ÿ/å·¥å…·åˆ‡æ›çš„æ™‚é–“é»

# éŒ¯èª¤ç¤ºç¯„ vs æ­£ç¢ºåšæ³•ï¼š
## âŒ éŒ¯èª¤ï¼ˆçµ•å°é¿å…ï¼‰ï¼š
00:00:00 - èª²ç¨‹ä»‹ç´¹
00:15:00 - åŸºç¤æ¦‚å¿µ  
00:30:00 - é€²éšæ‡‰ç”¨
00:45:00 - å¯¦ä½œç·´ç¿’

## âœ… æ­£ç¢ºï¼ˆåŸºæ–¼å¯¦éš›å…§å®¹ï¼‰ï¼š
{first_ts} - èª²ç¨‹é–‹å ´èˆ‡å­¸ç¿’ç›®æ¨™èªªæ˜
[å°‹æ‰¾é€å­—ç¨¿ä¸­ç¬¬ä¸€å€‹ä¸»é¡Œè½‰æ›çš„æ™‚é–“æˆ³] - ç¬¬ä¸€å€‹ä¸»è¦æ¦‚å¿µè¬›è§£
[å°‹æ‰¾é€å­—ç¨¿ä¸­ç†è«–è½‰å¯¦ä½œçš„æ™‚é–“æˆ³] - å¯¦éš›æ“ä½œæ¼”ç¤ºé–‹å§‹
[å°‹æ‰¾é€å­—ç¨¿ä¸­é‡è¦ç¯„ä¾‹çš„æ™‚é–“æˆ³] - é—œéµç¯„ä¾‹åˆ†æ

# å½±ç‰‡è³‡è¨Š
- ç¸½æ™‚é•·: {duration_hms}
- é€å­—ç¨¿æ™‚é–“ç¯„åœ: {first_ts} åˆ° {last_ts}
- ç›®æ¨™ç« ç¯€: {t_low}-{t_high} å€‹å­¸ç¿’å–®å…ƒ
- æœ€å°é–“éš”: {min_gap_sec//60} åˆ†é˜

# åˆ†ææ­¥é©Ÿï¼š
1. **è­˜åˆ¥æ™‚é–“ç¯„åœ**ï¼šç¢ºèªé€å­—ç¨¿å¾ {first_ts} é–‹å§‹ï¼Œåˆ° {last_ts} çµæŸ
2. **é€šè®€å…§å®¹**ï¼šç†è§£æ•´é«”æ•™å­¸æµç¨‹å’ŒçŸ¥è­˜æ¶æ§‹
3. **æ¨™è¨˜è½‰æŠ˜**ï¼šæ‰¾å‡º {t_low}-{t_high} å€‹æœ€é‡è¦çš„ä¸»é¡Œè½‰æ›é»
4. **æ™‚é–“å°æ‡‰**ï¼šæ¯å€‹ç« ç¯€æ™‚é–“å¿…é ˆå°æ‡‰é€å­—ç¨¿ä¸­å¯¦éš›è¨è«–çš„æ™‚é–“
5. **æ¨™é¡Œç²¾æº–**ï¼šç”¨å…·é«”è¡“èªæè¿°è©²æ™‚é–“é»é–‹å§‹çš„æ•™å­¸å…§å®¹

# å…§å®¹è³‡æ–™
## ä¸»è¦é€å­—ç¨¿ï¼ˆåŒ…å«çœŸå¯¦æ™‚é–“æˆ³ï¼‰ï¼š
{transcript[:50000]}... [å…¶é¤˜å…§å®¹å·²è¼‰å…¥]

## è¼”åŠ©è¦–è¦ºå…§å®¹ï¼š
{ocr_context if ocr_context else "ï¼ˆç„¡è¢å¹•å…§å®¹åƒè€ƒï¼‰"}

# è¼¸å‡ºæ ¼å¼
## ç¬¬ä¸€éƒ¨åˆ†ï¼šç« ç¯€åˆ—è¡¨
åš´æ ¼éµå®ˆï¼š`HH:MM:SS - å…·é«”ç« ç¯€æ¨™é¡Œ`
- æ™‚é–“æˆ³å¿…é ˆæ˜¯é€å­—ç¨¿ä¸­å¯¦éš›å­˜åœ¨æˆ–éå¸¸æ¥è¿‘ï¼ˆÂ±60ç§’å…§ï¼‰çš„æ™‚é–“
- æ¨™é¡Œç”¨ç¹é«”ä¸­æ–‡ï¼Œå…·é«”æè¿°è©²æ™‚é–“é»é–‹å§‹çš„æ•™å­¸å…§å®¹

## ç¬¬äºŒéƒ¨åˆ†ï¼šèª²ç¨‹æ‘˜è¦ï¼ˆç« ç¯€åˆ—è¡¨å®Œæˆå¾Œï¼Œç©ºä¸€è¡Œè¼¸å‡ºï¼‰
è«‹æä¾›çµæ§‹åŒ–çš„èª²ç¨‹æ‘˜è¦ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

èª²ç¨‹ä¸»é¡Œï¼š[ä¸»è¦æ•™å­¸é ˜åŸŸï¼Œå¦‚ï¼šPythonç¨‹å¼è¨­è¨ˆã€Premiere Proå‰ªè¼¯]
æ ¸å¿ƒå…§å®¹ï¼š[åˆ—å‡º6-12å€‹ä¸»è¦æ•™å­¸æ¦‚å¿µï¼Œä»¥é “è™Ÿåˆ†éš”ï¼Œæ¶µè“‹æ•´å€‹èª²ç¨‹çš„é—œéµçŸ¥è­˜é»]
å­¸ç¿’ç›®æ¨™ï¼š[å­¸ç”Ÿå®Œæˆå¾Œæ‡‰å…·å‚™çš„èƒ½åŠ›]
é©åˆå°è±¡ï¼š[ç›®æ¨™å­¸å“¡èƒŒæ™¯]
é›£åº¦ç´šåˆ¥ï¼š[åˆç´š/ä¸­ç´š/é«˜ç´š]

# æœ€çµ‚æª¢æŸ¥
ç”Ÿæˆæ¯å€‹ç« ç¯€å‰ï¼Œå•è‡ªå·±ï¼š
1. é€™å€‹æ™‚é–“é»åœ¨é€å­—ç¨¿ä¸­æ˜¯å¦æœ‰å°æ‡‰çš„å…§å®¹è½‰æ›ï¼Ÿ
2. ç« ç¯€æ™‚é–“æ˜¯å¦åœ¨ {first_ts} åˆ° {last_ts} ç¯„åœå…§ï¼Ÿ
3. æ¨™é¡Œæ˜¯å¦æº–ç¢ºåæ˜ å¾é€™å€‹æ™‚é–“é»é–‹å§‹çš„æ•™å­¸å…§å®¹ï¼Ÿ

å®Œæˆç« ç¯€å¾Œï¼Œæª¢æŸ¥æ‘˜è¦ï¼š
1. èª²ç¨‹ä¸»é¡Œæ˜¯å¦æº–ç¢ºåæ˜ æ ¸å¿ƒæ•™å­¸å…§å®¹ï¼Ÿ
2. æ ¸å¿ƒå…§å®¹æ˜¯å¦åŒ…å«æœ€é‡è¦çš„2-3å€‹æŠ€è¡“é»ï¼Ÿ
3. å­¸ç¿’ç›®æ¨™æ˜¯å¦å…·é«”å¯è¡¡é‡ï¼Ÿ
"""
    return prompt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Hierarchical Multi-Pass Generation (NEW)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def should_use_hierarchical(duration: float, transcript_length: int) -> bool:
    """Determine if hierarchical multi-pass should be used"""
    # Use hierarchical for longer, content-rich educational videos
    return (duration >= 1800 and  # 30+ minutes
            transcript_length >= 5000 and  # Substantial content
            duration <= 14400)  # Under 4 hours (very long videos might need different handling)

def hierarchical_multipass_generation(
    raw_asr_text: str,
    duration: float,
    ocr_context: str,
    video_title: Optional[str],
    client: Any,
    config: ChapterConfig,
    progress_callback: Optional[Callable[[str, int], None]] = None
) -> Tuple[str, Dict[str, str], Dict[str, Any]]:
    """
    Three-pass hierarchical generation for high-quality educational chapters.
    
    Strategy:
    - ASR (Primary): Provides teaching content, explanations, natural timing, and narrative flow
    - OCR (Supporting): Provides visual structure, precise terminology, and organized summaries
    
    Prioritization:
    - Chapter timing: ASR timestamps (when instructor announces topics)
    - Chapter titles: ASR content enriched with OCR terminology
    - Q&A content: ASR explanations supplemented with OCR structured data

    
    Token Budget:
    - ASR: 100,000 tokens per pass (primary source)
    - OCR: 15,000 tokens per pass (supporting detail)
    - Total: ~115,000 content + ~2,000 instructions = ~117,000 tokens (safe for GPT-4o's 128k context)
    
    Returns: (raw_llm_text, chapters, metadata)
    """
    
    # ==================== Token Budget Initialization ====================
    ASR_LIMIT = 100_000   # ASR transcript limit per pass
    OCR_LIMIT = 15_000    # OCR context limit per pass
    
    asr_tokens = count_tokens_llama(raw_asr_text)
    ocr_tokens = count_tokens_llama(ocr_context) if ocr_context else 0
    total_content_tokens = asr_tokens + ocr_tokens
    
    logger.info("=" * 60)
    logger.info("ğŸ“ HIERARCHICAL MULTI-PASS CHAPTER GENERATION")
    logger.info("   Strategy: ASR-primary (timing) + OCR-supporting (detail)")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Original ASR tokens: {asr_tokens:,} (limit: {ASR_LIMIT:,})")
    logger.info(f"ğŸ“Š Original OCR tokens: {ocr_tokens:,} (limit: {OCR_LIMIT:,})")
    logger.info(f"ğŸ“Š Total content tokens: {total_content_tokens:,}")
    logger.info(f"ğŸ“Š Video duration: {sec_to_hms(int(duration))}")
    
    # Truncate once, reuse in all passes for consistency
    asr_text = truncate_text_by_tokens(raw_asr_text, ASR_LIMIT)
    ocr_text = truncate_text_by_tokens(ocr_context, OCR_LIMIT) if ocr_context else ""
    
    asr_used = count_tokens_llama(asr_text)
    ocr_used = count_tokens_llama(ocr_text)
    content_used = asr_used + ocr_used
    
    asr_coverage = (asr_used / asr_tokens * 100) if asr_tokens > 0 else 100
    ocr_coverage = (ocr_used / ocr_tokens * 100) if ocr_tokens > 0 else 100
    
    logger.info(f"âœ… Using per pass:")
    logger.info(f"   â€¢ ASR: {asr_used:,} tokens ({asr_coverage:.1f}% of original)")
    logger.info(f"   â€¢ OCR: {ocr_used:,} tokens ({ocr_coverage:.1f}% of original)")
    logger.info(f"   â€¢ Total: {content_used:,} tokens")
    
    if asr_tokens > ASR_LIMIT:
        logger.warning(f"âš ï¸ ASR truncated from {asr_tokens:,} to {asr_used:,} tokens")
    if ocr_tokens > OCR_LIMIT:
        logger.warning(f"âš ï¸ OCR truncated from {ocr_tokens:,} to {ocr_used:,} tokens")
    
    # ==================== PASS 1: Course Structure Analysis ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ” PASS 1: Course Structure Analysis")
    logger.info("   Goal: Identify learning objectives and overall architecture")
    logger.info("   Approach: Analyze both ASR and OCR equally")
    logger.info("-" * 60)
    
    if progress_callback:
        progress_callback("analyzing_course_structure", 40)
    
    video_info = ""
    if video_title:
        clean_title = re.sub(r'\.(mp4|avi|mov|mkv|webm|flv|m4v)$', '', video_title, flags=re.IGNORECASE)
        video_info = f"èª²ç¨‹æª”åï¼š{clean_title}\n"
        logger.info(f"ğŸ“š Video title: {clean_title}")
    
    structure_prompt = f"""
ä½œç‚ºè³‡æ·±æ•™å­¸è¨­è¨ˆå°ˆå®¶ï¼Œåˆ†æé€™å€‹{sec_to_hms(int(duration))}æ•™å­¸å½±ç‰‡çš„æ•´é«”æ¶æ§‹ï¼š

{video_info}
ã€æ ¸å¿ƒå­¸ç¿’ç›®æ¨™ã€‘
1. å­¸ç”Ÿå®Œæˆæœ¬èª²ç¨‹å¾Œæ‡‰æŒæ¡å“ªäº›é—œéµèƒ½åŠ›ï¼Ÿ
2. æœ‰å“ªäº›å¿…é ˆç†è§£çš„æ ¸å¿ƒç†è«–æˆ–æ¦‚å¿µï¼Ÿ
3. æœ‰å“ªäº›éœ€è¦ç†Ÿç·´çš„å¯¦ç”¨æŠ€èƒ½ï¼Ÿ

ã€çŸ¥è­˜æ¶æ§‹åˆ†æã€‘
- åŸºç¤é‹ªé™³ï¼šå“ªäº›æ˜¯å‰æçŸ¥è­˜æˆ–åŸºç¤æ¦‚å¿µï¼Ÿ
- æ ¸å¿ƒæ•™å­¸ï¼šæœ€é‡è¦çš„ç†è«–/æ–¹æ³•/æŠ€è¡“æ˜¯ä»€éº¼ï¼Ÿ
- æ‡‰ç”¨å»¶ä¼¸ï¼šå¦‚ä½•å°‡æ‰€å­¸æ‡‰ç”¨æ–¼å¯¦éš›å ´æ™¯ï¼Ÿ
- ç¸½çµæ•´åˆï¼šå¦‚ä½•å°‡é›¶æ•£çŸ¥è­˜ç³»çµ±åŒ–ï¼Ÿ

ã€æ•™å­¸æ–¹æ³•è­˜åˆ¥ã€‘
- ç†è«–è¬›è§£ vs. å¯¦ä¾‹æ¼”ç¤º vs. æ“ä½œç·´ç¿’ çš„æ¯”ä¾‹åˆ†ä½ˆ
- æ˜¯å¦æœ‰å•ç­”äº’å‹•ã€æ€è€ƒé¡Œã€é‡é»å›é¡§ï¼Ÿ

ã€åˆ†æè¦é»ã€‘
- å¾è¬›å¸«çš„æ•™å­¸æ•˜è¿°ï¼ˆASRï¼‰ç†è§£æ•™å­¸é‚è¼¯å’Œé‡é»
- å¾æŠ•å½±ç‰‡å…§å®¹ï¼ˆOCRï¼‰è­˜åˆ¥ä¸»è¦ç« ç¯€çµæ§‹å’Œå°ˆæ¥­è¡“èª
- ç¶œåˆå…©è€…ï¼Œå»ºæ§‹å®Œæ•´çš„èª²ç¨‹æ¡†æ¶

å®Œæ•´é€å­—ç¨¿ï¼ˆè¬›å¸«æ•™å­¸å…§å®¹èˆ‡æ™‚é–“è»¸ï¼‰ï¼š
{asr_text}

è¦–è¦ºè¼”åŠ©å…§å®¹ï¼ˆæŠ•å½±ç‰‡/è¢å¹•æ–‡å­—ï¼Œç”¨æ–¼ç¢ºèªä¸»é¡Œèˆ‡è¡“èªï¼‰ï¼š
{ocr_text if ocr_text else "ç„¡è¦–è¦ºè¼”åŠ©å…§å®¹"}
"""
    
    logger.info(f"ğŸ“¤ PASS 1 prompt: ~{count_tokens_llama(structure_prompt):,} tokens")
    logger.info("ğŸ¤– Calling LLM for structure analysis...")
    t0 = time.time()
    
    try:
        structure_response = call_llm(
            service_type=config.service_type,
            client=client,
            system_message="ä½ æ˜¯èª²ç¨‹æ¶æ§‹åˆ†æå°ˆå®¶ï¼Œæ“…é•·è­˜åˆ¥æ•™å­¸å½±ç‰‡çš„æ•´é«”å­¸ç¿’ç›®æ¨™å’ŒçŸ¥è­˜é«”ç³»ã€‚ä½ æœƒç¶œåˆåˆ†æè¬›å¸«è¬›è§£ï¼ˆASRï¼‰å’ŒæŠ•å½±ç‰‡å…§å®¹ï¼ˆOCRï¼‰ä¾†ç†è§£èª²ç¨‹çš„å®Œæ•´çµæ§‹ã€‚",
            user_message=structure_prompt,
            model=config.openai_model if config.service_type == "openai" else config.azure_model,
            max_tokens=1200,
            temperature=0.3
        )
        
        elapsed = time.time() - t0
        logger.info(f"âœ… PASS 1 completed in {elapsed:.1f}s")
        
        structure_text = (structure_response.choices[0].message.content 
                         if config.service_type == "openai" 
                         else structure_response.choices[0].message.content)
        
        logger.info(f"ğŸ“ Structure analysis: {len(structure_text)} characters")
        
    except Exception as e:
        logger.error(f"âŒ PASS 1 failed: {e}", exc_info=True)
        raise
    
    # ==================== PASS 2: Learning Modules Identification ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ“š PASS 2: Learning Modules Identification")
    logger.info("   Goal: Break down course into 7-12 coherent learning units")
    logger.info("   Approach: ASR-primary (conceptual transitions)")
    logger.info("-" * 60)
    
    if progress_callback:
        progress_callback("identifying_learning_modules", 60)
    
    modules_prompt = f"""
åŸºæ–¼èª²ç¨‹çµæ§‹åˆ†æï¼š
{structure_text}

ç¾åœ¨è­˜åˆ¥å…·é«”çš„å­¸ç¿’æ¨¡å¡Šï¼ˆ7-12å€‹ï¼‰ï¼Œæ¯å€‹æ¨¡å¡Šæ‡‰æ»¿è¶³ï¼š
1. æœ‰æ˜ç¢ºçš„å­¸ç¿’ç›®æ¨™
2. åŒ…å«å®Œæ•´çš„æ•™å­¸é–‰ç’°ï¼ˆè¬›è§£â†’ç¯„ä¾‹â†’ç·´ç¿’ï¼‰
3. æ™‚é•·åˆç†ï¼ˆ10-30åˆ†é˜ï¼‰
4. æœ‰æ¸…æ™°çš„é–‹å§‹å’ŒçµæŸæ¨™è¨˜

ã€æ¨¡å¡Šé‚Šç•Œè­˜åˆ¥ç­–ç•¥ï¼ˆé‡è¦æ€§æ’åºï¼‰ã€‘

**ç¬¬ä¸€å„ªå…ˆï¼šè¬›å¸«çš„é‡å¤§ä¸»é¡Œè½‰æ›ï¼ˆASR - ä¸»è¦ä¾æ“šï¼‰**
- æ˜ç¢ºçš„ç« ç¯€å®£å‘Šï¼š"æ¥ä¸‹ä¾†é€²å…¥æ–°çš„ç« ç¯€/éƒ¨åˆ†"ã€"ç¬¬ä¸€éƒ¨åˆ†/ç¬¬äºŒéƒ¨åˆ†"
- é‡å¤§å…§å®¹è½‰æ›ï¼š"åŸºç¤/ç†è«–è¬›å®Œäº†ï¼Œç¾åœ¨ä¾†çœ‹..."ã€"å¾æ¦‚å¿µåˆ°å¯¦è¸"
- æ•™å­¸æ–¹æ³•çš„é‡å¤§è½‰è®Šï¼šç†è«–è¬›è§£ â†’ å¯¦éš›æ“ä½œ â†’ æ¡ˆä¾‹åˆ†æ
- è¬›å¸«çš„ç¸½çµèˆ‡éæ¸¡ï¼š"æˆ‘å€‘å‰›æ‰è¬›äº†...ï¼Œç¾åœ¨ä¾†çœ‹..."

**ç¬¬äºŒå„ªå…ˆï¼šæ•™å­¸é‚è¼¯çš„çµæ§‹è½‰æ›ï¼ˆASRå…§å®¹åˆ†æï¼‰**
- å¾ç°¡å–®åˆ°è¤‡é›œçš„æ˜é¡¯å±¤ç´šè®ŠåŒ–
- å¾å–®ä¸€å·¥å…·/æ¦‚å¿µåˆ°ç¶œåˆæ‡‰ç”¨
- å¾è¬›è§£åˆ°ç·´ç¿’çš„è½‰æ›
- éšæ®µæ€§ç¸½çµå¾Œé–‹å§‹æ–°ä¸»é¡Œ

**ç¬¬ä¸‰å„ªå…ˆï¼šè¦–è¦ºçµæ§‹è®ŠåŒ–ï¼ˆOCR - è¼”åŠ©åƒè€ƒï¼‰**
- æŠ•å½±ç‰‡çš„å¤§æ¨™é¡Œè®ŠåŒ–ï¼ˆç« ç¯€ç·¨è™Ÿã€å¤§æ®µè½æ¨™è¨˜ï¼‰
- é¡¯è‘—çš„å…§å®¹é¡å‹åˆ‡æ›ï¼ˆç†è«–æŠ•å½±ç‰‡ â†’ è»Ÿé«”æ“ä½œç•Œé¢ â†’ å¯¦ä¾‹æ¼”ç¤ºï¼‰
- ç”¨æ–¼ç¢ºèªæ¨¡å¡Šçš„ä¸»é¡Œåç¨±å’Œå°ˆæ¥­è¡“èª

âš ï¸ é‡é»æé†’ï¼š
- æ¨¡å¡Šæ˜¯å¤§çš„å­¸ç¿’å–®å…ƒï¼Œä¸è¦è¢«é »ç¹çš„å°æ¨™é¡Œè®ŠåŒ–èª¤å°
- å–®å€‹æŠ•å½±ç‰‡è®ŠåŒ–ä¸ç­‰æ–¼æ¨¡å¡Šé‚Šç•Œ
- å„ªå…ˆé—œæ³¨è¬›å¸«çš„èªè¨€ä¿¡è™Ÿï¼ŒæŠ•å½±ç‰‡ç”¨æ–¼ç¢ºèªä¸»é¡Œ

å®Œæ•´é€å­—ç¨¿ï¼ˆä¸»è¦ä¾æ“š - åŒ…å«è¬›å¸«çš„ä¸»é¡Œè½‰æ›ä¿¡è™Ÿï¼‰ï¼š
{asr_text}

è¦–è¦ºè¼”åŠ©å…§å®¹ï¼ˆæ¬¡è¦åƒè€ƒ - ç”¨æ–¼ç¢ºèªä¸»é¡Œåç¨±ï¼‰ï¼š
{ocr_text if ocr_text else "ç„¡è¦–è¦ºè¼”åŠ©å…§å®¹"}

è«‹è¼¸å‡ºæ ¼å¼ï¼š
æ¨¡å¡Šåç¨± ~ é ä¼°æ™‚é–“ç¯„åœ ~ æ ¸å¿ƒå­¸ç¿’é» ~ æ•™å­¸æ–¹æ³•

ç¯„ä¾‹ï¼š
åŸºç¤å·¥å…·æ“ä½œ ~ 00:00-00:25 ~ Illustratorä»‹é¢èªè­˜ã€åŸºæœ¬å·¥å…·ä½¿ç”¨ ~ ç†è«–è¬›è§£+å¯¦ä¾‹æ¼”ç¤º
é€²éšè¨­è¨ˆæŠ€å·§ ~ 00:25-00:50 ~ Logoè¨­è¨ˆã€è‰²å½©ç®¡ç†ã€è¼¸å‡ºè¨­å®š ~ ç¶œåˆæ¡ˆä¾‹+å¯¦éš›æ“ä½œ
"""
    
    logger.info(f"ğŸ“¤ PASS 2 prompt: ~{count_tokens_llama(modules_prompt):,} tokens")
    logger.info("ğŸ¤– Calling LLM for module identification...")
    t0 = time.time()
    
    try:
        modules_response = call_llm(
            service_type=config.service_type,
            client=client,
            system_message="ä½ æ˜¯èª²ç¨‹æ¨¡å¡Šè¨­è¨ˆå¸«ï¼Œæ“…é•·å°‡æ•™å­¸å…§å®¹åˆ†è§£ç‚ºé‚è¼¯é€£è²«çš„å­¸ç¿’å–®å…ƒã€‚ä½ ä¸»è¦ä¾æ“šè¬›å¸«çš„èªè¨€ä¿¡è™Ÿï¼ˆASRï¼‰ä¾†è­˜åˆ¥æ¨¡å¡Šé‚Šç•Œï¼Œå› ç‚ºæ¨¡å¡Šæ˜¯åŸºæ–¼æ¦‚å¿µè½‰æ›è€Œéè¦–è¦ºè®ŠåŒ–ã€‚æŠ•å½±ç‰‡ï¼ˆOCRï¼‰ä¸»è¦ç”¨æ–¼ç¢ºèªæ¨¡å¡Šçš„ä¸»é¡Œåç¨±ã€‚",
            user_message=modules_prompt,
            model=config.openai_model if config.service_type == "openai" else config.azure_model,
            max_tokens=1500,
            temperature=0.2
        )
        
        elapsed = time.time() - t0
        logger.info(f"âœ… PASS 2 completed in {elapsed:.1f}s")
        
        modules_text = (modules_response.choices[0].message.content 
                       if config.service_type == "openai" 
                       else modules_response.choices[0].message.content)
        
        logger.info(f"ğŸ“ Modules analysis: {len(modules_text)} characters")
        
    except Exception as e:
        logger.error(f"âŒ PASS 2 failed: {e}", exc_info=True)
        raise
    
    # ==================== PASS 3: Detailed Chapter Generation ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ“‘ PASS 3: Detailed Chapter Generation")
    logger.info("   Goal: Create 15-30 precise chapter timestamps with titles")
    logger.info("   Approach: ASR-primary (timing) + OCR-supporting (detail)")
    logger.info("-" * 60)
    
    if progress_callback:
        progress_callback("generating_detailed_chapters", 80)
    
    chapters_prompt = f"""
ã€èª²ç¨‹æ•´é«”çµæ§‹ã€‘
{structure_text}

ã€å­¸ç¿’æ¨¡å¡Šè¦åŠƒã€‘  
{modules_text}

ç¾åœ¨ç‚ºæ¯å€‹æ¨¡å¡Šç”Ÿæˆå…·é«”çš„ç« ç¯€æ™‚é–“é»ï¼ˆç¸½å…±15-30å€‹ç« ç¯€ï¼‰ï¼Œä¸¦æä¾›èª²ç¨‹æ‘˜è¦ã€‚

ã€ç« ç¯€è¨­è¨ˆåŸå‰‡ã€‘
1. æ¯å€‹ç« ç¯€ä»£è¡¨ä¸€å€‹å®Œæ•´çš„å­¸ç¿’å­ç›®æ¨™ï¼ˆ5-10åˆ†é˜ï¼‰
2. æ¨™è¨˜é—œéµæ¦‚å¿µçš„é¦–æ¬¡è©³ç´°è§£é‡‹
3. æ¨™è¨˜é‡è¦ç¯„ä¾‹æˆ–æ¡ˆä¾‹åˆ†æçš„é–‹å§‹
4. æ¨™è¨˜ç·´ç¿’é¡Œæˆ–äº’å‹•ç’°ç¯€
5. æ¨™è¨˜é‡é»å›é¡§æˆ–ç¸½çµè™•

ã€ç« ç¯€æ™‚é–“é»å®šä½ç­–ç•¥ï¼ˆé‡è¦æ€§æ’åºï¼‰ã€‘

**ç¬¬ä¸€å„ªå…ˆï¼šASRèªè¨€æ™‚é–“æˆ³ï¼ˆä¸»è¦ä¾æ“š - æ±ºå®šç« ç¯€é–‹å§‹æ™‚é–“ï¼‰**
- è¬›å¸«çš„æ˜ç¢ºä¸»é¡Œå®£å‘Šï¼š"æ¥ä¸‹ä¾†æˆ‘å€‘è¦è¬›..."ã€"ç¾åœ¨é€²å…¥..."ã€"é¦–å…ˆ..."
- æ•™å­¸è½‰æ›ä¿¡è™Ÿï¼š"å¥½çš„ï¼Œé€™éƒ¨åˆ†å®Œæˆäº†"ã€"ç¾åœ¨ä¾†çœ‹..."ã€"æˆ‘å€‘ä¾†ç¤ºç¯„..."
- é‡è¦æ¦‚å¿µçš„é¦–æ¬¡è©³ç´°è§£é‡‹é–‹å§‹é»
- å¯¦ä¾‹æ¼”ç¤ºçš„æ˜ç¢ºé–‹å§‹ï¼š"æˆ‘å€‘ä¾†å¯¦éš›æ“ä½œä¸€ä¸‹..."
- ç·´ç¿’æˆ–äº’å‹•çš„é–‹å§‹ï¼š"å¤§å®¶è©¦è©¦çœ‹..."

**ç¬¬äºŒå„ªå…ˆï¼šå…§å®¹é‚è¼¯è½‰æ›ï¼ˆASRå…§å®¹åˆ†æï¼‰**
- å¾ç†è«–è¬›è§£åˆ°å¯¦éš›æ¼”ç¤ºçš„è‡ªç„¶è½‰æ›é»
- æ–°å·¥å…·/æŠ€è¡“çš„é¦–æ¬¡è©³ç´°ä»‹ç´¹
- å¾ç°¡å–®ç¯„ä¾‹åˆ°è¤‡é›œæ‡‰ç”¨çš„éæ¸¡
- éšæ®µæ€§å°çµå¾Œé–‹å§‹æ–°çš„å­ä¸»é¡Œ

**ç¬¬ä¸‰å„ªå…ˆï¼šOCRè¦–è¦ºè¼”åŠ©ï¼ˆè£œå……ç¢ºèª - æä¾›ç« ç¯€æ¨™é¡Œç´°ç¯€ï¼‰**
- ç¢ºèªç•¶å‰è¨è«–çš„å…·é«”ä¸»é¡Œï¼ˆæŠ•å½±ç‰‡æ¨™é¡Œæä¾›æº–ç¢ºåç¨±ï¼‰
- æä¾›ç²¾ç¢ºçš„æŠ€è¡“è¡“èªï¼ˆç•¶è¬›å¸«èªª"é€™å€‹å·¥å…·"æ™‚ï¼ŒOCRé¡¯ç¤º"çŸ©å½¢å·¥å…·"ï¼‰
- è£œå……è¦–è¦ºå…§å®¹æè¿°ï¼ˆåœ–è¡¨æ¨™é¡Œã€ä»£ç¢¼ç‰‡æ®µã€æ“ä½œæ­¥é©Ÿï¼‰
- ç•¶ASRè¡¨è¿°ä¸å¤ æ˜ç¢ºæ™‚ï¼Œåƒè€ƒè¢å¹•å…§å®¹ä¾†è£œå……ç´°ç¯€

ã€æ¨™é¡Œå‘½åè¦ç¯„ã€‘
- **å„ªå…ˆä½¿ç”¨è¬›å¸«çš„è‡ªç„¶è¡¨è¿°**ï¼ˆä¾†è‡ªASRï¼Œæ›´å£èªåŒ–ã€æ˜“æ‡‚ï¼‰
- **çµåˆæŠ•å½±ç‰‡çš„å°ˆæ¥­è¡“èª**ï¼ˆä¾†è‡ªOCRï¼Œæä¾›æº–ç¢ºçš„æŠ€è¡“åç¨±ï¼‰
- ä½¿ç”¨å…·é«”ã€å¯æ“ä½œçš„æè¿°ï¼ˆé¿å…"ä»‹ç´¹"ã€"èªªæ˜"ç­‰æ¨¡ç³Šè©å½™ï¼‰
- åŒ…å«æ‰€å±¬æ¨¡å¡Šæ¨™ç±¤ï¼ˆå¦‚ï¼š[åŸºç¤å·¥å…·]ã€[é€²éšæŠ€å·§]ã€[å¯¦æˆ°æ¡ˆä¾‹]ï¼‰
- æ¨™é¡Œæ ¼å¼ï¼š[æ¨¡å¡Šæ¨™ç±¤] å‹•ä½œ/å°è±¡/ç›®æ¨™

ã€æ™‚é–“é»é¸æ“‡çš„é»ƒé‡‘åŸå‰‡ã€‘
âš ï¸ ASRæ™‚é–“æˆ³è¨˜éŒ„äº†è¬›å¸«å¯¦éš›é–‹å§‹è¬›è§£æ–°ä¸»é¡Œçš„æ™‚é–“ - é€™æ˜¯æœ€è‡ªç„¶ã€æœ€ç¬¦åˆå­¸ç¿’ç¯€å¥çš„ç« ç¯€èµ·é»
âš ï¸ æŠ•å½±ç‰‡ï¼ˆOCRï¼‰é€šå¸¸åœ¨è¬›å¸«å®£å‘Šä¸»é¡Œä¹‹å¾Œæ‰å‡ºç¾ï¼Œç”¨æ–¼ç¢ºèªå…§å®¹å’Œæä¾›è¡“èªï¼Œè€Œéæ±ºå®šæ™‚é–“é»
âš ï¸ å„ªå…ˆé¸æ“‡è¬›å¸«æ˜ç¢ºå®£å‘Šæ–°ä¸»é¡Œçš„æ™‚é–“é»ï¼ˆå¾ASRï¼‰ä½œç‚ºç« ç¯€é–‹å§‹æ™‚é–“
âš ï¸ ä½¿ç”¨æŠ•å½±ç‰‡å…§å®¹ï¼ˆå¾OCRï¼‰ä¾†è±å¯Œå’Œç²¾ç¢ºåŒ–ç« ç¯€æ¨™é¡Œ

å®Œæ•´é€å­—ç¨¿ï¼ˆå«ç²¾ç¢ºæ™‚é–“æˆ³ - ä¸»è¦ç”¨æ–¼ç¢ºå®šç« ç¯€æ™‚é–“ï¼‰ï¼š
{asr_text}

è¦–è¦ºè¼”åŠ©å…§å®¹ï¼ˆæŠ•å½±ç‰‡/è¢å¹•æ–‡å­— - ä¸»è¦ç”¨æ–¼è±å¯Œç« ç¯€æ¨™é¡Œï¼‰ï¼š
{ocr_text if ocr_text else "ç„¡è¦–è¦ºè¼”åŠ©å…§å®¹"}

ç¸½æ™‚é•·ï¼š{sec_to_hms(int(duration))}

ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ã€‘
è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š

ç¬¬ä¸€éƒ¨åˆ† - ç« ç¯€åˆ—è¡¨ï¼š
HH:MM:SS - [æ¨¡å¡Šæ¨™ç±¤] å…·é«”ç« ç¯€æ¨™é¡Œ
ï¼ˆæ¯è¡Œä¸€å€‹ç« ç¯€ï¼Œæ™‚é–“ä¾†è‡ªASRï¼Œæ¨™é¡ŒçµåˆASRè¡¨è¿°èˆ‡OCRè¡“èªï¼‰

å„ªè³ªç¯„ä¾‹ï¼ˆæ™‚é–“é»ä¾†è‡ªè¬›å¸«å®£å‘Šï¼Œæ¨™é¡Œçµåˆè¬›å¸«è¡¨è¿°èˆ‡æŠ•å½±ç‰‡è¡“èªï¼‰ï¼š
00:10:03 - [åŸºç¤å·¥å…·ä»‹ç´¹] Adobe Illustratorçš„åŸºæœ¬æ“ä½œå’Œä»‹é¢è¨­ç½®
00:19:09 - [æ•ˆæœå·¥å…·æ‡‰ç”¨] ä½¿ç”¨æ•ˆæœå·¥å…·å‰µå»ºæ–‡å­—å½æ›²æ•ˆæœ
00:31:51 - [çŸ©å½¢å’Œé‹¼ç­†å·¥å…·] ä½¿ç”¨çŸ©å½¢å’Œé‹¼ç­†å·¥å…·å‰µå»ºåŸºæœ¬å½¢ç‹€å’Œç·šæ¢

ï¼ˆç©ºä¸€è¡Œï¼‰

ç¬¬äºŒéƒ¨åˆ† - èª²ç¨‹æ‘˜è¦ï¼š
èª²ç¨‹ä¸»é¡Œï¼š[ä¸»è¦æ•™å­¸é ˜åŸŸï¼Œå¦‚ï¼šAdobe IllustratoråŸºç¤æ•™å­¸ã€Pythonè³‡æ–™åˆ†æ]
æ ¸å¿ƒå…§å®¹ï¼š[åˆ—å‡º6-12å€‹ä¸»è¦æ•™å­¸æ¦‚å¿µï¼Œä»¥é “è™Ÿåˆ†éš”ï¼Œåæ˜ æ•´å€‹èª²ç¨‹çš„é—œéµçŸ¥è­˜é»]
å­¸ç¿’ç›®æ¨™ï¼š[å­¸ç”Ÿå®Œæˆå¾Œå°‡æŒæ¡çš„å…·é«”ã€å¯è¡¡é‡çš„èƒ½åŠ›]
é©åˆå°è±¡ï¼š[ç›®æ¨™å­¸å“¡èƒŒæ™¯ï¼Œå¦‚ï¼šè¨­è¨ˆåˆå­¸è€…ã€æœ‰ç¨‹å¼åŸºç¤çš„é€²éšå­¸å“¡]
é›£åº¦ç´šåˆ¥ï¼š[åˆç´š/ä¸­ç´š/é«˜ç´š]
"""
    
    logger.info(f"ğŸ“¤ PASS 3 prompt: ~{count_tokens_llama(chapters_prompt):,} tokens")
    logger.info("ğŸ¤– Calling LLM for final chapter generation...")
    t0 = time.time()
    
    try:
        final_response = call_llm(
            service_type=config.service_type,
            client=client,
            system_message="ä½ æ˜¯ç´°å¿ƒçš„ç« ç¯€è¨­è¨ˆå¸«ï¼Œæ“…é•·ç‚ºå­¸ç¿’æ¨¡å¡Šå‰µå»ºç²¾ç¢ºçš„æ™‚é–“æ¨™è¨˜å’Œèª²ç¨‹æ‘˜è¦ã€‚ä½ çš„æ ¸å¿ƒåŸå‰‡æ˜¯ï¼šä½¿ç”¨ASRæ™‚é–“æˆ³ä¾†ç¢ºå®šç« ç¯€é–‹å§‹æ™‚é–“ï¼ˆå› ç‚ºé€™åæ˜ è¬›å¸«çš„è‡ªç„¶æ•™å­¸ç¯€å¥ï¼‰ï¼Œä½¿ç”¨OCRå…§å®¹ä¾†è±å¯Œç« ç¯€æ¨™é¡Œï¼ˆæä¾›æº–ç¢ºçš„å°ˆæ¥­è¡“èªï¼‰ã€‚è«‹åš´æ ¼éµå®ˆè¼¸å‡ºæ ¼å¼ï¼Œç¢ºä¿åŒ…å«ç« ç¯€åˆ—è¡¨å’Œèª²ç¨‹æ‘˜è¦å…©éƒ¨åˆ†ã€‚",
            user_message=chapters_prompt,
            model=config.openai_model if config.service_type == "openai" else config.azure_model,
            max_tokens=3000,
            temperature=0.1
        )
        
        elapsed = time.time() - t0
        logger.info(f"âœ… PASS 3 completed in {elapsed:.1f}s")
        
        final_text = (final_response.choices[0].message.content 
                     if config.service_type == "openai" 
                     else final_response.choices[0].message.content)
        
        logger.info(f"ğŸ“ Final output: {len(final_text)} characters")
        
    except Exception as e:
        logger.error(f"âŒ PASS 3 failed: {e}", exc_info=True)
        raise
    
    # ==================== Parse Results ====================
    logger.info("\n" + "-" * 60)
    logger.info("ğŸ” Parsing Generated Content")
    logger.info("-" * 60)
    
    # Parse chapters from final output
    chapters = parse_chapters_from_output(final_text)
    logger.info(f"ğŸ“Š Parsed {len(chapters)} chapters from LLM output")
    
    if chapters:
        first_chapter = list(chapters.keys())[0]
        last_chapter = list(chapters.keys())[-1]
        logger.info(f"ğŸ“ Chapter range: {first_chapter} to {last_chapter}")
    else:
        logger.warning("âš ï¸ No chapters were parsed from LLM output!")
    
    # Parse structured summary
    course_summary = parse_summary_from_output(final_text)
    
    if course_summary:
        logger.info(f"âœ… Successfully extracted course summary with {len(course_summary)} fields:")
        for key, value in course_summary.items():
            display_value = value[:80] + "..." if len(value) > 80 else value
            logger.info(f"   â€¢ {key}: {display_value}")
    else:
        logger.warning("âš ï¸ Course summary extraction failed, using empty dict")
    
    # Calculate educational quality score
    quality_score = estimate_educational_quality(chapters, structure_text)
    logger.info(f"ğŸ“ˆ Educational quality score: {quality_score:.2f}")
    
    # ==================== Build Metadata ====================
    metadata = {
        'generation_method': 'hierarchical_multi_pass_asr_primary',
        'strategy': 'ASR-primary for timing, OCR-supporting for detail',
        'structure_analysis': structure_text,
        'modules_analysis': modules_text,
        'educational_quality_score': quality_score,
        'course_summary': course_summary,
        'token_usage': {
            'original': {
                'asr_tokens': asr_tokens,
                'ocr_tokens': ocr_tokens,
                'total_tokens': total_content_tokens
            },
            'used_per_pass': {
                'asr_tokens': asr_used,
                'ocr_tokens': ocr_used,
                'total_tokens': content_used
            },
            'limits': {
                'asr_limit': ASR_LIMIT,
                'ocr_limit': OCR_LIMIT
            },
            'coverage': {
                'asr_coverage': f"{asr_coverage:.1f}%",
                'ocr_coverage': f"{ocr_coverage:.1f}%"
            }
        }
    }
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… HIERARCHICAL GENERATION COMPLETE")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Chapters generated: {len(chapters)}")
    logger.info(f"ğŸ“Š Summary fields: {len(course_summary)}")
    logger.info(f"ğŸ“Š Quality score: {quality_score:.2f}")
    logger.info(f"ğŸ“Š Strategy: ASR-primary (timing) + OCR-supporting (detail)")
    logger.info(f"ğŸ“Š Total content used: {content_used:,} tokens (ASR: {asr_used:,}, OCR: {ocr_used:,})")
    logger.info("=" * 60 + "\n")
    
    return final_text, chapters, metadata

def estimate_educational_quality(chapters: Dict[str, str], structure: str) -> float:
    """Simple heuristic to estimate educational quality of chapters"""
    quality_indicators = [
        'è¬›è§£', 'åŸç†', 'ç¯„ä¾‹', 'ç·´ç¿’', 'å¯¦ä½œ', 'æ‡‰ç”¨', 'ç¸½çµ', 'é‡é»',
        'æ¦‚å¿µ', 'æ–¹æ³•', 'æŠ€å·§', 'æ­¥é©Ÿ', 'æ¡ˆä¾‹', 'åˆ†æ'
    ]
    
    title_text = ' '.join(chapters.values())
    indicator_count = sum(1 for indicator in quality_indicators 
                         if indicator in title_text)
    
    total_titles = len(chapters)
    return min(1.0, indicator_count / max(1, total_titles * 0.7))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Enhanced Main Function with Smart Routing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_chapters_debug(
    raw_asr_text: str,
    ocr_segments: List[Dict],
    duration: float,
    video_id: str,
    video_title: Optional[str] = None,  # ADD THIS
    run_dir: Optional[Path] = None,
    progress_callback: Optional[Callable[[str, int], None]] = None,
    *,
    ocr_context_override: Optional[str] = None,
    # NEW: Add control parameter
    force_generation_method: Optional[str] = None,  # 'hierarchical' or 'single_pass'
) -> Tuple[str, Dict[str, str], Dict[str, str]]:
    """
    Enhanced version with smart routing between hierarchical and single-pass generation
    """
    if progress_callback:
        progress_callback("initializing", 0)

    if run_dir is None:
        run_dir = Path(f"/tmp/chapter_generation/{video_id}_{int(time.time())}")
    run_dir.mkdir(parents=True, exist_ok=True)

    try:
        logger.info(f"Starting chapter generation for video {video_id} (duration: {duration}s)")

        # Load configuration
        config = ChapterConfig()
        if not validate_config(config):
            logger.warning("Configuration validation failed, using time-based fallback")
            fallback = create_time_based_fallback(int(duration))
            fallback = ensure_traditional_chapters(fallback)
            return ("", {}, fallback)

        if progress_callback:
            progress_callback("processing_inputs", 10)

        # Build OCR context (existing logic)
        if ocr_context_override is not None:
            ocr_context = ocr_context_override
        else:
            ocr_context = build_ocr_context_from_segments(ocr_segments) if ocr_segments else ""

        min_gap_sec, target_range, max_caps = chapter_policy(int(duration))
        
        # Save raw inputs
        with open(run_dir / "raw_asr_text.txt", "w", encoding="utf-8") as f:
            f.write(raw_asr_text)
        if ocr_context_override is not None:
            with open(run_dir / "ocr_raw.txt", "w", encoding="utf-8") as f:
                f.write(ocr_context_override)
        else:
            with open(run_dir / "ocr_segments.json", "w", encoding="utf-8") as f:
                json.dump(ocr_segments, f, ensure_ascii=False, indent=2)

        if progress_callback:
            progress_callback("initializing_client", 20)

        # Initialize client (existing logic)
        service_type = config.service_type
        model = config.openai_model if service_type == "openai" else config.azure_model

        if service_type == "azure":
            client = initialize_client(
                service_type="azure",
                endpoint=config.azure_endpoint,
                key=config.azure_key,
                api_version=config.azure_api_version,
            )
        else:
            client = initialize_client(
                service_type="openai",
                api_key=config.openai_api_key,
                base_url=config.openai_base_url,
            )

        # ğŸ¯ NEW: Smart Generation Method Selection
        use_hierarchical = False
        if force_generation_method == 'hierarchical':
            use_hierarchical = True
        elif force_generation_method == 'single_pass':
            use_hierarchical = False
        else:
            # Auto-detect based on content characteristics
            use_hierarchical = should_use_hierarchical(duration, len(raw_asr_text))
        
        logger.info(f"Using generation method: {'hierarchical_multi_pass' if use_hierarchical else 'single_pass'}")

        if use_hierarchical:
            if progress_callback:
                progress_callback("hierarchical_analysis", 30)
            
            # Use hierarchical multi-pass generation
            raw_llm_text, chapters, metadata = hierarchical_multipass_generation(
                raw_asr_text=raw_asr_text,
                duration=duration,
                ocr_context=ocr_context,
                video_title=video_title,  # ADD THIS
                client=client,
                config=config,
                progress_callback=progress_callback
            )
            
            # Save hierarchical metadata
            with open(run_dir / "hierarchical_metadata.json", "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            with open(run_dir / "course_structure.txt", "w", encoding="utf-8") as f:
                f.write(metadata.get('structure_analysis', ''))
            with open(run_dir / "learning_modules.txt", "w", encoding="utf-8") as f:
                f.write(metadata.get('modules_analysis', ''))
                
        else:
            if progress_callback:
                progress_callback("single_pass_processing", 30)
            
            # Use original single-pass generation
            prompt_template = build_prompt_body("", int(duration), ocr_context)
            template_tokens = count_tokens_llama(prompt_template)
            CONTEXT_BUDGET = 128_000
            max_transcript_tokens = max(0, CONTEXT_BUDGET - template_tokens)
            transcript_for_prompt = truncate_text_by_tokens(raw_asr_text, max_transcript_tokens)
            full_prompt = build_prompt_body(transcript_for_prompt, int(duration), ocr_context, video_title)

            with open(run_dir / "full_prompt.txt", "w", encoding="utf-8") as f:
                f.write(full_prompt)

            if progress_callback:
                progress_callback("calling_llm", 50)

            enhanced_system_message = (
                "ä½ æ˜¯å°ˆæ¥­çš„ç·šä¸Šèª²ç¨‹è¨­è¨ˆå°ˆå®¶ï¼Œæ“…é•·ç‚ºå„ç¨®å­¸ç§‘å‰µå»ºé«˜å“è³ªæ•™è‚²ç« ç¯€çµæ§‹ã€‚"
                "è‡ªå‹•è­˜åˆ¥èª²ç¨‹é ˜åŸŸä¸¦ä½¿ç”¨é©ç•¶å°ˆæ¥­è¡“èªï¼Œå°ˆæ³¨æ–¼å­¸ç¿’åƒ¹å€¼å’Œæ•™è‚²é€£è²«æ€§ã€‚"
                "åš´æ ¼é¿å…é‡è¤‡æ¨¡å¼ï¼Œå‰µå»ºåæ˜ çœŸå¯¦æ•™è‚²é€²ç¨‹çš„å°ˆæ¥­ç« ç¯€æ¨™é¡Œã€‚"
                "åƒ…è¼¸å‡ºç« ç¯€æ¸…å–®ï¼Œæ¯è¡Œæ ¼å¼: `HH:MM:SS - æ¨™é¡Œ`ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‚"
            )

            logger.info(f"Calling {service_type} API for single-pass chapter generation...")
            t0 = time.time()
            resp = call_llm(
                service_type=service_type,
                client=client,
                system_message=enhanced_system_message,
                user_message=full_prompt,
                model=model,
                max_tokens=2048,
                temperature=0.2,
                top_p=0.9,
            )
            dt = time.time() - t0
            logger.info(f"LLM API call completed in {dt:.2f}s")

            if service_type == "azure":
                raw_llm_text = resp.choices[0].message.content
            else:
                raw_llm_text = resp.choices[0].message.content

            # Parse chapters
            chapters = parse_chapters_from_output(raw_llm_text)
            # Parse structured summary
            course_summary = parse_summary_from_output(raw_llm_text)
            metadata = {'generation_method': 'single_pass',
                        'course_summary': course_summary}

        # COMMON POST-PROCESSING (existing logic)
        if progress_callback:
            progress_callback("parsing_response", 70)

        with open(run_dir / "llm_output_raw.txt", "w", encoding="utf-8") as f:
            f.write(raw_llm_text)

        # Apply cleaning and Traditional Chinese conversion
        parsed_raw_clean_trad = ensure_traditional_chapters(clean_chapter_titles(chapters))

        with open(run_dir / "parsed_raw_chapters.json", "w", encoding="utf-8") as f:
            json.dump(parsed_raw_clean_trad, f, ensure_ascii=False, indent=2)

        if progress_callback:
            progress_callback("balancing_chapters", 80)

        # Balance according to policy
        chapters_final = globally_balance_chapters(
            parsed_raw_clean_trad, int(duration), min_gap_sec, target_range, max_caps
        )
        if not chapters_final:
            raise RuntimeError("No chapters left after balancing")

        with open(run_dir / "chapters_final.json", "w", encoding="utf-8") as f:
            json.dump(chapters_final, f, ensure_ascii=False, indent=2)

        # Save generation method info
        with open(run_dir / "generation_method.txt", "w", encoding="utf-8") as f:
            f.write(metadata.get('generation_method', 'unknown'))

        if progress_callback:
            progress_callback("completed", 100)

        return (raw_llm_text, parsed_raw_clean_trad, chapters_final)

    except Exception as e:
        logger.error(f"Chapter generation failed: {e}", exc_info=True)
        fallback = ensure_traditional_chapters(create_time_based_fallback(int(duration)))
        return ("", {}, fallback)
        
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN FUNCTIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def create_time_based_fallback(duration_sec: int) -> Dict[str, str]:
    """Create fallback chapters based on time intervals"""
    fallback_chapters: Dict[str, str] = {}
    interval = 300  # 5 minutes
    for i in range(0, int(duration_sec), interval):
        fallback_chapters[sec_to_hms(i)] = "ç« ç¯€ " + str((i // interval) + 1)
    logger.info(f"Created {len(fallback_chapters)} time-based fallback chapters")
    return fallback_chapters

def generate_chapters(
    raw_asr_text: str,
    ocr_segments: List[Dict],
    duration: float,
    video_id: str,
    video_title: Optional[str] = None,
    run_dir: Optional[Path] = None,
    progress_callback: Optional[Callable[[str, int], None]] = None,
    *,
    ocr_context_override: Optional[str] = None,
    # NEW: Add the same parameter here for consistency
    force_generation_method: Optional[str] = None,  # 'hierarchical' or 'single_pass'
) -> Dict[str, str]:
    """
    Backward-compatible wrapper returning only the FINAL balanced chapters.
    If you want raw model output as well, call `generate_chapters_debug` instead.
    """
    _raw_text, _parsed_raw, final_chapters = generate_chapters_debug(
        raw_asr_text=raw_asr_text,
        ocr_segments=ocr_segments,
        duration=duration,
        video_id=video_id,
        run_dir=run_dir,
        progress_callback=progress_callback,
        ocr_context_override=ocr_context_override,
        force_generation_method=force_generation_method  # Pass through the new parameter
    )
    return final_chapters

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(description="Generate video chapters from raw ASR and optional OCR.")
    parser.add_argument('--asr-file', type=argparse.FileType('r', encoding='utf-8'), required=True,
                        help='Path to file containing raw ASR text with timestamps.')
    parser.add_argument('--ocr-file', type=argparse.FileType('r', encoding='utf-8'),
                        help='Optional path to OCR file. In verbatim mode this is read as raw text.')
    parser.add_argument('--duration', type=float, required=True,
                        help='Duration of the video in seconds.')
    parser.add_argument('--video-id', type=str, required=True,
                        help='Unique identifier for the video (used for output directory).')
    parser.add_argument('--output-dir', type=str, default='./chapter_debug',
                        help='Directory to save debug outputs. Default: ./chapter_debug')
    parser.add_argument('--debug', action='store_true', help='Print RAW LLM output and parsed chapters too.')
    parser.add_argument('--ocr-mode', choices=['none', 'verbatim', 'segments'], default='verbatim',
                        help="How to include OCR: 'none' (omit), 'verbatim' (raw text), or 'segments' (legacy minimal formatting).")

    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        stream=sys.stdout
    )

    # Read ASR
    logger.info(f"Reading ASR text from {args.asr_file.name}...")
    raw_asr_text = args.asr_file.read()
    args.asr_file.close()

    # Read OCR according to the chosen mode
    ocr_segments: List[Dict] = []
    ocr_context_override: Optional[str] = None
    if args.ocr_file:
        if args.ocr_mode == 'none':
            logger.info("OCR mode: none (omit OCR from prompt).")
            try:
                args.ocr_file.close()
            except Exception:
                pass
        elif args.ocr_mode == 'verbatim':
            logger.info(f"OCR mode: verbatim. Reading {args.ocr_file.name} as raw text...")
            try:
                ocr_context_override = args.ocr_file.read()
            finally:
                try:
                    args.ocr_file.close()
                except Exception:
                    pass
            logger.info("OCR loaded verbatim.")
        else:
            logger.info(f"OCR mode: segments. Reading OCR segments from {args.ocr_file.name}...")
            try:
                ocr_segments = load_ocr_segments(args.ocr_file, args.ocr_file.name)
                args.ocr_file.close()
                logger.info(f"Loaded {len(ocr_segments)} OCR segments")
            except Exception as e:
                logger.warning(f"OCR file load failed, proceeding without OCR. Detail: {e}")
                try:
                    args.ocr_file.close()
                except Exception:
                    pass
                ocr_segments = []

    # Output directory
    run_dir = Path(args.output_dir) / args.video_id
    run_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"Saving debug outputs to: {run_dir}")

    # Simple progress callback
    def cli_progress_callback(stage: str, percent: int):
        logger.info(f"Progress: {percent}% - {stage}")

    # Generate
    logger.info("Starting chapter generation...")
    raw_text, parsed_raw, final_chapters = generate_chapters_debug(
        raw_asr_text=raw_asr_text,
        ocr_segments=ocr_segments,
        duration=args.duration,
        video_id=args.video_id,
        run_dir=run_dir,
        progress_callback=cli_progress_callback,
        ocr_context_override=ocr_context_override,  # raw OCR passes straight through
    )

    # Console output
    print("\n" + "="*50)
    print("âœ… CHAPTER GENERATION COMPLETE")
    print("="*50)

    if args.debug:
        print("\n--- RAW LLM OUTPUT (as returned) ---")
        print(raw_text if raw_text else "(empty/raw fallback)")
        print("\n--- PARSED (pre-balance) ---")
        for ts, title in parsed_raw.items():
            print(f"{ts} - {title}")

    print("\n--- FINAL (balanced) ---")
    for ts, title in final_chapters.items():
        print(f"{ts} - {title}")

    # Save final chapters to a clean file
    output_file = run_dir / "final_chapters.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        for timestamp, title in final_chapters.items():
            f.write(f"{timestamp} - {title}\n")
    logger.info(f"Final chapters saved to: {output_file}")

    # Also save a pre-balance view for convenience
    pre_file = run_dir / "parsed_raw_chapters.txt"
    with open(pre_file, 'w', encoding='utf-8') as f:
        for timestamp, title in parsed_raw.items():
            f.write(f"{timestamp} - {title}\n")
    logger.info(f"Parsed (pre-balance) chapters saved to: {pre_file}")

if __name__ == "__main__":
    main()
