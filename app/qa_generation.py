# app/qa_generation.py
"""
Module for generating educational content (MCQs and Lecture Notes) from pre-processed ASR and OCR segments.
Designed for enhancing student learning and review of educational materials.

This version:
- ASR-first prompts for MCQs and Lecture Notes (OCR is auxiliary; conflict -> ASR wins)
- Bloom-structured MCQs; detailed, past-tense lecture notes; strict JSON outputs
- Simplified->Traditional conversion safety net (OpenCC if available, else fallback map)
- NEW: Post-processing helpers (shuffle options, regenerate explanations, enforce difficulty)
- NEW: Post-processing **controlled by function parameters**, not environment variables
"""

import hashlib
import json
import logging
import os
import re
import time
import random
from datetime import datetime, timezone  # â† ADD THIS LINE
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Callable, Any
from pydantic import ValidationError


# Azure AI Inference imports
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

# OpenAI import
from openai import OpenAI

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

logger = logging.getLogger(__name__)

# ============================================================================
# UNIT VALIDATION SYSTEM
# ============================================================================

# Load threshold from environment
UNIT_VALIDATION_THRESHOLD = float(os.getenv("UNIT_VALIDATION_THRESHOLD", "0.35"))

def validate_units_relevance(
    units: List[Dict],
    content_analysis: Dict,
    chapters: Dict[str, str],
    video_title: str,
    threshold: float = None
) -> Tuple[bool, float, str]:
    """
    Validate if client-provided units are relevant to video content.
    
    Uses LLM to compare units against actual video content to prevent
    wrong/irrelevant units from poisoning Q&A and lecture notes.
    
    Args:
        units: Client-provided learning units
        content_analysis: From Pass 1 (topics, concepts, terms)
        chapters: AI-generated chapters
        video_title: Video title
        threshold: Minimum similarity score (defaults to env var)
    
    Returns:
        Tuple of (is_valid, similarity_score, reason)
        
    Examples:
        >>> # Good match
        >>> validate_units_relevance(
        ...     units=[{"Title": "Photoshopåœ–å±¤"}],
        ...     content_analysis={"main_topics": ["åœ–å±¤åˆæˆ", "é®è‰²ç‰‡"]},
        ...     chapters={"00:05:00": "åœ–å±¤åŸºç¤Ž"},
        ...     video_title="Photoshopæ•™å­¸"
        ... )
        (True, 0.92, "å­¸ç¿’å–®å…ƒèˆ‡å½±ç‰‡å…§å®¹é«˜åº¦ç›¸é—œ")
        
        >>> # Bad match
        >>> validate_units_relevance(
        ...     units=[{"Title": "å»šå…·è¦åŠƒ"}],
        ...     content_analysis={"main_topics": ["åœ–å±¤åˆæˆ", "é®è‰²ç‰‡"]},
        ...     chapters={"00:05:00": "åœ–å±¤åŸºç¤Ž"},
        ...     video_title="Photoshopæ•™å­¸"
        ... )
        (False, 0.15, "å­¸ç¿’å–®å…ƒä¸»é¡Œç‚ºå®¤å…§è¨­è¨ˆï¼Œèˆ‡å½±ç‰‡PhotoshopæŠ€è¡“å…§å®¹å®Œå…¨ä¸ç›¸é—œ")
    """
    if threshold is None:
        threshold = UNIT_VALIDATION_THRESHOLD
    
    # No units provided = automatically valid
    if not units or not isinstance(units, list):
        logger.info("â„¹ï¸  No units provided for validation")
        return True, 1.0, "No units provided"
    
    # Extract unit titles
    unit_titles = [u.get("Title", "").strip() for u in units if u.get("Title")]
    if not unit_titles:
        logger.warning("âš ï¸  Units provided but have no titles")
        return True, 1.0, "Units have no titles"
    
    # Build content summary from Pass 1 analysis
    main_topics = content_analysis.get("main_topics", [])[:5]  # Top 5
    key_concepts = content_analysis.get("key_concepts", [])[:10]  # Top 10
    technical_terms = content_analysis.get("technical_terms", [])[:10]  # Top 10
    
    # Sample chapter titles
    chapter_titles = []
    if chapters:
        chapter_titles = [title for title in list(chapters.values())[:5]]
    
    content_summary = {
        "video_title": video_title or "æœªæä¾›",
        "main_topics": main_topics,
        "key_concepts": key_concepts,
        "technical_terms": technical_terms,
        "chapter_samples": chapter_titles
    }
    
    # Build validation prompt
    prompt = f"""ä½ æ˜¯æ•™è‚²å…§å®¹é©—è­‰å°ˆå®¶ã€‚è«‹è©•ä¼°æä¾›çš„å­¸ç¿’å–®å…ƒæ˜¯å¦èˆ‡å½±ç‰‡å¯¦éš›å…§å®¹ç›¸é—œã€‚

å½±ç‰‡å…§å®¹æ‘˜è¦ï¼š
{json.dumps(content_summary, ensure_ascii=False, indent=2)}

æä¾›çš„å­¸ç¿’å–®å…ƒï¼š
{chr(10).join(f"{i+1}. {title}" for i, title in enumerate(unit_titles))}

è«‹åˆ†æžï¼š
1. å­¸ç¿’å–®å…ƒçš„ä¸»é¡Œæ˜¯å¦èˆ‡å½±ç‰‡å…§å®¹ç›¸ç¬¦ï¼Ÿ
2. ç›¸é—œåº¦è©•åˆ†ï¼ˆ0-1ä¹‹é–“çš„å°æ•¸ï¼‰
   - 0.0-0.2: å®Œå…¨ä¸ç›¸é—œï¼ˆä¾‹å¦‚ï¼šå½±ç‰‡è¬›Photoshopï¼Œå–®å…ƒæ˜¯å»šæˆ¿è¨­è¨ˆï¼‰
   - 0.2-0.35: ç›¸é—œæ€§å¾ˆå¼±ï¼ˆä¾‹å¦‚ï¼šä¸»é¡Œç›¸è¿‘ä½†å…§å®¹ä¸ç¬¦ï¼‰
   - 0.35-0.6: ä¸­ç­‰ç›¸é—œï¼ˆä¾‹å¦‚ï¼šåŒé ˜åŸŸä½†é‡é»žä¸åŒï¼‰
   - 0.6-0.8: é«˜åº¦ç›¸é—œï¼ˆä¾‹å¦‚ï¼šä¸»é¡Œä¸€è‡´ï¼Œå…§å®¹å»åˆï¼‰
   - 0.8-1.0: å®Œç¾ŽåŒ¹é…ï¼ˆä¾‹å¦‚ï¼šå–®å…ƒç²¾ç¢ºå°æ‡‰å½±ç‰‡ç« ç¯€ï¼‰
3. ç°¡çŸ­åŽŸå› èªªæ˜Žï¼ˆ30å­—å…§ï¼‰

è«‹å‹™å¿…ä»¥ JSON æ ¼å¼å›žç­”ï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "is_relevant": true,
    "relevance_score": 0.85,
    "reason": "å­¸ç¿’å–®å…ƒèˆ‡å½±ç‰‡å…§å®¹é«˜åº¦ç›¸é—œï¼Œæ¶µè“‹äº†ä¸»è¦ä¸»é¡Œ"
}}"""
    
    try:
        # Use project's configured model instead of hardcoded GPT-4
        config = EducationalContentConfig()
        service_type, model, validation_client = initialize_and_get_client(config)
        
        logger.debug(f"Using {model} for unit validation")

        response = call_llm(
            service_type=service_type,
            client=validation_client,
            system_message="ä½ æ˜¯æ•™è‚²å…§å®¹é©—è­‰å°ˆå®¶ã€‚",
            user_message=prompt,
            model=model,
            max_tokens=200,
            temperature=0.2,
            top_p=0.9
        )

        result_text = extract_text_from_response(response, service_type).strip()
        
        # Extract JSON (handle markdown code blocks)
        result_text = result_text.replace("```json", "").replace("```", "").strip()
        
        # Find JSON object
        json_match = re.search(r'\{[^{}]*\}', result_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            is_relevant = result.get("is_relevant", False)
            score = float(result.get("relevance_score", 0.0))
            reason = result.get("reason", "æœªæä¾›åŽŸå› ")
            
            # Apply threshold
            is_valid = is_relevant and score >= threshold
            
            # Log validation result
            logger.info(f"ðŸ“Š Unit Validation Results:")
            logger.info(f"   â€¢ Units Provided: {len(unit_titles)}")
            logger.info(f"   â€¢ Relevance Score: {score:.2f}")
            logger.info(f"   â€¢ Threshold: {threshold:.2f}")
            logger.info(f"   â€¢ Status: {'âœ… ACCEPTED' if is_valid else 'âŒ REJECTED'}")
            logger.info(f"   â€¢ Reason: {reason}")
            
            return is_valid, score, reason
        else:
            logger.error("âŒ Could not parse validation JSON response")
            logger.error(f"   Response: {result_text[:200]}")
            # Fail open - assume valid to avoid blocking legitimate content
            return True, 1.0, "Validation parsing failed (assumed valid)"
            
    except Exception as e:
        logger.error(f"âŒ Unit validation error: {e}", exc_info=True)
        # Fail open - assume valid to avoid blocking legitimate content
        return True, 1.0, f"Validation error: {str(e)[:50]}"


def log_validation_metrics(
    video_id: str,
    units_provided: int,
    validation_score: float,
    accepted: bool,
    threshold: float
):
    """
    Log validation metrics for monitoring and analysis.
    
    Saves to /workspace/logs/unit_validations.jsonl for later analysis.
    """
    try:
        log_dir = "/workspace/logs"
        os.makedirs(log_dir, exist_ok=True)
        
        validation_summary = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "video_id": str(video_id),
            "units_provided": units_provided,
            "validation_score": round(validation_score, 3),
            "accepted": accepted,
            "threshold": threshold
        }
        
        log_file = os.path.join(log_dir, "unit_validations.jsonl")
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(validation_summary, ensure_ascii=False) + "\n")
            
    except Exception as e:
        logger.warning(f"âš ï¸  Failed to log validation metrics: {e}")
        
# ==================== PROGRESS ====================
STAGES = {
    "initializing": 5,
    "processing_inputs": 15,
    "initializing_client": 25,
    "generating_topics_summary": 40,
    "generating_mcqs": 55,
    "generating_notes": 80,
    "processing_results": 92,
    "completed": 100,
}

def report(stage: str, progress_callback: Optional[Callable[[str, int], None]]):
    if progress_callback:
        progress_callback(stage, STAGES.get(stage, 0))

# ==================== CONFIGURATION ====================
@dataclass
class EducationalContentConfig:
    """Configuration for educational content generation service"""
    service_type: str = os.getenv("EDU_SERVICE_TYPE", "openai")
    openai_model: str = os.getenv("EDU_OPENAI_MODEL", "gpt-4o-mini")
    azure_model: str = os.getenv("EDU_AZURE_MODEL", "Meta-Llama-3.1-8B-Instruct")
    openai_api_key: Optional[str] = os.getenv("OPENAI_API_KEY")
    azure_endpoint: Optional[str] = os.getenv("AZURE_AI_ENDPOINT")
    azure_key: Optional[str] = os.getenv("AZURE_AI_KEY")
    azure_api_version: str = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    openai_base_url: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1/")
    max_questions: int = int(os.getenv("MAX_QUESTIONS", "10"))
    max_notes_pages: int = int(os.getenv("MAX_NOTES_PAGES", "5"))
    enable_cache: bool = os.getenv("EDU_ENABLE_CACHE", "true").lower() == "true"
    force_traditional: bool = os.getenv("EDU_FORCE_TRADITIONAL", "true").lower() == "true"

def validate_config(config: EducationalContentConfig) -> bool:
    """Validate that required configuration is present"""
    if config.service_type == "azure":
        if not config.azure_endpoint or not config.azure_key:
            logger.error("Azure AI credentials not configured. Set AZURE_AI_ENDPOINT and AZURE_AI_KEY.")
            return False
    elif config.service_type == "openai":
        if not config.openai_api_key:
            logger.error("OpenAI API key not configured. Set OPENAI_API_KEY.")
            return False
    else:
        logger.error(f"Unknown service type: {config.service_type}")
        return False
    return True

def get_content_hash(transcript: str, ocr_context: str, content_type: str) -> str:
    """Generate hash for content to enable caching"""
    content = f"{transcript}{ocr_context}{content_type}"
    return hashlib.md5(content.encode()).hexdigest()

# ==================== DATA STRUCTURES ====================
@dataclass
class MCQ:
    question: str
    options: List[str]
    correct_answer: str
    explanation: str
    difficulty: str
    topic: str
    tags: List[str] = field(default_factory=list)  # NEW
    course_type: str = "general"  # NEW

@dataclass
class LectureNoteSection:
    title: str
    content: str
    key_points: List[str]
    examples: List[str]

@dataclass
class EducationalContentResult:
    mcqs: List[MCQ]
    lecture_notes: List[LectureNoteSection]
    summary: str
    topics: List[Dict] = field(default_factory=list)          
    key_takeaways: List[str] = field(default_factory=list)    
    metadata: Dict = field(default_factory=dict)
# ==================== PYDANTIC MODELS FOR VALIDATION ====================
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional

class ValidatedLectureSection(BaseModel):
    """Pydantic model for validated lecture sections"""
    title: str = Field(..., min_length=1, max_length=300)
    content: str = Field(..., min_length=10, max_length=10000)
    key_points: List[str] = Field(default_factory=list, max_length=10)
    examples: List[str] = Field(default_factory=list, max_length=5)
    
    @field_validator('key_points', 'examples', mode='before')
    @classmethod
    def ensure_list(cls, v):
        if v is None:
            return []
        if isinstance(v, str):
            return [v]
        if not isinstance(v, list):
            return []
        # Filter out empty strings and ensure all items are strings
        return [str(item).strip() for item in v if item and str(item).strip()]

class ValidatedLectureNotes(BaseModel):
    """Pydantic model for complete lecture notes response"""
    sections: List[ValidatedLectureSection] = Field(..., min_length=1, max_length=20)
    summary: str = Field(..., min_length=10, max_length=1000)
    
    class Config:
        extra = 'ignore'  # Ignore unexpected fields
        str_strip_whitespace = True  # Auto strip whitespace
        
# ==================== UTILITIES ====================
def sec_to_hms(sec: int) -> str:
    """Convert seconds to HH:MM:SS format"""
    h, rem = divmod(int(sec), 3600)
    m, s = divmod(rem, 60)
    return f"{h:02d}:{m:02d}:{s:02d}"

def count_tokens_llama(text: str) -> int:
    """Approximate token counting for Llama-like tokenization"""
    chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
    non_chinese_len = len(text) - chinese_chars
    return chinese_chars + max(1, non_chinese_len // 4)

def truncate_text_by_tokens(text: str, max_tokens: int = 120_000) -> str:
    """Truncate to approx max_tokens, preserving whole sentences"""
    if max_tokens <= 0:
        return ""
    tokens = count_tokens_llama(text)
    if tokens <= max_tokens:
        return text
    logger.warning(f"Truncating content from {tokens:,} tokens to {max_tokens:,} tokens")
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])', text)
    truncated, current = [], 0
    for sentence in sentences:
        t = count_tokens_llama(sentence)
        if current + t > max_tokens:
            break
        truncated.append(sentence)
        current += t
    return "".join(truncated)

def format_chapters_for_prompt(chapters: Dict[str, str]) -> List[Dict]:
    """Convert chapter dict to list format for prompts"""
    formatted = []
    for ts, title in chapters.items():
        formatted.append({
            "ts": ts,
            "title": title
        })
    return formatted

def build_ocr_context_from_segments(ocr_segments: List[Dict]) -> str:
    """Convert OCR segments into a descriptive context string (sentence-level bullets)."""
    if not ocr_segments:
        return ""
    context_lines = ["# å¾žæŠ•å½±ç‰‡èˆ‡èž¢å¹•æ•æ‰åˆ°çš„ç›¸é—œæ–‡å­—ï¼š"]
    SENT_SPLIT = re.compile(r"[ã€‚ï¼›;ï¼ï¼Ÿ!?]\s*|\n+")
    for seg in ocr_segments:
        start = int(seg.get('start', 0))
        text = (seg.get('text') or "").strip()
        if not text:
            continue
        timestamp = sec_to_hms(start)
        context_lines.append(f"*   æ–¼ {timestamp} å·¦å³æ•æ‰åˆ°:")
        for sent in filter(None, (s.strip() for s in SENT_SPLIT.split(text))):
            context_lines.append(f"    - ã€Œ{sent}ã€")
    return "\n".join(context_lines)

def ocr_segments_to_raw_text(ocr_segments: List[Dict]) -> str:
    """Flatten OCR segments to raw lines (optionally timestamp-prefixed)."""
    lines: List[str] = []
    for seg in ocr_segments or []:
        text = (seg.get("text") or "").strip()
        if not text:
            continue
        start = seg.get("start")
        ts = f"[{sec_to_hms(int(start))}] " if isinstance(start, (int, float)) else ""
        lines.append(f"{ts}{text}")
    return "\n".join(lines)

# ---------- Simplified -> Traditional conversion ----------
# ==================== Initialize OpenCC (REQUIRED) ====================
def _init_opencc():
    """
    Initialize OpenCC converter. This is REQUIRED for proper Traditional Chinese output.
    Falls back to limited character mapping with warning if OpenCC not installed.
    """
    try:
        from opencc import OpenCC
        converter = OpenCC('s2t')
        logger.info("OpenCC initialized successfully for S->T conversion")
        return converter
    except ImportError:
        logger.error(
            "OpenCC is not installed but is required for proper Traditional Chinese conversion. "
            "Please install it with: pip install opencc-python-reimplemented"
        )
        # Return None to use fallback, but log warning on every conversion
        return None
    except Exception as e:
        logger.error(f"Failed to initialize OpenCC: {e}")
        return None

# Initialize on module load
_OPENCC = _init_opencc()

# ==================== Comprehensive Fallback Mapping ====================
# Extended character mapping for when OpenCC is unavailable
# This covers common educational and technical terms
_S2T_FALLBACK = str.maketrans({
    # Basic common characters
    "åŽ": "å¾Œ", "é‡Œ": "è£¡", "å°": "è‡º", "ä¸‡": "è¬", "ä¸Ž": "èˆ‡", "ä¹¦": "æ›¸", 
    "ä½“": "é«”", "ä»·": "åƒ¹", "ä¼˜": "å„ª", "å„¿": "å…’", "åŠ¨": "å‹•", "åŽ": "è¯", 
    "å‘": "ç™¼", "å¤": "å¾©", "å›½": "åœ‹", "å¹¿": "å»£", "æ±‰": "æ¼¢", "ä¼š": "æœƒ", 
    "çºª": "ç´€", "ç®€": "ç°¡", "ç»": "ç¶“", "åŽ†": "æ­·", "é©¬": "é¦¬", "é—¨": "é–€", 
    "é¢": "éºµ", "å†…": "å…§", "æ°”": "æ°£", "æƒ": "æ¬Š", "ç¡®": "ç¢º", "å®ž": "å¯¦", 
    "æœ¯": "è¡“", "äº‘": "é›²", "ä¼—": "çœ¾", "ä¸º": "ç‚º", "ä»Ž": "å¾ž", "å†²": "è¡",
    
    # Educational and learning terms
    "ç»ƒ": "ç·´", "ä¹ ": "ç¿’", "é¢˜": "é¡Œ", "è®¾": "è¨­", "è¯†": "è­˜", "å¯¼": "å°Ž",
    "ç»Ÿ": "çµ±", "è®®": "è­°", "è®º": "è«–", "éªŒ": "é©—", "ç±»": "é¡ž", "è¯": "è­‰",
    "é‡Š": "é‡‹", "è¯‘": "è­¯", "ç¼–": "ç·¨", "è¯¾": "èª²", "è®²": "è¬›", "ä¹‰": "ç¾©",
    
    # Technical and programming terms
    "åº“": "åº«", "ç ": "ç¢¼", "æ‰§": "åŸ·", "æ€": "æ…‹", "å‚¨": "å„²", "è½½": "è¼‰",
    "è¾“": "è¼¸", "è¿›": "é€²", "é€‰": "é¸", "é”™": "éŒ¯", "æ•°": "æ•¸", "æ®": "æ“š",
    "æž„": "æ§‹", "èŠ‚": "ç¯€", "å—": "å¡Š", "é“¾": "éˆ", "é˜Ÿ": "éšŠ", "æ ˆ": "æ£§",
    
    # Common verbs and actions
    "è¯´": "èªª", "è¯»": "è®€", "å†™": "å¯«", "é—®": "å•", "åº”": "æ‡‰", "è§": "è¦‹",
    "å¼€": "é–‹", "å…³": "é—œ", "ä¹°": "è²·", "å–": "è³£", "å¬": "è½", "è§‚": "è§€",
    "è®°": "è¨˜", "è®¤": "èª", "è®©": "è®“", "è°ˆ": "è«‡", "è¯·": "è«‹", "è½¬": "è½‰",
    
    # Analysis and evaluation terms
    "è¯„": "è©•", "æµ‹": "æ¸¬", "è¯•": "è©¦", "æ£€": "æª¢", "æŸ¥": "æŸ¥", "å®¡": "å¯©",
    "å¯¹": "å°", "é”™": "éŒ¯", "éš¾": "é›£", "é¢˜": "é¡Œ", "ç­”": "ç­”", "æ€»": "ç¸½",
    
    # Additional common characters in educational content
    "å¸ˆ": "å¸«", "å­¦": "å­¸", "å£°": "è²", "æˆ˜": "æˆ°", "é’Ÿ": "é˜", "çº§": "ç´š",
    "è¿™": "é€™", "çˆ±": "æ„›", "æ—¶": "æ™‚", "é—´": "é–“", "çŽ°": "ç¾", "ç”µ": "é›»",
    "è§†": "è¦–", "é¢‘": "é »", "ç½‘": "ç¶²", "ç»œ": "çµ¡", "çº¿": "ç·š", "è¿ž": "é€£",
    "å›¾": "åœ–", "ç”»": "ç•«", "åœº": "å ´", "æŠ¥": "å ±", "çº¸": "ç´™", "å¼ ": "å¼µ",
})

# ==================== Conversion Function ====================
def to_traditional(text: str) -> str:
    """
    Convert Simplified Chinese to Traditional Chinese.
    
    Priority:
    1. Use OpenCC if available (recommended)
    2. Fall back to character mapping with warning
    
    Args:
        text: Input text potentially containing Simplified Chinese
    
    Returns:
        Text converted to Traditional Chinese
    """
    if not text:
        return text
    
    # Try OpenCC first (recommended path)
    if _OPENCC is not None:
        try:
            return _OPENCC.convert(text)
        except Exception as e:
            logger.warning(f"OpenCC conversion failed: {e}, using fallback")
    
    # Fallback path - warn on first use in session
    if not hasattr(to_traditional, '_fallback_warned'):
        logger.warning(
            "Using limited character mapping for S->T conversion. "
            "For best results, install OpenCC: pip install opencc-python-reimplemented"
        )
        to_traditional._fallback_warned = True
    
    # Apply fallback character mapping
    return text.translate(_S2T_FALLBACK)

# ==================== Validation Function (Optional) ====================
def validate_traditional_conversion() -> bool:
    """
    Validate that Traditional Chinese conversion is working properly.
    Can be called during initialization to ensure system is ready.
    
    Returns:
        True if OpenCC is available and working, False otherwise
    """
    test_pairs = [
        ("å­¦ä¹ ", "å­¸ç¿’"),
        ("ç¼–ç¨‹", "ç·¨ç¨‹"),
        ("é—®é¢˜", "å•é¡Œ"),
        ("è¿™ä¸ª", "é€™å€‹"),
    ]
    
    if _OPENCC is None:
        logger.warning("OpenCC not available - using fallback conversion")
        return False
    
    try:
        for simplified, expected in test_pairs:
            result = to_traditional(simplified)
            if result != expected:
                logger.warning(f"Conversion test failed: {simplified} -> {result} (expected {expected})")
                return False
        logger.info("Traditional Chinese conversion validated successfully")
        return True
    except Exception as e:
        logger.error(f"Conversion validation failed: {e}")
        return False

# ==================== PROMPT BUILDERS (Topics and Summary, ASR-first) ====================
def build_topics_summary_prompt(transcript: str, 
                                video_title: Optional[str] = None,  # ADD THIS
                                context: Optional[Dict[str, str]] = None) -> str:
    """
    Build prompt for LLM to extract meaningful topics and global summary from ASR transcript.
    
    Args:
        transcript: The lecture transcript text
        context: Optional context about the lecture (course name, instructor, etc.)
    
    Returns:
        Formatted prompt string for the LLM
    """
    
    context_info = ""
    if context:
        context_items = [f"- {k}: {v}" for k, v in context.items()]
        context_info = f"""
# èª²ç¨‹èƒŒæ™¯è³‡è¨Š
{chr(10).join(context_items)}
"""
    video_title_info = ""
    if video_title:
        clean_title = re.sub(r'\.(mp4|avi|mov|mkv|webm|flv|m4v)$', '', video_title, flags=re.IGNORECASE)
        video_title_info = f"""
# èª²ç¨‹æª”å
åŽŸå§‹æª”åï¼š{clean_title}
è«‹åƒè€ƒæª”åç†è§£èª²ç¨‹çš„ä¸»é¡Œç¯„åœå’Œé‡é»žã€‚
"""

    prompt = f"""
# è§’è‰²å®šä½
ä½ æ˜¯ä¸€ä½è³‡æ·±çš„èª²ç¨‹åˆ†æžå°ˆå®¶ï¼Œå°ˆç²¾æ–¼æ•™å­¸è¨­è¨ˆå’ŒçŸ¥è­˜çµæ§‹åŒ–ã€‚ä½ çš„ä»»å‹™æ˜¯åˆ†æžè¬›åº§é€å­—ç¨¿ï¼Œ
æå–æ ¸å¿ƒä¸»é¡Œä¸¦ç”Ÿæˆé«˜è³ªé‡çš„èª²ç¨‹æ‘˜è¦ã€‚

{context_info}
{video_title_info}

# åˆ†æžæŒ‡ä»¤

## 1. æ·±åº¦ç†è§£
- ä»”ç´°é–±è®€é€å­—ç¨¿ï¼Œç†è§£è¬›åº§çš„æ•´é«”è„ˆçµ¡
- è­˜åˆ¥æ•™å­¸ç›®æ¨™ã€æ ¸å¿ƒæ¦‚å¿µå’Œé‚è¼¯æµç¨‹
- æ³¨æ„è¬›è€…çš„é‡é»žå’Œå¼·èª¿å…§å®¹

## 2. ä¸»é¡Œæå–
è­˜åˆ¥ **5-8 å€‹**æœ€é‡è¦çš„æ•™å­¸ä¸»é¡Œï¼Œæ¯å€‹ä¸»é¡Œæ‡‰è©²ï¼š
- ä»£è¡¨ä¸€å€‹å®Œæ•´ã€æœ‰æ„ç¾©çš„å­¸ç¿’å–®å…ƒ
- å…·æœ‰æ˜Žç¢ºçš„æ•™å­¸åƒ¹å€¼
- æœ‰è¶³å¤ çš„å…§å®¹æ·±åº¦ï¼ˆç´„ä½”15-30åˆ†é˜çš„è¬›åº§æ™‚é–“ï¼‰

## 3. å…§å®¹éŽæ¿¾
- æŽ’é™¤ï¼šè¡Œæ”¿å…¬å‘Šã€å€‹äººé–’èŠã€æŠ€è¡“å•é¡Œ
- åˆä½µï¼šé‡è¤‡æˆ–é›¶æ•£ä½†ç›¸é—œçš„å…§å®¹
- ä¿ç•™ï¼šæ‰€æœ‰å…·æ•™å­¸åƒ¹å€¼çš„æ ¸å¿ƒå…§å®¹

## 4. æ‘˜è¦æ’°å¯«
- ç°¡æ½”ä½†å…¨é¢åœ°ç¸½çµèª²ç¨‹
- ä½¿ç”¨æ¸…æ™°ã€å°ˆæ¥­çš„èªžè¨€
- æ•æ‰è¬›åº§ç²¾é«“å’Œå­¸ç¿’åƒ¹å€¼

# è¼¸å‡ºæ ¼å¼ï¼ˆå‹™å¿…åš´æ ¼éµå®ˆï¼‰

```json
{{
  "topics": [
    {{
      "id": "01",
      "title": "ä¸»é¡Œåç¨±ï¼ˆå…·é«”ä¸”æè¿°æ€§ï¼‰",
      "summary": "è©²ä¸»é¡Œçš„èªªæ˜Žï¼Œ2-3å¥è©±ï¼Œæ¶µè“‹æ ¸å¿ƒæ¦‚å¿µã€å­¸ç¿’è¦é»žå’Œæ‡‰ç”¨å ´æ™¯",
      "keywords": ["é—œéµè©ž1", "é—œéµè©ž2", "é—œéµè©ž3"]
    }}
  ],
  "global_summary": "æ•´å€‹è¬›åº§çš„ç¶œåˆæ‘˜è¦ï¼ˆ3-5å¥è©±ï¼‰ï¼Œèªªæ˜Žï¼š1) èª²ç¨‹ç›®æ¨™ 2) ä¸»è¦å…§å®¹ 3) å­¸ç¿’æˆæžœ",
  "key_takeaways": [
    "æ ¸å¿ƒè¦é»ž1",
    "æ ¸å¿ƒè¦é»ž2",
    "æ ¸å¿ƒè¦é»ž3"
  ]
}}
```

# ä¸»é¡Œå“è³ªæ¨™æº–

1. **æ•™å­¸ç›¸é—œæ€§**: æ¯å€‹ä¸»é¡Œå¿…é ˆå…·æœ‰æ•™è‚²åƒ¹å€¼
2. **å…·é«”æ˜Žç¢º**: ä½¿ç”¨ç²¾ç¢ºçš„ä¸»é¡Œåç¨±
   - âœ… å¥½ï¼š"Pythonåˆ—è¡¨åˆ‡ç‰‡èˆ‡ç´¢å¼•æ“ä½œ"
   - âŒ å·®ï¼š"PythonåŸºç¤Ž"
3. **é‚è¼¯é€£è²«**: ä¸»é¡Œé †åºæ‡‰åæ˜ çŸ¥è­˜éžé€²é—œä¿‚
4. **é©ç•¶ç²’åº¦**: ä¸è¦éŽæ–¼ç´°ç¢Žæˆ–å¯¬æ³›
5. **å¯¦ç”¨å°Žå‘**: å¼·èª¿å¯æ‡‰ç”¨çš„çŸ¥è­˜å’ŒæŠ€èƒ½

# è¼¸å…¥è³‡æ–™

## ASR é€å­—ç¨¿å…§å®¹ï¼š
{transcript}

# é‡è¦æé†’
- æ™‚é–“æˆ³å’Œç« ç¯€æ¨™è¨˜åƒ…ä¾›åƒè€ƒï¼Œä¸è¦å®Œå…¨ä¾è³´
- é—œæ³¨è¬›è€…çš„æ•™å­¸æ„åœ–ï¼Œè€Œéžè¡¨é¢å…§å®¹
- ä¿æŒå®¢è§€ä¸­ç«‹ï¼Œé¿å…ä¸»è§€è©•åƒ¹
- ç¢ºä¿è¼¸å‡ºç‚ºæœ‰æ•ˆçš„JSONæ ¼å¼
"""
    
    return prompt


# ==================== EDUCATIONAL METADATA HELPERS ====================
def build_educational_metadata_context(
    section_title: Optional[str],
    units: Optional[List[Dict]]
) -> str:
    """
    Build educational context from SectionTitle and Units metadata.
    Similar to chapter_generation.py approach but for Q&A context.
    
    Args:
        section_title: Course section title (e.g., "å®¤å…§è¨­è¨ˆå¯¦å‹™ å»šå…·è¦åŠƒ")
        units: List of units with UnitNo and Title
        
    Returns:
        Formatted context string for prompt enhancement
    """
    if not section_title and not units:
        return ""
    
    context_parts = []
    
    if section_title:
        context_parts.append(f"# ðŸ“š èª²ç¨‹å–®å…ƒè³‡è¨Š")
        context_parts.append(f"æœ¬å½±ç‰‡å±¬æ–¼èª²ç¨‹å–®å…ƒï¼š**{section_title}**")
        context_parts.append("")
    
    if units:
        context_parts.append(f"## é å®šæ•™å­¸å–®å…ƒçµæ§‹ ({len(units)} å€‹å–®å…ƒ)")
        context_parts.append("æœ¬èª²ç¨‹åŒ…å«ä»¥ä¸‹æ•™å­¸å–®å…ƒï¼š")
        for unit in units:
            unit_no = unit.get("UnitNo", "")
            unit_title = unit.get("Title", "")
            context_parts.append(f"   {unit_no}. {unit_title}")
        context_parts.append("")
        
        context_parts.append("## Q&A è¨­è¨ˆæŒ‡å¼•")
        context_parts.append("âœ… é¡Œç›®æ‡‰æ¶µè“‹å„å€‹æ•™å­¸å–®å…ƒçš„æ ¸å¿ƒçŸ¥è­˜é»ž")
        context_parts.append("âœ… åœ¨å¯èƒ½çš„æƒ…æ³ä¸‹ï¼Œç‚ºæ¯å€‹å–®å…ƒè¨­è¨ˆç›¸æ‡‰é¡Œç›®")
        context_parts.append(f"âœ… å»ºè­°åˆ†é…ï¼šæ¯å–®å…ƒ {max(1, 10 // len(units))} é¡Œå·¦å³")
        context_parts.append("âœ… é¡Œç›®æ¨™ç±¤ (tags) ä¸­å¯æ¨™è¨»ç›¸é—œå–®å…ƒç·¨è™Ÿ")
        context_parts.append("")
        
        context_parts.append("## è¬›ç¾©ç­†è¨˜æŒ‡å¼•")
        context_parts.append("âœ… è¬›ç¾©ç« ç¯€æ‡‰å°æ‡‰æ•™å­¸å–®å…ƒçµæ§‹")
        context_parts.append("âœ… ç‚ºæ¯å€‹å–®å…ƒæä¾›æ¸…æ™°çš„å­¸ç¿’è¦é»žæ•´ç†")
        context_parts.append("âœ… ç« ç¯€æ¨™é¡Œå»ºè­°æ ¼å¼ï¼š[å–®å…ƒNï¼šå–®å…ƒåç¨±] å…·é«”å…§å®¹")
    
    return "\n".join(context_parts)


def extract_unit_tags(units: Optional[List[Dict]]) -> List[str]:
    """
    Extract unit titles as potential tags for MCQs.
    
    Args:
        units: List of units with UnitNo and Title
        
    Returns:
        List of unit titles suitable for tagging
    """
    if not units:
        return []
    
    return [f"å–®å…ƒ{unit['UnitNo']}ï¼š{unit['Title']}" for unit in units]

def parse_topics_summary_response(response_text: str) -> tuple[List[Dict], str, List[str]]:
    """
    Parse topics, summary, and key takeaways from LLM response.
    
    Args:
        response_text: Raw LLM response containing JSON
    
    Returns:
        Tuple of (topics_list, global_summary, key_takeaways)
        Returns empty structures if parsing fails
    """
    # Use module-level logger instead of print
    logger = logging.getLogger(__name__)
    
    # Parse JSON from response
    data = _safe_load_json(response_text)  # Use your existing function
    if not data:
        logger.warning("Failed to parse topics/summary response JSON")
        return [], "", []
    
    # Extract with defensive parsing
    topics = []
    for i, topic_data in enumerate(data.get('topics', [])):
        if not isinstance(topic_data, dict):
            continue
            
        # Ensure required fields with sensible defaults
        topic_id = str(topic_data.get('id', f"{i+1:02d}")).strip()
        title = str(topic_data.get('title', f"ä¸»é¡Œ {i+1}")).strip()
        summary = str(topic_data.get('summary', '')).strip()
        
        # Handle keywords - ensure it's a list of strings
        keywords = topic_data.get('keywords', [])
        if isinstance(keywords, str):
            # Split comma-separated keywords: "word1, word2" â†’ ["word1", "word2"]
            keywords = [k.strip() for k in keywords.split(',') if k.strip()]
        elif not isinstance(keywords, list):
            keywords = []
        else:
            # Ensure all keywords are strings
            keywords = [str(k).strip() for k in keywords if k]
        
        # Only add topics with meaningful content
        if len(title) > 3 and len(summary) > 10:  # Basic validation
            topics.append({
                "id": topic_id,
                "title": title,
                "summary": summary,
                "keywords": keywords
            })
        else:
            logger.debug(f"Skipping topic {topic_id}: insufficient content")
    
    # Extract global summary
    global_summary = str(data.get('global_summary', '')).strip()
    if not global_summary:
        # Create a fallback summary from the first few topics
        if topics:
            topic_titles = [t['title'] for t in topics[:3]]
            global_summary = f"æœ¬è¬›åº§æ¶µè“‹{len(topics)}å€‹ä¸»è¦ä¸»é¡Œï¼ŒåŒ…æ‹¬{'ã€'.join(topic_titles)}{'ç­‰' if len(topics) > 3 else ''}é‡è¦å…§å®¹ã€‚"
        else:
            global_summary = "ç„¡æ³•å¾žå…§å®¹ç”Ÿæˆæ‘˜è¦ã€‚"
    
    # Extract key takeaways
    key_takeaways = []
    raw_takeaways = data.get('key_takeaways', [])
    
    if isinstance(raw_takeaways, str):
        # Handle string input - split by newlines or bullets
        lines = [line.strip() for line in raw_takeaways.split('\n') if line.strip()]
        for line in lines:
            # Remove common bullet markers: â€¢, -, *, numbers, etc.
            clean_line = re.sub(r'^[\sâ€¢\-*\d\.\)]+', '', line).strip()
            if clean_line:
                key_takeaways.append(clean_line)
    elif isinstance(raw_takeaways, list):
        for item in raw_takeaways:
            if isinstance(item, str) and item.strip():
                key_takeaways.append(item.strip())
            elif isinstance(item, (int, float)):
                key_takeaways.append(str(item))
    
    # Ensure we have at least some takeaways
    if not key_takeaways and topics:
        key_takeaways = [f"æŽŒæ¡{t['title']}çš„æ ¸å¿ƒæ¦‚å¿µ" for t in topics[:3]]
    
    logger.info(f"Parsed {len(topics)} topics, summary: {len(global_summary)} chars, {len(key_takeaways)} takeaways")
    return topics, global_summary, key_takeaways

def parse_modules_to_topics(modules_analysis: str) -> List[Dict]:
    """
    Parse modules_analysis from chapter generation into topics format.
    
    Args:
        modules_analysis: Text like "æ¨¡å¡Š1ï¼šåŸºç¤Žå·¥å…·æ“ä½œ ~ 00:00-00:25 ~ ä»‹é¢ã€å·¥å…· ~ ç†è«–+æ¼”ç¤º"
    
    Returns:
        List of topic dicts compatible with topics_list format
    """
    topics = []
    
    if not modules_analysis or not modules_analysis.strip():
        return topics
    
    # Parse each line
    lines = modules_analysis.strip().split('\n')
    
    for i, line in enumerate(lines, 1):
        line = line.strip()
        if not line or line.startswith('#') or line.startswith('-'):
            continue
        
        # Try to parse format: "æ¨¡å¡Šåç¨± ~ æ™‚é–“ç¯„åœ ~ æ ¸å¿ƒå­¸ç¿’é»ž ~ æ•™å­¸æ–¹æ³•"
        parts = [p.strip() for p in line.split('~')]
        
        if len(parts) >= 3:
            module_name = parts[0]
            time_range = parts[1] if len(parts) > 1 else ""
            learning_points = parts[2] if len(parts) > 2 else ""
            teaching_method = parts[3] if len(parts) > 3 else ""
            
            # Clean module name (remove "æ¨¡å¡Š1ï¼š" prefix if present)
            module_name = re.sub(r'^æ¨¡å¡Š\d+[ï¼š:]\s*', '', module_name)
            
            # Extract keywords from learning points
            keywords = [kw.strip() for kw in learning_points.split('ã€') if kw.strip()][:5]
            
            # Build summary
            summary_parts = []
            if learning_points:
                summary_parts.append(learning_points)
            if teaching_method:
                summary_parts.append(f"æ•™å­¸æ–¹å¼ï¼š{teaching_method}")
            summary = 'ï¼Œ'.join(summary_parts)
            
            topics.append({
                "id": str(i).zfill(2),
                "title": module_name,
                "summary": summary,
                "keywords": keywords,
                "time_range": time_range  # Extra info for context
            })
    
    # If parsing failed, try simpler format
    if not topics:
        for i, line in enumerate(lines, 1):
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('-'):
                # Use the whole line as title
                topics.append({
                    "id": str(i).zfill(2),
                    "title": line[:100],  # Limit length
                    "summary": "å¾žç« ç¯€æ¨¡å¡Šæå–çš„æ•™å­¸ä¸»é¡Œ",
                    "keywords": []
                })
    
    logger.info(f"Parsed {len(topics)} topics from modules_analysis")
    return topics

# ==================== SUGGESTED UNITS EXTRACTION ====================

def extract_suggested_units_from_chapters(
    chapters_dict: Dict[str, str],
    max_units: int = 5
) -> List[Dict]:
    """
    Extract suggested units from generated chapters.
    
    Strategy: Takes the first N chapters and converts them to unit format.
    Works well when chapters represent major learning segments.
    
    Args:
        chapters_dict: Chapter dictionary {"00:05:30": "[å–®å…ƒ1ï¼šå»šå…·è¦åŠƒ] å…§å®¹"} 
                      or {"00:05:30": "ç« ç¯€æ¨™é¡Œ"}
        max_units: Maximum number of suggested units (default: 5)
        
    Returns:
        List of suggested units in client API format:
        [{"UnitNo": 1, "Title": "å–®å…ƒæ¨™é¡Œ", "Time": "00:05:30"}, ...]
    """
    suggested = []
    unit_counter = 1
    
    # Take first N chapters as unit representatives
    for timestamp, title in list(chapters_dict.items())[:max_units]:
        # Extract clean title (remove unit tags if present like [å–®å…ƒ1ï¼šxxx])
        clean_title = re.sub(r'^\[å–®å…ƒ\d+[ï¼š:][^\]]+\]\s*', '', title)
        
        # Also remove other common prefixes
        clean_title = re.sub(r'^\[.*?\]\s*', '', clean_title)
        clean_title = clean_title.strip()
        
        # Limit title length for readability
        if len(clean_title) > 50:
            clean_title = clean_title[:47] + "..."
        
        suggested.append({
            "UnitNo": unit_counter,
            "Title": clean_title,
            "Time": timestamp  # Already in HH:mm:ss format from chapters
        })
        unit_counter += 1
    
    logger.info(f"Extracted {len(suggested)} suggested units from {len(chapters_dict)} chapters")
    return suggested


def extract_suggested_units_from_topics(
    topics_list: List[Dict],
    chapters_dict: Optional[Dict[str, str]] = None
) -> List[Dict]:
    """
    Convert topics to suggested units format with optional timestamp matching.
    
    Strategy: Uses topic analysis for unit titles, tries to find matching 
    chapter timestamps based on topic keywords.
    
    Args:
        topics_list: Topics from parse_modules_to_topics() or LLM extraction
                    Format: [{"id": "01", "title": "ä¸»é¡Œ", "summary": "...", "keywords": [...]}]
        chapters_dict: Optional chapter timestamps for time matching
        
    Returns:
        List of suggested units in client API format:
        [{"UnitNo": 1, "Title": "å–®å…ƒæ¨™é¡Œ", "Time": "00:05:30"}, ...]
    """
    suggested = []
    
    for i, topic in enumerate(topics_list[:5], 1):  # Limit to 5 units
        title = topic.get('title', f'ä¸»é¡Œ {i}')
        
        # Limit title length
        if len(title) > 50:
            title = title[:47] + "..."
        
        # Try to find matching timestamp from chapters
        time = ""
        if chapters_dict:
            # Strategy 1: Look for chapters that mention this topic
            # Extract first 3 significant words from topic title as keywords
            topic_keywords = [w for w in title.split() if len(w) > 1][:3]
            
            for chapter_ts, chapter_title in chapters_dict.items():
                # Check if any topic keyword appears in chapter title
                if any(keyword in chapter_title for keyword in topic_keywords):
                    time = chapter_ts
                    logger.debug(f"Matched topic '{title}' to chapter at {time}")
                    break
        
        # Strategy 2: Fallback to time_range if available in topic
        if not time and topic.get('time_range'):
            time_range = topic['time_range']
            # Extract start time from "00:00-00:25" format
            if '-' in time_range:
                time = time_range.split('-')[0].strip()
            else:
                time = time_range
        
        suggested.append({
            "UnitNo": i,
            "Title": title,
            "Time": time  # Empty string if no match found
        })
    
    logger.info(f"Extracted {len(suggested)} suggested units from {len(topics_list)} topics")
    return suggested
    
def validate_topics_output(data: Dict) -> tuple[bool, List[str]]:
    """
    Validate the structure of parsed topics/summary data.
    Returns (is_valid, list_of_errors)
    """
    errors = []
    
    if not isinstance(data, dict):
        return False, ["Data is not a dictionary"]
    
    # Check required top-level fields
    if 'topics' not in data:
        errors.append("Missing 'topics' field")
    elif not isinstance(data['topics'], list):
        errors.append("'topics' should be a list")
    
    if 'global_summary' not in data:
        errors.append("Missing 'global_summary' field")
    elif not isinstance(data['global_summary'], str):
        errors.append("'global_summary' should be a string")
    
    # Validate individual topics
    if isinstance(data.get('topics'), list):
        for i, topic in enumerate(data['topics']):
            if not isinstance(topic, dict):
                errors.append(f"Topic {i} is not a dictionary")
                continue
                
            if 'title' not in topic:
                errors.append(f"Topic {i} missing 'title'")
            elif not isinstance(topic['title'], str):
                errors.append(f"Topic {i} title is not a string")
                
            if 'summary' not in topic:
                errors.append(f"Topic {i} missing 'summary'")
            elif not isinstance(topic['summary'], str):
                errors.append(f"Topic {i} summary is not a string")
    
    return len(errors) == 0, errors
                                    
# ==================== PROMPT BUILDERS (V2, ASR-first) ====================

def build_mcq_prompt_v2(
    transcript: str,
    *,
    ocr_context: str = "",
    video_title: Optional[str] = None,  # â† ADD THIS PARAMETER
    num_questions: int = 10,
    chapters: Optional[List[Dict]] = None,
    global_summary: str = "",
    hierarchical_metadata: Optional[Dict] = None,
    section_title: Optional[str] = None,      # â† NEW
    units: Optional[List[Dict]] = None        # â† NEW
) -> str:
    """ASR-first MCQ prompt with Bloom structuring, global context, and practical constraints.
       Schema preserved: {"mcqs":[{question, options[A..D], correct_answer, explanation, difficulty, topic, tags, course_type}]}.
    """
    base = num_questions // 3
    rem  = num_questions % 3
    recall_n      = base + (1 if rem >= 1 else 0)
    application_n = base + (1 if rem >= 2 else 0)
    analysis_n    = base

    # Build educational metadata context (must NOT depend on chapters)
    edu_metadata_context = build_educational_metadata_context(section_title, units)

    # Log if metadata provided (also should NOT depend on chapters)
    if section_title or units:
        logger.info("=" * 60)
        logger.info("ðŸ“š EDUCATIONAL METADATA FOR MCQ GENERATION")
        if section_title:
            logger.info(f"   ðŸ“– Section: {section_title}")
        if units:
            logger.info(f"   ðŸ“‘ Units: {len(units)} predefined units")
            for unit in units:
                logger.info(f"      {unit['UnitNo']}. {unit['Title']}")
        logger.info("=" * 60)

    chap_lines = []
    if chapters:
        # Enhanced question distribution if units provided
        if units and len(units) > 0:
            questions_per_unit = max(1, num_questions // len(units))
            logger.info(f"ðŸ“Š Suggested distribution: ~{questions_per_unit} questions per unit")
        
        for c in chapters[:18]:
            ts = c.get("ts") or c.get("timestamp") or ""
            title = c.get("title") or ""
            if ts or title:
                chap_lines.append(f"- {ts}ï¼š{title}")
    video_title_context = ""
    if video_title:
        # Strip common video extensions
        clean_title = re.sub(r'\.(mp4|avi|mov|mkv|webm|flv|m4v)$', '', video_title, flags=re.IGNORECASE)
        video_title_context = f"ðŸ“š æ­¤å½±ç‰‡æª”åç‚ºï¼šã€Œ{clean_title}ã€ï¼Œè«‹åƒè€ƒæª”åè³‡è¨Šè¨­è¨ˆç›¸é—œé¡Œç›®ã€‚\n\n"
                
    global_ctx = []
    if global_summary.strip():
        global_ctx.append(f"- æ‘˜è¦ï¼š{global_summary.strip()}")

    if hierarchical_metadata:
        # Add structured educational context
        course_summary = hierarchical_metadata.get('course_summary', {})
        if course_summary:
            global_ctx.extend([
                f"- æ ¸å¿ƒä¸»é¡Œï¼š{course_summary.get('topic', '')}",
                f"- é—œéµæŠ€è¡“ï¼š{course_summary.get('core_content', '')}",
                f"- å­¸ç¿’ç›®æ¨™ï¼š{course_summary.get('learning_objectives', '')}",
                f"- ç›®æ¨™å­¸å“¡ï¼š{course_summary.get('target_audience', '')}",
                f"- é›£åº¦ç´šåˆ¥ï¼š{course_summary.get('difficulty', '')}"
            ])
        
        # Add structure analysis (PASS 1 insights)
        structure_analysis = hierarchical_metadata.get('structure_analysis', '')
        if structure_analysis:
            # Extract key points from structure (limit length)
            structure_summary = structure_analysis[:500] + "..." if len(structure_analysis) > 500 else structure_analysis
            global_ctx.append(f"- èª²ç¨‹çµæ§‹åˆ†æžï¼š{structure_summary}")
        
        # Add module analysis for question distribution guidance
        modules_analysis = hierarchical_metadata.get('modules_analysis', '')
        if modules_analysis:
            global_ctx.append(f"- æ•™å­¸æ¨¡çµ„åŠƒåˆ†ï¼š\n{modules_analysis}")
        
        # Add quality score for context
        quality_score = hierarchical_metadata.get('educational_quality_score', 0)
        if quality_score > 0:
            quality_label = "é«˜" if quality_score > 0.7 else "ä¸­" if quality_score > 0.4 else "åŸºç¤Ž"
            global_ctx.append(f"- æ•™è‚²å“è³ªè©•åˆ†ï¼š{quality_score:.2f} ({quality_label})")
            
    if chap_lines:
        global_ctx.append("- ç« ç¯€ï¼š\n" + "\n".join(chap_lines))
    global_ctx_block = "\n".join(global_ctx) if global_ctx else "ï¼ˆç„¡ï¼‰"

    ocr_block = ""
    if ocr_context.strip():
        ocr_block = f"## èž¢å¹•æ–‡å­—ï¼ˆOCRï¼Œåƒ…ä½œè¼”åŠ©åƒè€ƒï¼‰\n{ocr_context}\n\n"
        
    # NEW: Enhanced question distribution logic based on metadata
    if hierarchical_metadata and hierarchical_metadata.get('educational_quality_score', 0) > 0.7:
        # High-quality content: shift toward application/analysis
        if hierarchical_metadata['educational_quality_score'] > 0.8:
            recall_n = max(2, recall_n - 1)
            analysis_n = analysis_n + 1
        # Adjust based on difficulty level
        difficulty = hierarchical_metadata.get('course_summary', {}).get('difficulty', '')
        if difficulty == 'é«˜ç´š':
            recall_n = max(1, recall_n - 2)
            analysis_n = analysis_n + 2
            
    # ADD THE LOGGING RIGHT HERE (after distribution logic, before prompt construction)
    if hierarchical_metadata and hierarchical_metadata.get('educational_quality_score', 0) > 0.7:
        original_recall = base + (1 if rem >= 1 else 0)
        original_analysis = base
        if recall_n != original_recall or analysis_n != original_analysis:
            logger.info(f"Adjusted question distribution based on metadata: "
                       f"Recall {original_recall}â†’{recall_n}, "
                       f"Application {application_n}â†’{application_n}, "
                       f"Analysis {original_analysis}â†’{analysis_n}")

    
    # --- KEY ENHANCEMENT: Revised Prompt (WITH ADDITIONS FOR TAGS AND COURSE_TYPE) --- 
    prompt = f"""
{video_title_context}{edu_metadata_context}
ä½ æ˜¯ä¸€ä½è³‡æ·±çš„æ•™å­¸è¨­è¨ˆå°ˆå®¶ï¼Œè² è²¬ç‚ºã€Œ{global_summary.splitlines()[0] if global_summary else "å„ç¨®ç§‘ç›®"}ã€èª²ç¨‹è¨­è¨ˆé«˜å“è³ªçš„å¤šé¸é¡Œï¼ˆMCQï¼‰ã€‚è«‹åš´æ ¼ä¾ç…§ä¸‹åˆ—è¦å‰‡å‡ºé¡Œï¼Œä¸¦**åƒ…**è¼¸å‡º JSONã€‚

### æ ¸å¿ƒåŽŸå‰‡
- **å•é¡Œå¿…é ˆåŸºæ–¼å°é€å­—ç¨¿çš„æ•´é«”ç†è§£**ï¼Œè€Œéžå­¤ç«‹çš„å–®å¥ã€‚é¦–å…ˆåˆ†æžæ•´æ®µæ–‡æœ¬çš„ 5-8 å€‹æ ¸å¿ƒä¸»é¡Œèˆ‡æ•™å­¸ç›®æ¨™ï¼Œå†æ“šæ­¤è¨­è¨ˆé¡Œç›®ã€‚
- **æ¸¬è©¦æ·±åº¦ç†è§£**ï¼šå•é¡Œæ‡‰ä¿ƒä½¿å­¸ç”Ÿæ‡‰ç”¨ã€åˆ†æžã€è©•ä¼°æ‰€å­¸ï¼Œè€Œä¸åƒ…æ˜¯å›žæ†¶äº‹å¯¦ã€‚

### è³‡æ–™ä¾†æºå„ªå…ˆåº
1) **ASR é€å­—ç¨¿ï¼ˆä¸»è¦ä¾æ“šï¼‰**ï¼šæ‰€æœ‰é¡Œç›®å¿…é ˆåŸºæ–¼æ­¤å…§å®¹ã€‚
2) **OCR èž¢å¹•æ–‡å­—ï¼ˆè¼”åŠ©åƒè€ƒï¼‰**ï¼šå¯ç”¨æ–¼ç”Ÿæˆæœ‰é—œè¦–è¦ºå…§å®¹ï¼ˆå¦‚è»Ÿé«”ç•Œé¢ã€åœ–è¡¨ã€ä»£ç¢¼ï¼‰çš„é¡Œç›®ã€‚è‹¥èˆ‡ ASR è¡çªï¼Œä»¥ ASR ç‚ºæº–ã€‚

### å…¨åŸŸè„ˆçµ¡ï¼ˆGlobal Contextï¼‰
{global_ctx_block}

### å‡ºé¡Œçµæ§‹ï¼ˆBloom's åˆ†é¡žæ³•ï¼›åˆè¨ˆ {num_questions} é¡Œï¼‰
- **Recallï¼ˆè¨˜æ†¶ï¼‰{recall_n} é¡Œ**ï¼šæ¸¬é©—é—œéµè¡“èªžã€æ¦‚å¿µã€æ­¥é©Ÿçš„åç¨±ã€‚*Example: ã€ŒAdobe Premiere ä¸­å‰ªè¼¯å½±ç‰‡çš„å¿«æ·éµæ˜¯ä»€éº¼ï¼Ÿã€*
- **Applicationï¼ˆæ‡‰ç”¨ï¼‰{application_n} é¡Œ**ï¼šæ¸¬é©—åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹é‹ç”¨æ‰€å­¸çŸ¥è­˜çš„èƒ½åŠ›ã€‚
  - *ç·¨ç¨‹èª²ç¨‹ï¼šå¿…é ˆåŒ…å«ã€Œé æ¸¬ä»£ç¢¼è¼¸å‡ºã€æˆ–ã€Œæ‰¾å‡ºä»£ç¢¼éŒ¯èª¤ã€çš„é¡Œç›®ã€‚è«‹æä¾›å®Œæ•´ä»£ç¢¼ç‰‡æ®µã€‚*
  - *è¨­è¨ˆ/è¡ŒéŠ·èª²ç¨‹ï¼šæ¸¬é©—å·¥å…·æ“ä½œï¼ˆe.g., ã€Œè¦é”æˆXXæ•ˆæžœï¼Œä¸‹ä¸€æ­¥è©²é»žé¸å“ªå€‹å·¥å…·ï¼Ÿã€ï¼‰æˆ–ç­–ç•¥æ‡‰ç”¨ï¼ˆe.g., ã€Œå°æ–¼ä¸€æ¬¾æ–°ç”¢å“ï¼Œæ‡‰å„ªå…ˆæŽ¡ç”¨å“ªç¨®è¡ŒéŠ·ç­–ç•¥ï¼Ÿã€ï¼‰ã€‚*
- **Analysisï¼ˆåˆ†æžï¼‰{analysis_n} é¡Œ**ï¼šæ¸¬é©—æ¯”è¼ƒã€å°ç…§ã€è§£é‡‹æ¦‚å¿µå’ŒæŽ¨ç†çš„èƒ½åŠ›ã€‚*Example: ã€Œç‚ºä»€éº¼è¬›å¸«å»ºè­°ä½¿ç”¨ A æ–¹æ³•è€Œä¸æ˜¯ B æ–¹æ³•ï¼Ÿã€ã€ã€Œé€™å€‹è¨­è¨ˆåŽŸå‰‡èƒŒå¾Œçš„ç›®çš„æ˜¯ä»€éº¼ï¼Ÿã€*

### é¡Œç›®å“è³ªæŒ‡å¼•
- **é¸é …è¨­è¨ˆ**ï¼šç”Ÿæˆ 4 å€‹å…·å‚™ã€Œè¿·æƒ‘æ€§ã€çš„é¸é …ã€‚éŒ¯èª¤é¸é …å¿…é ˆåŸºæ–¼**å¸¸è¦‹çš„å­¸ç”ŸéŒ¯èª¤ã€å¯¦å‹™ä¸Šçš„èª¤è§£æˆ–å®¹æ˜“æ··æ·†çš„æ¦‚å¿µ**ã€‚é¿å…ç„¡é—œæˆ–æ˜Žé¡¯éŒ¯èª¤çš„çŽ©ç¬‘å¼é¸é …ã€‚
- **é›£åº¦æ¯”ä¾‹**ï¼š30% easy / 40% medium / 30% hardã€‚
- **è§£é‡‹èªªæ˜Ž**ï¼šæ¯é¡Œçš„è§£é‡‹å¿…é ˆåŒ…å«ã€Œç‚ºä½•æ­£ç¢ºã€ä»¥åŠã€Œå¸¸è¦‹çš„éŒ¯èª¤é¸æ“‡åŠå…¶åŽŸå› ã€ã€‚
- **ä¸»é¡Œæ¨™ç±¤**ï¼š`topic` å­—æ®µæ‡‰æ¨™æ˜Žè©²é¡Œæ¸¬é©—çš„å…·é«”çŸ¥è­˜é»žï¼ˆe.g., `Pythonåˆ—è¡¨ç´¢å¼•`, `è‰²å½©ç†è«–`, `Facebookå»£å‘Šå—çœ¾è¨­å®š`ï¼‰ã€‚
- **æ¨™ç±¤ç”Ÿæˆ**ï¼šç‚ºæ¯é¡Œç”Ÿæˆ 3-5 å€‹ç›¸é—œæ¨™ç±¤ï¼ˆtagsï¼‰ï¼Œæ¶µè“‹æ ¸å¿ƒæ¦‚å¿µã€æŠ€è¡“ã€æ‡‰ç”¨å ´æ™¯ã€‚æ¨™ç±¤æ‡‰å…·é«”ä¸”æœ‰åŠ©æ–¼åˆ†é¡žå’Œæœç´¢ã€‚
- **èª²ç¨‹é¡žåž‹åˆ¤æ–·**ï¼šæ ¹æ“šé¡Œç›®å…§å®¹è‡ªå‹•åˆ¤æ–·ä¸¦æ¨™è¨˜èª²ç¨‹é¡žåž‹ï¼ˆcourse_typeï¼‰ã€‚

### èª²ç¨‹é¡žåž‹åˆ†é¡žæŒ‡å—
- **è¨­è¨ˆ**ï¼šæ¶‰åŠè¦–è¦ºè¨­è¨ˆã€UI/UXã€è‰²å½©ç†è«–ã€æŽ’ç‰ˆã€å‰µæ„è»Ÿé«”ï¼ˆPhotoshopã€Illustratorã€Figmaç­‰ï¼‰
- **ç¨‹å¼**ï¼šæ¶‰åŠç·¨ç¨‹èªžè¨€ã€æ¼”ç®—æ³•ã€è³‡æ–™çµæ§‹ã€è»Ÿé«”é–‹ç™¼ã€APIã€æ¡†æž¶ã€è³‡æ–™åº«
- **æ•¸å­¸**ï¼šæ¶‰åŠæ•¸å­¸é‹ç®—ã€å…¬å¼ã€å®šç†ã€çµ±è¨ˆã€å¾®ç©åˆ†ã€å¹¾ä½•ã€ä»£æ•¸
- **èªžè¨€**ï¼šæ¶‰åŠèªžè¨€å­¸ç¿’ã€æ–‡æ³•ã€è©žå½™ã€å¯«ä½œã€ç¿»è­¯ã€å£èªžè¡¨é”
- **å•†æ¥­**ï¼šæ¶‰åŠç®¡ç†ã€è¡ŒéŠ·ã€è²¡å‹™ã€ç¶“æ¿Ÿã€ç­–ç•¥ã€å‰µæ¥­ã€å•†æ¥­æ¨¡å¼
- **ç§‘å­¸**ï¼šæ¶‰åŠç‰©ç†ã€åŒ–å­¸ã€ç”Ÿç‰©ã€åœ°çƒç§‘å­¸ã€å¯¦é©—æ–¹æ³•ã€ç§‘å­¸ç†è«–
- **å…¶ä»–**ï¼šä¸å±¬æ–¼ä»¥ä¸Šé¡žåˆ¥çš„èª²ç¨‹å…§å®¹

### è¼¸å‡ºæ ¼å¼ï¼ˆåƒ… JSONï¼‰
```json
{{
  "mcqs": [
    {{
      "question": "å•é¡Œï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
      "options": ["é¸é …A", "é¸é …B", "é¸é …C", "é¸é …D"],
      "correct_answer": "A|B|C|D",
      "explanation": "ç‚ºä½•æ­£ç¢ºï¼‹å¸¸è¦‹èª¤è§£",
      "difficulty": "easy|medium|hard",
      "topic": "ä¸»é¡Œ/æ¦‚å¿µ",
      "tags": ["æ¨™ç±¤1", "æ¨™ç±¤2", "æ¨™ç±¤3"],
      "course_type": "è¨­è¨ˆ|ç¨‹å¼|æ•¸å­¸|èªžè¨€|å•†æ¥­|ç§‘å­¸|å…¶ä»–"
    }}
  ]
}}

### è¼¸å…¥è³‡æ–™
## ASR é€å­—ç¨¿ï¼ˆä¸»è¦ä¾æ“šï¼‰
{transcript}

{ocr_block}
"""
    return prompt


def build_lecture_notes_prompt_v2(
    transcript: str,
    *,
    ocr_context: str = "",
    num_pages: int = 5,
    chapters: Optional[List[Dict]] = None,
    topics: Optional[List[Dict]] = None,
    video_title: Optional[str] = None,
    global_summary: str = "",
    hierarchical_metadata: Optional[Dict] = None,
    section_title: Optional[str] = None,
    units: Optional[List[Dict]] = None
) -> str:
    """ASR-first lecture notes prompt. Transforms transcripts into structured, hierarchical study guides.
       Schema: sections[{title, content, key_points[], examples[]}], summary, key_terms[]
    """
    edu_metadata_context = build_educational_metadata_context(section_title, units)

    # Log if metadata provided
    if section_title or units:
        logger.info("=" * 60)
        logger.info("ðŸ“š EDUCATIONAL METADATA FOR LECTURE NOTES")
        if section_title:
            logger.info(f"   ðŸ“– Section: {section_title}")
        if units:
            logger.info(f"   ðŸ“‘ Units: {len(units)} predefined units")
            for unit in units:
                logger.info(f"      {unit.get('UnitNo')}. {unit.get('Title')}")
        logger.info("=" * 60)

    # Topics snippet
    topics_snippet = ""
    if topics:
        lines = []
        for i, t in enumerate(topics, 1):
            tid = t.get("id", str(i).zfill(2))
            title = t.get("title", "")
            summ = t.get("summary", "")
            lines.append(f"{tid}. {title}ï¼š{summ}")
        topics_snippet = "\n".join(lines)

    # Chapters snippet
    chap_lines = []
    if chapters:
        for c in chapters[:18]:
            ts = c.get("ts") or c.get("timestamp") or ""
            title = c.get("title", "")
            if ts or title:
                chap_lines.append(f"- {ts}ï¼š{title}")

    # Global context block
    global_ctx = []
    if (video_title or "").strip():
        global_ctx.append(f"- å½±ç‰‡/èª²ç¨‹æ¨™é¡Œï¼š{(video_title or '').strip()}")
    if global_summary.strip():
        global_ctx.append(f"- æ‘˜è¦ï¼š{global_summary.strip()}")

    if hierarchical_metadata:
        course_summary = hierarchical_metadata.get("course_summary", {}) or {}
        if course_summary:
            global_ctx.extend([
                f"- æ ¸å¿ƒä¸»é¡Œï¼š{course_summary.get('topic', '')}",
                f"- é—œéµæŠ€è¡“ï¼š{course_summary.get('core_content', '')}",
                f"- å­¸ç¿’ç›®æ¨™ï¼š{course_summary.get('learning_objectives', '')}",
                f"- ç›®æ¨™å­¸å“¡ï¼š{course_summary.get('target_audience', '')}",
                f"- é›£åº¦ç´šåˆ¥ï¼š{course_summary.get('difficulty', '')}",
            ])

        structure_analysis = hierarchical_metadata.get("structure_analysis", "") or ""
        if structure_analysis:
            structure_summary = structure_analysis[:500] + "..." if len(structure_analysis) > 500 else structure_analysis
            global_ctx.append(f"- èª²ç¨‹æž¶æ§‹ï¼š{structure_summary}")

        modules_analysis = hierarchical_metadata.get("modules_analysis", "") or ""
        if modules_analysis:
            global_ctx.append(f"- æ¨¡çµ„æž¶æ§‹ï¼š\n{modules_analysis}")

    if chap_lines:
        global_ctx.append("- ç« ç¯€ï¼š\n" + "\n".join(chap_lines))
    if topics_snippet:
        global_ctx.append("- ä¸»é¡Œå¤§ç¶±ï¼š\n" + topics_snippet)

    global_ctx_block = "\n".join(global_ctx) if global_ctx else "ï¼ˆç„¡ï¼‰"

    # OCR block
    ocr_block = ""
    if ocr_context.strip():
        ocr_block = f"## èž¢å¹•æ–‡å­—ï¼ˆOCRï¼Œåƒ…ä½œè¼”åŠ©åƒè€ƒï¼‰\n{ocr_context}\n\n"

    min_words = num_pages * 400
    max_words = (num_pages + 1) * 350

    prompt = f"""
{edu_metadata_context}
ä½ æ˜¯ä¸€ä½è³‡æ·±çš„èª²ç¨‹ç·¨è¼¯å’Œæ•™å­¸è¨­è¨ˆå°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡é€å­—ç¨¿**è½‰åŒ–ã€æç…‰ã€é‡æ§‹**æˆã€Œå¯è¤‡ç¿’ã€å¯ç…§åšã€çš„çµ‚æ¥µè¬›ç¾©ã€‚è«‹**åªè¼¸å‡º JSON**ã€‚

### æ ¸å¿ƒåŽŸå‰‡ï¼ˆå¿…é ˆéµå®ˆï¼‰
1) **ASR-first**ï¼šä»¥ ASR é€å­—ç¨¿ç‚ºä¸»è¦ä¾æ“šï¼›OCR åƒ…è¼”åŠ©æè¿°ç•«é¢å…§å®¹ã€‚è¡çªæ™‚ä»¥ ASR ç‚ºæº–ã€‚
2) **é‡æ§‹ï¼Œå‹¿æŠ„å¯«**ï¼šåˆªé™¤è´…è©žèˆ‡é›¢é¡Œå…§å®¹ï¼Œå¯é‡æŽ’é †åºä»¥æå‡å­¸ç¿’æ•˜äº‹ã€‚
3) **å¯æŽƒè®€**ï¼šæ¨™é¡Œå±¤ç´šæ¸…æ¥šã€åˆ—è¡¨åŒ–ã€æ­¥é©ŸåŒ–ï¼›60 ç§’å…§èƒ½å®šä½ä¸»é¡Œã€‚
4) **å¯æ“ä½œ**ï¼šæ¯ç¯€éƒ½è¦ç”¢å‡ºã€Œå¯ä»¥ç…§åšã€çš„æ­¥é©Ÿèˆ‡ä¾‹å­ï¼Œä¸è¦åªæœ‰æ¦‚è¿°ã€‚

### å…¨åŸŸè„ˆçµ¡ï¼ˆGlobal Contextï¼‰
{global_ctx_block}

### ç« ç¯€/Units ä½¿ç”¨è¦å‰‡ï¼ˆéžå¸¸é‡è¦ï¼‰
- **Units/ç« ç¯€åªèƒ½ç”¨ä¾†æ±ºå®šåˆ†æ®µèˆ‡æŽ’åº**ã€‚
- **ç¦æ­¢**æŠŠ Units æ¨™é¡Œæ”¹å¯«æˆã€Œæœ¬ç¯€ä»‹ç´¹â€¦ã€å°±çµæŸã€‚
- æ¯ä¸€ç¯€éƒ½å¿…é ˆåŒ…å«ï¼šæ­¥é©Ÿã€æ³¨æ„äº‹é …ã€é™·é˜±ã€æŠ€å·§ã€æ‡‰ç”¨æƒ…å¢ƒã€ä¾‹å­ï¼ˆå«ç¨‹å¼ç¢¼è‹¥é©ç”¨ï¼‰ã€‚

---

## âœ… æ¯å€‹ section.content å¿…é ˆç…§ä¸‹é¢æ¨¡æ¿è¼¸å‡ºï¼ˆæ¨™é¡Œä¸å¯æ”¹åï¼Œä¸å¯çœç•¥ï¼‰

### èª²ç¨‹ç›®æ¨™èˆ‡æ¦‚è¿°
ï¼ˆ2-4 è¡Œï¼Œæè¿°æœ¬ç¯€å­¸ä»€éº¼ã€ç‚ºä»€éº¼é‡è¦ï¼‰

### æ ¸å¿ƒæ¦‚å¿µè¬›è§£
- **è¡“èªž/æ¦‚å¿µ**ï¼šå®šç¾©ï¼ˆå‹™å¿…æ¸…æ¥šï¼‰
- **è¡“èªž/æ¦‚å¿µ**ï¼šå®šç¾©

### æ“ä½œæŒ‡å—èˆ‡å¯¦ä¾‹
1) æ­¥é©Ÿâ€¦
2) æ­¥é©Ÿâ€¦
- è‹¥æ˜¯ç·¨ç¨‹/å‰ç«¯/è³‡æ–™è™•ç†ï¼ˆHTML/JS/CSV/JSON/Chart.js/FileReader/äº‹ä»¶ç›£è½ç­‰ï¼‰ï¼Œæœ¬å°ç¯€**å¿…é ˆè‡³å°‘åŒ…å« 1 å€‹å¯åŸ·è¡Œçš„ code block**ï¼ˆ```html / ```javascript / ```pythonï¼‰ã€‚

### âŒ å¸¸è¦‹éŒ¯èª¤èˆ‡é™·é˜±
- éŒ¯èª¤ï¼šâ€¦ â†’ å¾Œæžœï¼šâ€¦
- éŒ¯èª¤ï¼šâ€¦ â†’ å¾Œæžœï¼šâ€¦

### âœ… æœ€ä½³å¯¦è¸èˆ‡æŠ€å·§
- æŠ€å·§ï¼šâ€¦ â†’ åŽŸå› /æ•ˆæžœï¼šâ€¦
- æŠ€å·§ï¼šâ€¦ â†’ åŽŸå› /æ•ˆæžœï¼šâ€¦

### ðŸ’¡ çœŸå¯¦æ‡‰ç”¨å ´æ™¯
- æƒ…å¢ƒï¼šâ€¦ â†’ å¦‚ä½•æ‡‰ç”¨ï¼šâ€¦

---

## âœ… è¼¸å‡ºç¡¬æ€§è¦å‰‡ï¼ˆé•åè¦–ç‚ºå¤±æ•—ï¼‰
1) æ¯å€‹ section çš„ content å¿…é ˆåŒ…å«ä¸Šè¿° **6 å€‹å°ç¯€æ¨™é¡Œ**ï¼ˆä¸å¯çœç•¥ï¼‰ã€‚
2) è‹¥é€å­—ç¨¿æ²’æœ‰è¬›åˆ°æŸå°ç¯€ï¼šå¯æ ¹æ“šä¸Šä¸‹æ–‡**åˆç†æŽ¨æ–·**ä¸¦ä¿æŒç°¡çŸ­ï¼›çœŸçš„ç„¡æ³•æŽ¨æ–·å°±å¯«ã€Œï¼ˆæœ¬ç¯€ç„¡ï¼‰ã€ä½†**ä¸å¯åˆªæ¨™é¡Œ**ã€‚
3) **ç¨‹å¼/å‰ç«¯ç›¸é—œå…§å®¹**ï¼š`### æ“ä½œæŒ‡å—èˆ‡å¯¦ä¾‹` å¿…é ˆè‡³å°‘ 1 å€‹å¯åŸ·è¡Œç¨‹å¼ç¢¼å€å¡Šï¼ˆç¦æ­¢åªæœ‰å½ç¢¼ï¼‰ã€‚
4) âŒ/âœ…/ðŸ’¡ çš„æœ€ä½Žè¦æ±‚ï¼š
   - âŒ è‡³å°‘ 2 æ¢ï¼ˆéŒ¯åœ¨å“ª + å¾Œæžœï¼‰
   - âœ… è‡³å°‘ 2 æ¢ï¼ˆå¯æ“ä½œæŠ€å·§ï¼‰
   - ðŸ’¡ è‡³å°‘ 1 æ¢ï¼ˆçœŸå¯¦æƒ…å¢ƒï¼‰
5) `key_points`ï¼šæ¯ç¯€ 2â€“3 æ¢ï¼Œå¿…é ˆå¯ç”¨ä¾†å‡ºé¡Œï¼ˆé¿å…ç©ºæ³›ï¼‰ã€‚
6) `examples`ï¼šæ¯ç¯€ 3â€“5 æ¢å…·é«”ä¾‹å­ï¼š
   - ç·¨ç¨‹èª²ç¨‹ï¼šè‡³å°‘ 1 æ¢å« codeï¼ˆå¯èˆ‡ content é‡è¤‡æˆ–è£œå……ï¼‰
   - éžç·¨ç¨‹èª²ç¨‹ï¼šè‡³å°‘ 1 æ¢æä¾›å¯¦å‹™æ¡ˆä¾‹

---

### è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼ JSONï¼›åªè¼¸å‡º JSONï¼‰
```json
{{
  "sections": [
    {{
      "title": "å±¤ç´šåŒ–æ¨™é¡Œï¼ˆä¾‹ï¼š'1.2 äº‹ä»¶ç›£è½ï¼šclick èˆ‡å›žå‘¼å‡½å¼'ï¼‰",
      "content": "å¿…é ˆåŒ…å« 6 å€‹å°ç¯€æ¨™é¡Œçš„ Markdown å…§å®¹",
      "key_points": ["2-3 æ¢å¯è¤‡ç¿’è€ƒé»ž"],
      "examples": ["3-5 æ¢å…·é«”ä¾‹å­ï¼ˆç·¨ç¨‹éœ€è‡³å°‘ 1 æ¢å« codeï¼‰"]
    }}
  ],
  "summary": "å…¨æ–‡éŽåŽ»å¼ç¸½çµï¼š3-5 å€‹æ”¶ç©« + å¾ŒçºŒè¡Œå‹•å»ºè­°",
  "key_terms": [
    {{ "term": "è¡“èªž1", "definition": "æ¸…æ™°å®šç¾©" }},
    {{ "term": "è¡“èªž2", "definition": "æ¸…æ™°å®šç¾©" }}
  ]
}}
```
å­—æ•¸å»ºè­°: {min_words}â€“{max_words}ï¼ˆè»Ÿé™åˆ¶ï¼‰ã€‚å“è³ªå’Œæ¸…æ™°åº¦å„ªå…ˆæ–¼åš´æ ¼éµå®ˆå­—æ•¸ã€‚

### è¼¸å…¥è³‡æ–™
## ASR é€å­—ç¨¿ï¼ˆä¸»è¦ä¾æ“šï¼‰
{transcript}

{ocr_block}
"""
    return prompt

# ==================== SYSTEM MESSAGES (ASR-first) ====================
MCQ_SYSTEM_MESSAGE = (
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•™å­¸è¨­è¨ˆå°ˆå®¶ã€‚ä½ çš„æ ¸å¿ƒä»»å‹™æ˜¯åŸºæ–¼å°ã€ŒASR é€å­—ç¨¿ã€çš„æ•´é«”ç†è§£ï¼Œç‚ºå­¸ç”Ÿè¨­è¨ˆèƒ½æ¸¬è©¦æ·±åº¦çŸ¥è­˜æ‡‰ç”¨çš„é«˜å“è³ªå¤šé¸é¡Œã€‚"
    "ã€ŒOCR æ–‡å­—ã€åƒ…ä½œè¼”åŠ©è¦–è¦ºåƒè€ƒã€‚å‡ºé¡Œæ™‚é ˆéµå¾ª Bloom åˆ†é¡žæ³•çµæ§‹ï¼Œä¸¦ç¢ºä¿éŒ¯èª¤é¸é …åŸºæ–¼å¸¸è¦‹èª¤è§£ã€‚"
    "è«‹åš´æ ¼éµå®ˆæŒ‡å®šçš„ JSON è¼¸å‡ºæ ¼å¼ï¼Œä¸”åƒ…è¼¸å‡º JSONï¼Œä¸åšä»»ä½•å…¶ä»–èªªæ˜Žã€‚"
)

NOTES_SYSTEM_MESSAGE = (
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„èª²ç¨‹ç·¨è¼¯å’Œæ•™å­¸è¨­è¨ˆå°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡åŽŸå§‹é€å­—ç¨¿æç…‰ã€é‡æ§‹ç‚ºçµæ§‹æ¸…æ™°ã€æ¥µå…·å­¸ç¿’åƒ¹å€¼çš„å°ˆæ¥­è¬›ç¾©ã€‚"
    "å°ˆæ³¨æ–¼æ·±åº¦ç†è§£èˆ‡é‚è¼¯é‡çµ„ï¼Œè€Œéžç°¡å–®æŠ„å¯«ã€‚ä»¥ã€ŽASR é€å­—ç¨¿ã€ç‚ºæ ¸å¿ƒä¾æ“šï¼›ã€ŽOCR æ–‡å­—ã€åƒ…ä½œè¼”åŠ©è¦–è¦ºåƒè€ƒï¼Œè¡çªæ™‚ä»¥ ASR ç‚ºæº–ã€‚"
    "è«‹åš´æ ¼éµå®ˆæŒ‡å®šçš„ JSON è¼¸å‡ºæ ¼å¼ï¼Œä¸”åƒ…è¼¸å‡º JSONï¼Œä¸åšä»»ä½•å…¶ä»–èªªæ˜Žã€‚"
)

TOPICS_SUMMARY_SYSTEM_MESSAGE = (
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•™è‚²å…§å®¹åˆ†æžåŠ©æ‰‹ã€‚"
    "è«‹å¾žæ•™å­¸å…§å®¹ä¸­æå–ä¸»è¦ä¸»é¡Œå’Œæ¦‚å¿µï¼Œä¸¦æä¾›ç°¡æ½”çš„æ‘˜è¦ã€‚"
    "è«‹åš´æ ¼è¼¸å‡ºæŒ‡å®šçš„ JSON æž¶æ§‹ï¼Œä¸”åƒ…è¼¸å‡º JSONã€‚"
)
# ==================== CLIENT INITIALIZATION ====================
def initialize_client(service_type: str, **kwargs) -> Any:
    """Initialize the appropriate LLM client"""
    if service_type == "azure":
        return ChatCompletionsClient(
            endpoint=kwargs["endpoint"],
            credential=AzureKeyCredential(kwargs["key"]),
            api_version=kwargs.get("api_version", "2024-05-01-preview")
        )
    elif service_type == "openai":
        return OpenAI(
            api_key=kwargs["api_key"],
            base_url=kwargs.get("base_url", "https://api.openai.com/v1/")
        )
    else:
        raise ValueError(f"Unknown service type: {service_type}")

# ==================== LLM API CALLS ====================

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((Exception,)),
    reraise=True
)
def call_llm(
    service_type: str,
    client: Any,
    system_message: str,
    user_message: str,
    model: str,
    max_tokens: int = 4096,
    temperature: float = 0.2,
    top_p: float = 0.9,
    force_json: bool = False  # NEW PARAMETER
) -> Any:
    """Call LLM API with retry logic and optional JSON format enforcement"""
    if service_type == "azure":
        # Azure doesn't support response_format yet
        return client.complete(
            messages=[
                SystemMessage(content=system_message),
                UserMessage(content=user_message),
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            model=model
        )
    elif service_type == "openai":
        # Build kwargs
        kwargs = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p
        }
        
        # Add JSON format enforcement if requested
        if force_json and model in ["gpt-4o-mini", "gpt-4-turbo-preview", "gpt-3.5-turbo-1106"]:
            kwargs["response_format"] = {"type": "json_object"}
            logger.info("Using JSON format enforcement for OpenAI API")
        
        response = client.chat.completions.create(**kwargs)
        return response
    else:
        raise ValueError(f"Unknown service type: {service_type}")

def extract_text_from_response(resp, service_type: str) -> str:
    """Handle Azure/OpenAI response shape differences safely."""
    try:
        if service_type == "azure":
            choice_list = getattr(resp, "choices", None)
            if not choice_list:
                return ""
            choice0 = choice_list[0]
            msg = getattr(choice0, "message", None)
            if msg and getattr(msg, "content", None):
                return msg.content
            if isinstance(choice0, dict):
                msg = choice0.get("message") or {}
                return msg.get("content", "") or ""
            return ""
        else:
            return resp.choices[0].message.content
    except Exception:
        logger.exception("Failed to extract content from LLM response")
        return ""

# ==================== SAFE JSON HELPERS ====================
def _safe_load_json(text: str) -> Optional[dict]:
    """Extract JSON from fenced block if present and gently repair common issues."""
    m = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL) or re.search(r"```\s*(\{.*?\})\s*```", text, re.DOTALL)
    blob = (m.group(1) if m else text).strip()
    try:
        return json.loads(blob)
    except json.JSONDecodeError:
        blob2 = (blob
                 .replace("â€œ", '\"').replace("â€", '\"')
                 .replace("â€™", "'").replace("\u0000", ""))
        try:
            return json.loads(blob2)
        except Exception:
            logger.error("JSON parse failed. First 2k chars:\n%s", blob[:2000])
            return None

def _coerce_list(x):
    if x is None:
        return []
    return x if isinstance(x, list) else [x]

def _norm_mcq(d: dict) -> dict:
    """Normalize MCQ schema fields/types."""
    d = dict(d)
    if "correct_option" in d and "correct_answer" not in d:
        d["correct_answer"] = d.pop("correct_option")
    d["options"] = [str(o) for o in _coerce_list(d.get("options"))][:4]
    diff = str(d.get("difficulty", "medium")).lower()
    d["difficulty"] = diff if diff in {"easy", "medium", "hard"} else "medium"
    return d

# ==================== RESPONSE PARSING ====================
def parse_mcq_response(response_text: str, force_traditional: bool = True) -> List[MCQ]:
    """Parse MCQ response from LLM and convert to Traditional if requested"""
    data = _safe_load_json(response_text)
    if not data:
        return []
    
    mcqs: List[MCQ] = []
    for mcq_data in data.get('mcqs', []):
        d = _norm_mcq(mcq_data)
        
        # Extract basic fields
        q = d.get('question', '')
        opts = d.get('options', [])
        exp = d.get('explanation', '')
        topic = d.get('topic', '')
        
        # NEW: Extract tags
        tags = d.get('tags', [])
        if not isinstance(tags, list):
            tags = [tags] if tags else []
        tags = [str(tag).strip() for tag in tags if tag][:5]  # Limit to 5 tags
        
        # NEW: Extract course_type
        course_type = str(d.get('course_type', 'å…¶ä»–')).strip()
        valid_types = ['è¨­è¨ˆ', 'ç¨‹å¼', 'æ•¸å­¸', 'èªžè¨€', 'å•†æ¥­', 'ç§‘å­¸', 'å…¶ä»–']
        if course_type not in valid_types:
            course_type = 'å…¶ä»–'
        
        # Apply Traditional Chinese conversion
        if force_traditional:
            q = to_traditional(q)
            opts = [to_traditional(o) for o in opts]
            exp = to_traditional(exp)
            topic = to_traditional(topic)
            tags = [to_traditional(tag) for tag in tags]
            # course_type is already in Traditional
        
        mcqs.append(MCQ(
            question=q,
            options=opts,
            correct_answer=d.get('correct_answer', ''),
            explanation=exp,
            difficulty=d.get('difficulty', 'medium'),
            topic=topic,
            tags=tags,  # NEW
            course_type=course_type  # NEW
        ))
    
    return mcqs

def parse_lecture_notes_response(response_text: str, force_traditional: bool = True) -> Tuple[List[LectureNoteSection], str]:
    """Parse lecture notes response from LLM and convert to Traditional if requested"""
    data = _safe_load_json(response_text)
    if not data:
        return [], ''
    sections: List[LectureNoteSection] = []
    for section_data in data.get('sections', []):
        title = section_data.get('title', '')
        content = section_data.get('content', '')
        key_points = section_data.get('key_points', [])
        examples = section_data.get('examples', [])
        if force_traditional:
            title = to_traditional(title)
            content = to_traditional(content)
            key_points = [to_traditional(x) for x in key_points]
            examples = [to_traditional(x) for x in examples]
        sections.append(LectureNoteSection(
            title=title,
            content=content,
            key_points=key_points,
            examples=examples
        ))
    summary = data.get('summary', '')
    if force_traditional:
        summary = to_traditional(summary)
    return sections, summary
    
def parse_lecture_notes_with_validation(
    response_text: str,
    force_traditional: bool = True,
    run_dir: Optional[Path] = None
) -> Tuple[List[LectureNoteSection], str]:
    """
    Parse lecture notes using Pydantic validation for guaranteed structure.
    Never fails - always returns valid data.
    """
    sections: List[LectureNoteSection] = []
    summary: str = ""
    
    try:
        # Step 1: Extract JSON from response
        json_text = response_text.strip()
        
        # Handle markdown code blocks
        if "```json" in json_text:
            match = re.search(r"```json\s*(.*?)\s*```", json_text, re.DOTALL)
            if match:
                json_text = match.group(1)
        elif "```" in json_text:
            match = re.search(r"```\s*(.*?)\s*```", json_text, re.DOTALL)
            if match:
                json_text = match.group(1)
        
        # Step 2: Parse JSON
        data = json.loads(json_text)
        
        # Step 3: Validate with Pydantic
        validated = ValidatedLectureNotes(**data)
        
        # Step 4: Convert to your dataclass format
        for section in validated.sections:
            title = section.title
            content = section.content
            key_points = section.key_points
            examples = section.examples
            
            # Apply Traditional Chinese conversion if needed
            if force_traditional:
                title = to_traditional(title)
                content = to_traditional(content)
                key_points = [to_traditional(kp) for kp in key_points]
                examples = [to_traditional(ex) for ex in examples]
            
            sections.append(LectureNoteSection(
                title=title,
                content=content,
                key_points=key_points,
                examples=examples
            ))
        
        summary = validated.summary
        if force_traditional:
            summary = to_traditional(summary)
            
        logger.info(f"Successfully validated {len(sections)} lecture sections")
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON parsing failed at position {e.pos}: {e.msg}")
        if run_dir:
            error_file = run_dir / "notes_json_error.txt"
            with open(error_file, 'w', encoding='utf-8') as f:
                f.write(f"JSON Error: {e}\n\nRaw response:\n{response_text}")
        
        # Create fallback content from raw text
        sections, summary = _create_fallback_notes(response_text, force_traditional)
        
    except ValidationError as e:
        logger.error(f"Pydantic validation failed: {e}")
        if run_dir:
            error_file = run_dir / "notes_validation_error.txt"
            with open(error_file, 'w', encoding='utf-8') as f:
                f.write(f"Validation Error: {e}\n\nRaw response:\n{response_text}")
        
        # Try to use partial data if available
        try:
            if isinstance(data, dict):
                sections, summary = _extract_partial_notes(data, force_traditional)
            else:
                sections, summary = _create_fallback_notes(response_text, force_traditional)
        except:
            sections, summary = _create_fallback_notes(response_text, force_traditional)
    
    except Exception as e:
        logger.error(f"Unexpected error in lecture notes parsing: {e}")
        sections, summary = _create_fallback_notes(response_text, force_traditional)
    
    # Ensure we always have at least one section
    if not sections:
        sections = [LectureNoteSection(
            title="è¬›ç¾©å…§å®¹",
            content="ç„¡æ³•ç”Ÿæˆå®Œæ•´è¬›ç¾©å…§å®¹",
            key_points=["è«‹åƒè€ƒèª²ç¨‹éŒ„å½±"],
            examples=[]
        )]
    
    if not summary:
        summary = "è¬›ç¾©æ‘˜è¦ç”Ÿæˆä¸­ç™¼ç”ŸéŒ¯èª¤"
    
    return sections, summary

def _extract_partial_notes(
    data: dict,
    force_traditional: bool
) -> Tuple[List[LectureNoteSection], str]:
    """Extract whatever valid data we can from partial response"""
    sections = []
    
    # Try to extract sections
    if 'sections' in data and isinstance(data['sections'], list):
        for section_data in data['sections'][:10]:  # Limit to 10 sections
            if not isinstance(section_data, dict):
                continue
                
            try:
                # Use defaults for missing fields
                title = str(section_data.get('title', 'æœªå‘½åç« ç¯€'))[:300]
                content = str(section_data.get('content', ''))[:10000]
                key_points = section_data.get('key_points', [])
                examples = section_data.get('examples', [])
                
                # Ensure lists
                if not isinstance(key_points, list):
                    key_points = [str(key_points)] if key_points else []
                if not isinstance(examples, list):
                    examples = [str(examples)] if examples else []
                
                # Limit list sizes
                key_points = [str(kp) for kp in key_points[:10] if kp]
                examples = [str(ex) for ex in examples[:5] if ex]
                
                if force_traditional:
                    title = to_traditional(title)
                    content = to_traditional(content)
                    key_points = [to_traditional(kp) for kp in key_points]
                    examples = [to_traditional(ex) for ex in examples]
                
                if title or content:
                    sections.append(LectureNoteSection(
                        title=title,
                        content=content,
                        key_points=key_points,
                        examples=examples
                    ))
            except Exception as e:
                logger.debug(f"Failed to extract section: {e}")
                continue
    
    # Extract summary
    summary = str(data.get('summary', ''))[:1000]
    if force_traditional:
        summary = to_traditional(summary)
    
    return sections, summary

def _create_fallback_notes(
    response_text: str,
    force_traditional: bool
) -> Tuple[List[LectureNoteSection], str]:
    """Create minimal notes from raw text when all parsing fails"""
    # Try to extract any meaningful content from the response
    content = response_text[:5000]  # Use first 5000 chars
    
    if force_traditional:
        content = to_traditional(content)
    
    sections = [
        LectureNoteSection(
            title="è¬›ç¾©å…§å®¹ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰" if force_traditional else "Lecture Notes (Auto-generated)",
            content=content,
            key_points=["ç³»çµ±è‡ªå‹•æå–çš„å…§å®¹ï¼Œå¯èƒ½ä¸å®Œæ•´"],
            examples=[]
        )
    ]
    
    summary = "ç”±æ–¼æ ¼å¼éŒ¯èª¤ï¼Œç³»çµ±è‡ªå‹•ç”Ÿæˆäº†åŸºæœ¬è¬›ç¾©å…§å®¹" if force_traditional else "Basic notes auto-generated due to format error"
    
    return sections, summary
    
# ==================== ANSWER DISTRIBUTION & POST-PROCESSING ====================
def regenerate_explanation_with_llm(
    mcq: MCQ,
    *,
    service_type: str,
    client: Any,
    model: str,
    force_traditional: bool = True
) -> None:
    """Regenerate explanation text based on the (possibly shuffled) correct answer/option set."""
    labels = ["A", "B", "C", "D"]
    lines = [f"{labels[i]}. {opt}" for i, opt in enumerate(mcq.options[:4])]
    options_block = "\n".join(lines)

    sys = "ä½ æ˜¯ä¸€ä½å‡ºé¡Œè€å¸«ï¼Œè² è²¬ç‚ºæ¸¬é©—é¡Œæä¾›è§£æžï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‚è«‹ç°¡æ½”ä¸¦æŒ‡å‡ºå¸¸è¦‹èª¤è§£ã€‚"
    prompt = f"""
è«‹æ ¹æ“šä»¥ä¸‹é¡Œç›®èˆ‡é¸é …ï¼Œæä¾›ä¸­æ–‡è§£æžèªªæ˜Žç‚ºä½•æ­£ç¢ºç­”æ¡ˆæ˜¯ {mcq.correct_answer}ï¼Œä¸¦ç°¡è¦èªªæ˜Žå…¶ä»–é¸é …ç‚ºä½•ä¸æ­£ç¢ºã€‚
é–‹é ­è«‹ä½¿ç”¨ï¼šã€Œæ­£ç¢ºç­”æ¡ˆæ˜¯ {mcq.correct_answer}ï¼Œå› ç‚ºâ€¦ã€

é¡Œç›®ï¼š
{mcq.question}

é¸é …ï¼š
{options_block}
""".strip()

    resp = call_llm(
        service_type=service_type,
        client=client,
        system_message=sys,
        user_message=prompt,
        model=model,
        max_tokens=400,
        temperature=0.2,
        top_p=0.9
    )
    text = extract_text_from_response(resp, service_type) or ""
    mcq.explanation = to_traditional(text.strip()) if force_traditional else text.strip()

def shuffle_mcq_options(
    mcqs: List[MCQ],
    *,
    seed: Optional[int] = None,
    regenerate_explanations: bool = False,
    regeneration_cb: Optional[Callable[[MCQ], None]] = None
) -> None:
    """Shuffle options for each MCQ, update correct_answer letter accordingly.
       If regenerate_explanations is True and the correct letter moved,
       call regeneration_cb(mcq) to rebuild the explanation.
    """
    rng = random.Random(seed) if seed is not None else random
    for mcq in mcqs:
        if not mcq.options:
            continue
        letter_to_idx = {"A": 0, "B": 1, "C": 2, "D": 3}
        old_idx = letter_to_idx.get(mcq.correct_answer, 0)
        correct_text = mcq.options[old_idx] if old_idx < len(mcq.options) else mcq.options[0]
        rng.shuffle(mcq.options)
        mcq.options = mcq.options[:4]
        new_idx = next((i for i, t in enumerate(mcq.options) if t == correct_text), 0)
        new_letter = "ABCD"[new_idx]
        moved = (new_letter != mcq.correct_answer)
        mcq.correct_answer = new_letter
        if regenerate_explanations and moved and regeneration_cb:
            try:
                regeneration_cb(mcq)
            except Exception as e:
                logger.warning(f"Explanation regeneration failed: {e}")
        else:
            mcq.explanation = re.sub(r"æ­£ç¢ºç­”æ¡ˆæ˜¯\s+[A-D]", f"æ­£ç¢ºç­”æ¡ˆæ˜¯ {mcq.correct_answer}", mcq.explanation or "")

def enforce_difficulty_distribution(
    mcqs: List[MCQ],
    target_ratio: Tuple[float, float, float] = (0.3, 0.4, 0.3)
) -> List[MCQ]:
    """Reassign difficulty labels to approximate target ratios using a simple proxy (explanation length)."""
    total = len(mcqs) or 1
    target_easy   = round(target_ratio[0] * total)
    target_medium = round(target_ratio[1] * total)
    target_hard   = total - target_easy - target_medium
    ranked = sorted(mcqs, key=lambda q: len(q.explanation or ""))
    for idx, q in enumerate(ranked):
        if idx < target_easy:
            q.difficulty = "easy"
        elif idx < target_easy + target_medium:
            q.difficulty = "medium"
        else:
            q.difficulty = "hard"
    return mcqs

def postprocess_mcqs(
    mcqs: List[MCQ],
    *,
    shuffle: bool,
    regenerate_explanations: bool,
    enforce_difficulty: bool,
    seed: Optional[int],
    service_type: Optional[str],
    client: Optional[Any],
    model: Optional[str],
    force_traditional: bool
) -> List[MCQ]:
    """Apply optional post-processing steps controlled by function parameters."""
    if enforce_difficulty:
        enforce_difficulty_distribution(mcqs)
    if shuffle:
        def regen_cb(m: MCQ):
            if regenerate_explanations and service_type and client and model:
                regenerate_explanation_with_llm(
                    m,
                    service_type=service_type,
                    client=client,
                    model=model,
                    force_traditional=force_traditional
                )
        shuffle_mcq_options(
            mcqs,
            seed=seed,
            regenerate_explanations=regenerate_explanations,
            regeneration_cb=regen_cb if (regenerate_explanations and service_type and client and model) else None
        )
    return mcqs

# ==================== OUTPUT ADAPTERS (Hook points for legacy formats) ====================
def result_to_simple_json(result: EducationalContentResult) -> dict:
    return {
        "mcqs": [
            {
                "question": m.question,
                "options": m.options,
                "correct_answer": m.correct_answer,
                "explanation": m.explanation,
                "difficulty": m.difficulty,
                "topic": m.topic,
            }
            for m in result.mcqs
        ],
        "lecture_notes": [
            {
                "title": s.title,
                "content": s.content,
                "key_points": s.key_points,
                "examples": s.examples,
            }
            for s in result.lecture_notes
        ],
        "summary": result.summary,
    }

def result_to_markdown(result: EducationalContentResult) -> str:
    lines = ["# æ¸¬é©—é¡Œ (MCQs)", ""]
    for i, m in enumerate(result.mcqs, 1):
        lines.append(f"## Q{i}. {m.question}")
        for idx, opt in enumerate(m.options[:4]):
            lines.append(f"- {'ABCD'[idx]}. {opt}")
        lines.append(f"**æ­£ç¢ºç­”æ¡ˆ**ï¼š{m.correct_answer}")
        if m.explanation:
            lines.append(f"**è§£æž**ï¼š{m.explanation}")
        lines.append(f"**é›£åº¦**ï¼š{m.difficulty}ã€€**ä¸»é¡Œ**ï¼š{m.topic}")
        lines.append("")
    lines.append("# è¬›ç¾©ç­†è¨˜")
    for s in result.lecture_notes:
        lines.append(f"## {s.title}")
        lines.append(s.content or "")
        if s.key_points:
            lines.append("**é‡é»žï¼š**")
            for k in s.key_points:
                lines.append(f"- {k}")
        if s.examples:
            lines.append("**ç¯„ä¾‹ï¼š**")
            for ex in s.examples:
                lines.append(f"- {ex}")
        lines.append("")
    lines.append("## æ‘˜è¦")
    lines.append(result.summary or "")
    return "\n".join(lines)

def _mcqs_as_items(mcqs: List[MCQ]) -> List[dict]:
    items = []
    for i, m in enumerate(mcqs, 1):
        items.append({
            "QuestionId": f"Q{str(i).zfill(3)}",
            "QuestionText": m.question,
            "Options": [
                {"Label": "A", "Text": m.options[0] if len(m.options) > 0 else ""},
                {"Label": "B", "Text": m.options[1] if len(m.options) > 1 else ""},
                {"Label": "C", "Text": m.options[2] if len(m.options) > 2 else ""},
                {"Label": "D", "Text": m.options[3] if len(m.options) > 3 else ""},
            ],
            "CorrectAnswer": m.correct_answer,
            "Explanation": m.explanation,
            "Difficulty": {"easy": "ç°¡å–®", "medium": "ä¸­ç­‰", "hard": "å›°é›£"}.get(m.difficulty, m.difficulty),
            "Topic": m.topic,
        })
    return items

def result_to_pipeline_like(
    result: EducationalContentResult,
    *,
    num_questions: int,
    num_pages: int,
    meta: Optional[dict] = None
) -> dict:
    meta = meta or {}
    return {
        "success": True,
        "qa_and_notes": {
            "questions": _mcqs_as_items(result.mcqs),
            "lecture_notes": {
                "sections": [
                    {
                        "title": s.title,
                        "content": s.content,
                        "key_points": s.key_points,
                        "examples": s.examples
                    } for s in result.lecture_notes
                ],
                "summary": result.summary
            }
        },
        "summary": {
            "questions_generated": len(result.mcqs),
            "lecture_notes_pages": num_pages,
        },
        "pipeline_info": {**meta}
    }


# ==================== LEGACY CLIENT ADAPTER ====================

def result_to_legacy_client_format(
    result: EducationalContentResult,
    *,
    id: str,
    team_id: str,
    section_no: int,
    created_at: str,
    chapters: Optional[List[Dict]] = None,
    original_units: Optional[List[Dict]] = None,      # â† NEW
    suggested_units: Optional[List[Dict]] = None      # â† NEW
) -> dict:
    """
    Convert to client's expected API format with proper Options structure,
    Tags, CourseType fields, and Units/SuggestedUnits.
    
    Args:
        result: Educational content result with MCQs and notes
        id: Video/section ID
        team_id: Team identifier
        section_no: Section number
        created_at: ISO timestamp
        chapters: Optional chapter list for response
        original_units: Original units from client (pass-through)
        suggested_units: AI-generated suggested units
    
    Returns:
        Dictionary in client API format ready for webhook POST
    """
    
    # Difficulty mapping to Chinese
    difficulty_map = {
        "easy": "ç°¡å–®",
        "medium": "ä¸­ç­‰",
        "hard": "å›°é›£"
    }

    from collections import OrderedDict

    # Transform questions to client format
    client_questions = []
    for i, mcq in enumerate(result.mcqs, start=1):
        # Ensure we have tags (fallback if needed)
        tags = mcq.tags if hasattr(mcq, 'tags') and mcq.tags else []
        if not tags and mcq.topic:
            tags = [mcq.topic]
            
        # Ensure we have course_type
        course_type = mcq.course_type if hasattr(mcq, 'course_type') else 'å…¶ä»–'
        
        # Build Options with explicit order
        options = [  
            OrderedDict([("Label", label), ("Text", text)])
            for label, text in zip(["A", "B", "C", "D"], mcq.options)
        ]
        # Build question with EXPLICIT field order (client's expected format)
        question = OrderedDict([
            ("QuestionId", f"Q{str(i).zfill(3)}"),
            ("QuestionText", mcq.question),
            ("Options", options),
            ("CorrectAnswer", mcq.correct_answer),
            ("Explanation", mcq.explanation),
            ("Tags", tags[:5]),
            ("Difficulty", difficulty_map.get(mcq.difficulty, mcq.difficulty)),
            ("CourseType", course_type)
        ])
        client_questions.append(question)
            
    # Build lecture notes markdown
    markdown_lines = []
    for section in result.lecture_notes:
        markdown_lines.append(f"## {section.title}")
        markdown_lines.append(section.content)
        if section.key_points:
            markdown_lines.append("### é‡é»ž")
            for point in section.key_points:
                markdown_lines.append(f"- {point}")
        if section.examples:
            markdown_lines.append("### ç¯„ä¾‹")
            for example in section.examples:
                markdown_lines.append(f"- {example}")
        markdown_lines.append("")
    
    markdown_lines.append("## ç¸½çµ")
    markdown_lines.append(result.summary)
    
    # Build response with Units fields
    response = {
        "Id": id,
        "TeamId": team_id,
        "SectionNo": section_no,
        "CreatedAt": created_at,
        "Questions": client_questions,
        "CourseNote": "\n".join(markdown_lines).strip(),
        "Units": original_units or [],           # â† NEW: Original units (pass-through)
        "SuggestedUnits": suggested_units or []  # â† NEW: AI-generated suggestions
    }
    
    # Add chapters if provided (optional - may be removed in future)
    if chapters:
        response["chapters"] = chapters
    
    return response

# ==================== ASR PREPROCESSING ====================
def preprocess_asr_text(raw_asr_text: str, min_chunk_duration: int = 60, max_gap: int = 10) -> str:
    """Preprocess raw ASR text by combining lines into meaningful chunks."""
    lines = raw_asr_text.strip().split('\n')
    chunks: List[str] = []
    current_chunk: List[str] = []
    current_start_time: Optional[int] = None
    current_end_time: Optional[int] = None
    for line in lines:
        if not line.strip():
            continue
        if ':' in line and len(line.split(':', 1)) > 1:
            time_part, content = line.split(':', 1)
            time_part = time_part.strip()
            content = content.strip()
            if not content:
                continue
            try:
                if time_part.count(':') == 2:  # HH:MM:SS
                    h, m, s = map(int, time_part.split(':'))
                    timestamp_sec = h * 3600 + m * 60 + s
                elif time_part.count(':') == 1:  # MM:SS
                    m, s = map(int, time_part.split(':'))
                    timestamp_sec = m * 60 + s
                else:
                    continue
            except ValueError:
                continue
            if current_start_time is None:
                current_start_time = timestamp_sec
                current_end_time = timestamp_sec
                current_chunk = [content]
            elif timestamp_sec - current_end_time > max_gap:
                if current_chunk and current_end_time - current_start_time >= min_chunk_duration:
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(f"[{sec_to_hms(current_start_time)}-{sec_to_hms(current_end_time)}] {chunk_text}")
                current_start_time = timestamp_sec
                current_end_time = timestamp_sec
                current_chunk = [content]
            else:
                current_chunk.append(content)
                current_end_time = timestamp_sec
        else:
            if current_chunk:
                current_chunk.append(line.strip())
    if current_chunk and current_end_time is not None and current_start_time is not None:
        dur = current_end_time - current_start_time
        if dur >= min_chunk_duration or not chunks:
            chunk_text = ' '.join(current_chunk)
            chunks.append(f"[{sec_to_hms(current_start_time)}-{sec_to_hms(current_end_time)}] {chunk_text}")
    if not chunks:
        cleaned_lines = []
        for line in lines:
            if ':' in line and len(line.split(':', 1)) > 1:
                _, content = line.split(':', 1)
                if content.strip():
                    cleaned_lines.append(content.strip())
        return ' '.join(cleaned_lines)
    return '\n\n'.join(chunks)

# ==================== MAIN FUNCTIONS ====================
def initialize_and_get_client(config: EducationalContentConfig):
    service_type = config.service_type
    model = config.openai_model if service_type == "openai" else config.azure_model
    if service_type == "azure":
        client = initialize_client(
            service_type="azure",
            endpoint=config.azure_endpoint,
            key=config.azure_key,
            api_version=config.azure_api_version
        )
    else:
        client = initialize_client(
            service_type="openai",
            api_key=config.openai_api_key,
            base_url=config.openai_base_url
        )
    return service_type, model, client

from typing import Union

def generate_educational_content(
    raw_asr_text: str,
    ocr_segments: Union[List[Dict], str],
    video_id: str,
    video_title: Optional[str] = None,
    run_dir: Optional[Path] = None,
    progress_callback: Optional[Callable[[str, int], None]] = None,
    *,
    # NEW PARAMETERS
    chapters: Optional[Dict[str, str]] = None,  # {"00:10:17": "[èª²ç¨‹å°Žå…¥] èª²ç¨‹é–‹å§‹"}
    course_summary: Optional[Dict[str, str]] = None,  # DEPRECATED: Use hierarchical_metadata instead
    hierarchical_metadata: Optional[Dict] = None,  # â† ADD THIS! Full metadata from chapter generation
    section_title: Optional[str] = None,      # â† ADD THIS
    units: Optional[List[Dict]] = None,       # â† ADD THIS
    num_questions: Optional[int] = None,  # â† ADD THIS
    num_pages: Optional[int] = None,      # â† ADD THIS
    # Existing parameters
    shuffle_options: bool = False,
    regenerate_explanations: bool = False,
    enforce_difficulty: bool = True,
    shuffle_seed: Optional[int] = None,
    ocr_text_override: Optional[str] = None,
) -> EducationalContentResult:
    """
    Main function to generate educational content from pre-processed segments.
    
    Args:
        hierarchical_metadata: Full metadata from video_chaptering.hierarchical_multipass_generation()
                              Contains: structure_analysis, modules_analysis, course_summary, 
                              educational_quality_score, token_usage
    """
    
    report("initializing", progress_callback)

    if run_dir is None:
        run_dir = Path(f"/tmp/educational_content/{video_id}_{int(time.time())}")
    run_dir.mkdir(parents=True, exist_ok=True)

    try:
        logger.info(f"Starting educational content generation for video {video_id}")
        
        # Log educational metadata if provided
        if section_title or units:
            logger.info("=" * 60)
            logger.info("ðŸ“š EDUCATIONAL METADATA RECEIVED")
            if section_title:
                logger.info(f"   ðŸ“– Section Title: {section_title}")
            if units:
                logger.info(f"   ðŸ“‘ Units ({len(units)}):")
                for unit in units:
                    logger.info(f"      {unit['UnitNo']}. {unit['Title']}")
            logger.info("=" * 60)

        config = EducationalContentConfig()
        # Use passed values or fall back to config defaults
        actual_num_questions = num_questions if num_questions is not None else config.max_questions
        actual_num_pages = num_pages if num_pages is not None else config.max_notes_pages

        if not validate_config(config):
            raise RuntimeError("Configuration validation failed")

        report("processing_inputs", progress_callback)

        transcript = (raw_asr_text)
        if ocr_text_override is not None:
            ocr_context = ocr_text_override
        elif isinstance(ocr_segments, str):
            ocr_context = ocr_segments
        else:
            ocr_context = "\n".join(
                (seg.get("text") or "").strip()
                for seg in (ocr_segments or [])
                if (seg.get("text") or "").strip()
            )
        
        logger.info(f"ASR-first policy active. Generating {actual_num_questions} MCQs and {actual_num_pages}p notes.")
        logger.info(f"Preprocessed transcript chars: {len(transcript)}, OCR context chars: {len(ocr_context)}")

        # Save input files
        with open(run_dir / "raw_asr_text.txt", "w", encoding="utf-8") as f:
            f.write(raw_asr_text)
        with open(run_dir / "preprocessed_transcript.txt", "w", encoding="utf-8") as f:
            f.write(transcript)
        with open(run_dir / "ocr_segments.json", "w", encoding="utf-8") as f:
            json.dump(ocr_segments, f, ensure_ascii=False, indent=2)
        with open(run_dir / "ocr_context.txt", "w", encoding="utf-8") as f:
            f.write(ocr_context)

        report("initializing_client", progress_callback)
        service_type, model, client = initialize_and_get_client(config)

        # Centralize context budgets per model
        MODEL_BUDGETS = {
            "gpt-4o-mini": 128_000,
            "Meta-Llama-3.1-8B-Instruct": 128_000,
        }
        ctx_budget = MODEL_BUDGETS.get(model, 100_000)

        # ========================================================================
        # Topic Extraction - Use metadata if available, else extract
        # ========================================================================

        # Priority 1: Use full hierarchical_metadata (best)
        if hierarchical_metadata and hierarchical_metadata.get('modules_analysis'):
            logger.info("=" * 60)
            logger.info("ðŸ“Š Using FULL hierarchical metadata from chapter generation")
            logger.info("   âœ… Skipping LLM call - using existing analysis")
            logger.info("=" * 60)
    
            # Extract rich context from metadata
            structure_analysis = hierarchical_metadata.get('structure_analysis', '')
            modules_analysis = hierarchical_metadata.get('modules_analysis', '')
            course_summary_data = hierarchical_metadata.get('course_summary', {})
            quality_score = hierarchical_metadata.get('educational_quality_score', 0)
    
            # Parse modules into topics
            topics_list = parse_modules_to_topics(modules_analysis)
            
            # Build global summary from multiple sources
            summary_parts = []
            if course_summary_data.get('topic'):
                summary_parts.append(course_summary_data['topic'])
            if course_summary_data.get('core_content'):
                summary_parts.append(f"æ ¸å¿ƒå…§å®¹åŒ…æ‹¬{course_summary_data['core_content']}")
            if course_summary_data.get('learning_objectives'):
                summary_parts.append(course_summary_data['learning_objectives'])
            
            global_summary = "ã€‚".join(summary_parts) if summary_parts else "æ•™å­¸èª²ç¨‹å…§å®¹"
    
            # Extract key takeaways
            key_takeaways = []
            if course_summary_data.get('learning_objectives'):
                key_takeaways.append(course_summary_data['learning_objectives'])
            if structure_analysis and 'å­¸ç¿’ç›®æ¨™' in structure_analysis:
                goals_match = re.search(r'å­¸ç¿’ç›®æ¨™[ï¼š:](.*?)(?:\n|$)', structure_analysis)
                if goals_match:
                    key_takeaways.append(goals_match.group(1).strip())
            
            if not key_takeaways:
                key_takeaways = [f"æŽŒæ¡{t['title']}" for t in topics_list[:3]]
            
            logger.info(f"âœ… Extracted from metadata:")
            logger.info(f"   â€¢ Topics: {len(topics_list)}")
            logger.info(f"   â€¢ Quality score: {quality_score}")
            logger.info(f"   â€¢ Summary: {global_summary[:100]}...")

            # Create topics_output for file saving later
            topics_output = f"""Topics extracted from hierarchical metadata (no LLM call needed):

Source: hierarchical_metadata from 3-pass chapter generation
Method: Parsed from modules_analysis
Quality Score: {quality_score:.2f}

Topics ({len(topics_list)}):
{chr(10).join(f"{i+1}. {t['title']} - {t['summary']}" for i, t in enumerate(topics_list))}

Course Summary:
{global_summary}

Key Takeaways:
{chr(10).join(f"- {kt}" for kt in key_takeaways)}

Structure Analysis (excerpt):
{structure_analysis[:500]}...

Modules Analysis:
{modules_analysis}
"""
            
            # Save extracted info for debugging
            with open(run_dir / "metadata_extracted_topics.json", "w", encoding="utf-8") as f:
                json.dump({
                    "topics": topics_list,
                    "global_summary": global_summary,
                    "key_takeaways": key_takeaways,
                    "source": "hierarchical_metadata"
                }, f, ensure_ascii=False, indent=2)

        # Priority 2: Fallback to course_summary only (limited)
        elif course_summary:
            logger.info("=" * 60)
            logger.info("ðŸ“Š Using LIMITED course_summary (backward compatibility)")
            logger.info("   âš ï¸  Consider passing full hierarchical_metadata for better results")
            logger.info("=" * 60)
    
            # Convert course_summary to expected format
            topics_list = []
            if course_summary.get('core_content'):
                core_items = course_summary['core_content'].split('ã€')
                for i, item in enumerate(core_items[:5], 1):
                    topics_list.append({
                        "id": str(i).zfill(2),
                        "title": item.strip(),
                        "summary": f"èª²ç¨‹é‡é»žï¼š{item}",
                        "keywords": []
                    })
            
            global_summary = f"{course_summary.get('topic', '')}èª²ç¨‹ï¼Œ{course_summary.get('core_content', '')}ã€‚{course_summary.get('learning_objectives', '')}"
            key_takeaways = [course_summary.get('learning_objectives', '')] if course_summary.get('learning_objectives') else []
    
            logger.info(f"âœ… Extracted from course_summary: {len(topics_list)} topics")
    
            # Create topics_output for file saving later
            topics_output = f"""Topics extracted from course_summary (legacy mode):

Source: course_summary (limited metadata)
Warning: Consider using full hierarchical_metadata for better results

Topics ({len(topics_list)}):  
{chr(10).join(f"{i+1}. {t['title']}" for i, t in enumerate(topics_list))}

Course Summary:
{global_summary}

Key Takeaways:
{chr(10).join(f"- {kt}" for kt in key_takeaways if kt)}
"""
        # Priority 3: Extract topics ourselves (slowest)
        else:
            report("generating_topics_summary", progress_callback)
            logger.info("=" * 60)
            logger.info("ðŸ“Š NO metadata provided - extracting topics from transcript")
            logger.info("   â±ï¸  This will take extra time (~5-10 seconds)")
            logger.info("=" * 60)
            
            # Calculate budget for topic extraction
            topics_prompt_template_tokens = count_tokens_llama(
                build_topics_summary_prompt(transcript="", context=None)
            )
            topics_budget = max(2_000, ctx_budget - topics_prompt_template_tokens)
            topics_transcript = truncate_text_by_tokens(transcript, topics_budget)
    
            # Build context (optional)
            topics_context = {
                "è¦–é »ID": video_id,
                "å…§å®¹é¡žåž‹": "æ•™å­¸è¦–é »"
            }
         
            # Generate topics prompt
            topics_prompt = build_topics_summary_prompt(
                transcript=topics_transcript,
                video_title=video_title,
                context=topics_context
            )
            logger.info(f"Topics extraction prompt approx tokens: {count_tokens_llama(topics_prompt):,}")
            
            # Call LLM for topics extraction
            topics_response = call_llm(
                service_type=service_type,
                client=client,
                system_message=TOPICS_SUMMARY_SYSTEM_MESSAGE,
                user_message=topics_prompt,
                model=model,
                max_tokens=2048,
                temperature=0.15,
                top_p=0.9
            )
            # Parse the response
            topics_output = extract_text_from_response(topics_response, service_type)
            topics_list, global_summary, key_takeaways = parse_topics_summary_response(topics_output)

            # Log extraction results
            logger.info(f"âœ… Extracted via LLM: {len(topics_list)} topics")
                
            # Save topics to file for debugging
            with open(run_dir / "extracted_topics.json", "w", encoding="utf-8") as f:
                json.dump({
                    "topics": topics_list,
                    "global_summary": global_summary,
                    "key_takeaways": key_takeaways,
                    "source": "llm_extraction"
                }, f, ensure_ascii=False, indent=2)
                

        # ========================================================================
        # UPDATED: MCQ Generation with Topics and Summary
        # ========================================================================
        report("generating_mcqs", progress_callback)
        # Format chapters if provided
        formatted_chapters = format_chapters_for_prompt(chapters) if chapters else None

        mcq_prompt_template_tokens = count_tokens_llama(build_mcq_prompt_v2(
            transcript="",
            ocr_context=ocr_context,
            num_questions=config.max_questions,
            chapters=formatted_chapters,
            global_summary=global_summary, 
        ))
        mcq_budget = max(2_000, ctx_budget - mcq_prompt_template_tokens)
        mcq_transcript = truncate_text_by_tokens(transcript, mcq_budget)
        
        final_mcq_prompt = build_mcq_prompt_v2(
            transcript=mcq_transcript,
            ocr_context=ocr_context,
            video_title=video_title,  # â† ADD THIS
            num_questions=actual_num_questions,
            chapters=formatted_chapters,
            global_summary=global_summary, 
            hierarchical_metadata=hierarchical_metadata,
            section_title=section_title,      # â† ADD
            units=units                        # â† ADD
        )

        logger.info(f"MCQ prompt approx tokens: {count_tokens_llama(final_mcq_prompt):,}")
        logger.info(f"ðŸ“š Generating {actual_num_questions} MCQs with ASR-first policy, chapters, and topic context")
        

        mcq_response = call_llm(
            service_type=service_type,
            client=client,
            system_message=MCQ_SYSTEM_MESSAGE,
            user_message=final_mcq_prompt,
            model=model,
            max_tokens=4096,
            temperature=0.2,
            top_p=0.9
        )
        mcq_output = extract_text_from_response(mcq_response, service_type)
        mcqs = parse_mcq_response(mcq_output, force_traditional=config.force_traditional)

        # Post-processing
        mcqs = postprocess_mcqs(
            mcqs,
            shuffle=shuffle_options,
            regenerate_explanations=regenerate_explanations,
            enforce_difficulty=enforce_difficulty,
            seed=shuffle_seed,
            service_type=service_type,
            client=client,
            model=model,
            force_traditional=config.force_traditional
        )

        # ========================================================================
        # UPDATED: Lecture Notes Generation with Topics and Summary
        # ========================================================================
        report("generating_notes", progress_callback)

        # Build the notes prompt (same as before)
        notes_prompt_template_tokens = count_tokens_llama(build_lecture_notes_prompt_v2(
            transcript="",
            ocr_context=ocr_context,
            num_pages=config.max_notes_pages,
            chapters=None,
            topics=topics_list,
            global_summary=global_summary,
        ))
        notes_budget = max(2_000, ctx_budget - notes_prompt_template_tokens)
        notes_transcript = truncate_text_by_tokens(transcript, notes_budget)

        notes_prompt = build_lecture_notes_prompt_v2(
            transcript=notes_transcript,
            ocr_context=ocr_context,
            video_title=video_title,  # â† ADD THIS
            num_pages=actual_num_pages,
            chapters=formatted_chapters,
            topics=topics_list,
            global_summary=global_summary,
            hierarchical_metadata=hierarchical_metadata,
            section_title=section_title,      # â† ADD
            units=units                        # â† ADD
        )
        logger.info(f"ðŸ“˜ Generating {actual_num_pages} pages of lecture notes with validation")

        # Call LLM with JSON format enforcement (if using OpenAI)
        notes_response = call_llm(
            service_type=service_type,
            client=client,
            system_message=NOTES_SYSTEM_MESSAGE,
            user_message=notes_prompt,
            model=model,
            max_tokens=8096,
            temperature=0.2,
            top_p=0.9,
            force_json=(service_type == "openai")  # Enable JSON format for OpenAI
        )
        notes_output = extract_text_from_response(notes_response, service_type)

        # Save raw response for debugging
        with open(run_dir / "notes_response.txt", "w", encoding="utf-8") as f:
            f.write(notes_output)
            
        # Use the new validation-based parsing
        lecture_sections, summary = parse_lecture_notes_with_validation(
            notes_output,
            force_traditional=config.force_traditional,
            run_dir=run_dir
        )
        logger.info(f"âœ… Generated {len(lecture_sections)} validated lecture sections")

        # ========================================================================
        # Rest of the function remains the same
        # ========================================================================
        report("processing_results", progress_callback)

        result = EducationalContentResult(
            mcqs=mcqs,
            lecture_notes=lecture_sections,
            summary=summary,
            topics=topics_list,           
            key_takeaways=key_takeaways
        )

        # Optional caching
        if config.enable_cache:
            cache_dir = run_dir / "cache"
            cache_dir.mkdir(exist_ok=True)
            mcq_key = get_content_hash(mcq_transcript, ocr_context, "mcq")
            notes_key = get_content_hash(notes_transcript, ocr_context, "notes")
            with open(cache_dir / f"{mcq_key}.json", "w", encoding="utf-8") as f:
                json.dump([vars(x) for x in mcqs], f, ensure_ascii=False, indent=2)
            with open(cache_dir / f"{notes_key}.json", "w", encoding="utf-8") as f:
                json.dump({"sections": [vars(s) for s in lecture_sections], "summary": summary}, f, ensure_ascii=False, indent=2)

        # Persist raw LLM outputs & final results
        with open(run_dir / "mcq_response.txt", "w", encoding="utf-8") as f:
            f.write(mcq_output)
        with open(run_dir / "notes_response.txt", "w", encoding="utf-8") as f:
            f.write(notes_output)
        with open(run_dir / "topics_response.txt", "w", encoding="utf-8") as f:
            f.write(topics_output)  # â† SAVE TOPICS RESPONSE TOO
        with open(run_dir / "final_result.json", "w", encoding="utf-8") as f:
            json.dump({
                "mcqs": [vars(mcq) for mcq in mcqs],
                "lecture_notes": [vars(section) for section in lecture_sections],
                "summary": summary,
                "topics": topics_list,  # â† INCLUDE TOPICS IN FINAL OUTPUT
                "key_takeaways": key_takeaways  # â† INCLUDE KEY TAKEAWAYS TOO
            }, f, ensure_ascii=False, indent=2)

        report("completed", progress_callback)
        logger.info(f"Successfully generated {len(mcqs)} MCQs and {len(lecture_sections)} lecture note sections")
        return result

    except Exception as e:
        logger.error(f"Educational content generation failed: {e}", exc_info=True)
        raise

# ---- Adapter for tasks.py compatibility ----

def process_text_for_qa_and_notes(
    *,
    # Prefer raw_asr_text (matches new tasks.py). Fallback to audio_segments if not provided.
    raw_asr_text: str = "",
    audio_segments: Optional[List[Dict]] = None,
    ocr_segments: Optional[List[Dict]] = None,
    video_title: Optional[str] = None,
    chapters: Optional[Dict[str, str]] = None,
    hierarchical_metadata: Optional[Dict] = None,
    section_title: Optional[str] = None,
    units: Optional[List[Dict]] = None,
    num_questions: int = 10,
    num_pages: int = 3,
    id: str = "",
    team_id: str = "",
    section_no: int = 0,
    created_at: str = "",
) -> EducationalContentResult:
    """
    Adapter so tasks.py (and other callers) can invoke the generator.
    - If raw_asr_text is provided, we use it directly (ASR-first).
    - Else, we reconstruct a raw ASR string from audio_segments ("HH:MM:SS: text" per line).
    Returns the raw EducationalContentResult object (not the pipeline format).
    """
    
    # ========== VALIDATE CLIENT-PROVIDED UNITS ==========
    validated_units = None
    unit_validation_info = {
        "units_provided": bool(units and isinstance(units, list) and len(units) > 0),
        "units_count": len(units) if units else 0,
        "units_validated": False,
        "validation_score": 0.0,
        "validation_reason": "",
        "units_accepted": False
    }
    
    if units and isinstance(units, list) and len(units) > 0 and hierarchical_metadata:
        logger.info("=" * 60)
        logger.info("ðŸ” VALIDATING CLIENT-PROVIDED UNITS")
        logger.info("=" * 60)
        
        # Extract content analysis from Pass 1
        content_analysis = hierarchical_metadata.get("content_analysis", {})
        
        if content_analysis:
            # Validate units against actual video content
            is_valid, score, reason = validate_units_relevance(
                units=units,
                content_analysis=content_analysis,
                chapters=chapters or {},
                video_title=video_title or "",
                threshold=UNIT_VALIDATION_THRESHOLD
            )
            
            # Update validation info
            unit_validation_info.update({
                "units_validated": True,
                "validation_score": score,
                "validation_reason": reason,
                "units_accepted": is_valid
            })
            
            # Log to metrics file
            log_validation_metrics(
                video_id=id or "unknown",
                units_provided=len(units),
                validation_score=score,
                accepted=is_valid,
                threshold=UNIT_VALIDATION_THRESHOLD
            )
            
            # Decide whether to use units
            if is_valid:
                validated_units = units
                logger.info(f"âœ… Client units VALIDATED and ACCEPTED")
                logger.info(f"   These units will be used in Q&A and lecture notes generation")
            else:
                validated_units = None
                logger.warning(f"âš ï¸  Client units REJECTED - too low relevance")
                logger.warning(f"   Q&A and notes will rely on AI-generated chapters only")
        else:
            logger.warning("âš ï¸  No content_analysis in metadata - cannot validate units")
            logger.warning("   Using units without validation (assuming valid)")
            validated_units = units
    
    elif units and isinstance(units, list) and len(units) > 0:
        logger.warning("âš ï¸  Units provided but no hierarchical_metadata - cannot validate")
        logger.warning("   Using units without validation (assuming valid)")
        validated_units = units
    
    logger.info("=" * 60)
    
    # Rest of function continues...
    ocr_segments = ocr_segments or []

    # 1) Choose ASR source
    if raw_asr_text and raw_asr_text.strip():
        asr_text_for_prompt = raw_asr_text
    else:
        audio_segments = audio_segments or []
        def _line(seg: Dict) -> str:
            ts = sec_to_hms(int(seg.get("start", 0)))
            txt = (seg.get("text") or "").strip()
            return f"{ts}: {txt}" if txt else ""
        asr_text_for_prompt = "\n".join(filter(None, (_line(s) for s in audio_segments)))

    # 2) Honor per-call counts by temporarily overriding env
    old_max_q = os.environ.get("MAX_QUESTIONS")
    old_max_p = os.environ.get("MAX_NOTES_PAGES")
    os.environ["MAX_QUESTIONS"] = str(num_questions)
    os.environ["MAX_NOTES_PAGES"] = str(num_pages)

    try:
        # Call with validated_units instead of units
        result = generate_educational_content(
            raw_asr_text=asr_text_for_prompt,
            ocr_segments=ocr_segments,
            video_id=id or "video",
            video_title=video_title,
            chapters=chapters,
            hierarchical_metadata=hierarchical_metadata,
            section_title=section_title,
            units=validated_units,  # â† USE VALIDATED UNITS HERE!
            num_questions=num_questions,
            num_pages=num_pages,
            run_dir=None,
            progress_callback=None,
            shuffle_options=False,
            regenerate_explanations=False,
            enforce_difficulty=True,
            shuffle_seed=None,
            ocr_text_override=None,
        )
        # ========== ADD VALIDATION METADATA TO RESULT ==========
        # Add validation metadata to result (for tracking)
        if not hasattr(result, 'metadata'):
            result.metadata = {}
        result.metadata['unit_validation'] = unit_validation_info
        
        return result

    finally:
        if old_max_q is not None:
            os.environ["MAX_QUESTIONS"] = old_max_q
        else:
            os.environ.pop("MAX_QUESTIONS", None)
        if old_max_p is not None:
            os.environ["MAX_NOTES_PAGES"] = old_max_p
        else:
            os.environ.pop("MAX_NOTES_PAGES", None)
  

# ==================== USAGE EXAMPLE ====================
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    sample_transcript = """
    00:00:03: ä»Šå¤©æˆ‘å€‘ä¾†å­¸ç¿’å¾®ç©åˆ†çš„åŸºæœ¬æ¦‚å¿µã€‚é¦–å…ˆï¼Œå°Žæ•¸è¡¨ç¤ºå‡½æ•¸åœ¨æŸä¸€é»žçš„çž¬æ™‚è®ŠåŒ–çŽ‡ã€‚
    00:01:05: ç©åˆ†å‰‡æ˜¯å°Žæ•¸çš„é€†é‹ç®—ï¼Œç”¨ä¾†è¨ˆç®—é¢ç©å’Œç´¯ç©é‡ã€‚
    """
    sample_ocr = [
        {"start": 0, "end": 10, "text": "å°Žæ•¸å®šç¾©: f'(x) = lim(hâ†’0) [f(x+h)-f(x)]/h"},
        {"start": 60, "end": 70, "text": "ç©åˆ†ç¬¦è™Ÿ: âˆ« f(x) dx"}
    ]

    # Example: turn on shuffling + difficulty enforcement for this run
    result = generate_educational_content(
        raw_asr_text=sample_transcript,
        ocr_segments=sample_ocr,
        video_id="calc_101",
        shuffle_options=True,
        regenerate_explanations=False,
        enforce_difficulty=True,
        shuffle_seed=42
    )

    print(f"Generated {len(result.mcqs)} MCQs and {len(result.lecture_notes)} lecture sections")
    print("Summary:", (result.summary or "")[:120], "â€¦")
